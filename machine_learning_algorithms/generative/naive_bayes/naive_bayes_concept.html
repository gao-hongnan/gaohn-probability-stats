
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Naive Bayes Concept &#8212; Probability &amp; Statistics</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script src="../../../_static/tabs.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"defeq": "\\overset{\\text{def}}{=}", "defa": "\\overset{\\text{(a)}}{=}", "defb": "\\overset{\\text{(b)}}{=}", "defc": "\\overset{\\text{(c)}}{=}", "defd": "\\overset{\\text{(d)}}{=}", "st": "\\mid", "mod": "\\mid", "S": "\\Omega", "s": "\\omega", "e": "\\exp", "P": "\\mathbb{P}", "R": "\\mathbb{R}", "expectation": "\\mathbb{E}", "v": "\\mathbf{v}", "a": "\\mathbf{a}", "b": "\\mathbf{b}", "c": "\\mathbf{c}", "u": "\\mathbf{u}", "w": "\\mathbf{w}", "x": "\\mathbf{x}", "y": "\\mathbf{y}", "z": "\\mathbf{z}", "0": "\\mathbf{0}", "1": "\\mathbf{1}", "A": "\\mathbf{A}", "B": "\\mathbf{B}", "C": "\\mathbf{C}", "E": "\\mathcal{F}", "lset": "\\left\\{", "rset": "\\right\\}", "lsq": "\\left[", "rsq": "\\right]", "lpar": "\\left(", "rpar": "\\right)", "lcurl": "\\left\\{", "rcurl": "\\right\\}", "pmf": "p_X", "pdf": "f_X", "pdftwo": "f_{X,Y}", "pdfjoint": "f_{\\mathbf{X}}", "cdf": "F_X", "pspace": "(\\Omega, \\mathcal{F}, \\mathbb{P})", "var": "\\operatorname{Var}", "std": "\\operatorname{Std}", "bern": "\\operatorname{Bernoulli}", "binomial": "\\operatorname{Binomial}", "geometric": "\\operatorname{Geometric}", "poisson": "\\operatorname{Poisson}", "uniform": "\\operatorname{Uniform}", "normal": "\\operatorname{Normal}", "gaussian": "\\operatorname{Gaussian}", "gaussiansymbol": "\\mathcal{N}", "exponential": "\\operatorname{Exponential}", "iid": "\\textbf{i.i.d.}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Example: Penguins Dataset" href="naive_bayes_example_penguin.html" />
    <link rel="prev" title="Brain Dump" href="../../../05_joint_distributions/braindump.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Probability & Statistics</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Chapter 1. Mathematical Preliminaries
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../01_mathematical_preliminaries/01_combinatorics.html">
   Permutations and Combinations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../01_mathematical_preliminaries/02_calculus.html">
   Calculus
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../01_mathematical_preliminaries/exercises.html">
   Exercises
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Chapter 2. Probability
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../02_probability/0202_probability_space.html">
   Probability Space
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../02_probability/0203_probability_axioms.html">
   Probability Axioms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../02_probability/0204_conditional_probability.html">
   Conditional Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../02_probability/0205_independence.html">
   Independence
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../02_probability/0206_bayes_theorem.html">
   Baye’s Theorem and the Law of Total Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../02_probability/summary.html">
   Summary
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Chapter 3. Discrete Random Variables
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../03_discrete_random_variables/0301_random_variables.html">
   Random Variables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../03_discrete_random_variables/0302_discrete_random_variables.html">
   Discrete Random Variables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../03_discrete_random_variables/0303_probability_mass_function.html">
   Probability Mass Function
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../03_discrete_random_variables/0304_cumulative_distribution_function.html">
   Cumulative Distribution Function
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../03_discrete_random_variables/0305_expectation.html">
   Expectation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../03_discrete_random_variables/0306_moments_and_variance.html">
   Moments and Variance
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../03_discrete_random_variables/0307_discrete_uniform_distribution_concept.html">
   Discrete Uniform Distribution
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../03_discrete_random_variables/0307_discrete_uniform_distribution_implementation.html">
     Implementation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../03_discrete_random_variables/0308_bernoulli_distribution_concept.html">
   Bernoulli Distribution
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../03_discrete_random_variables/0308_bernoulli_distribution_implementation.html">
     Implementation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../03_discrete_random_variables/iid.html">
   Independent and Identically Distributed (IID)
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../03_discrete_random_variables/0309_binomial_distribution_concept.html">
   Binomial Distribution
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../03_discrete_random_variables/0309_binomial_distribution_implementation.html">
     Implementation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../03_discrete_random_variables/0309_binomial_distribution_application.html">
     Real World Examples
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../03_discrete_random_variables/0310_geometric_distribution_concept.html">
   Geometric Distribution
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../03_discrete_random_variables/0311_poisson_distribution_concept.html">
   Poisson Distribution
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../03_discrete_random_variables/0311_poisson_distribution_implementation.html">
     Implementation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../03_discrete_random_variables/summary.html">
   Important
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../03_discrete_random_variables/exercises.html">
   Exercises
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Chapter 4. Continuous Random Variables
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../04_continuous_random_variables/from_discrete_to_continuous.html">
   From Discrete to Continuous
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../04_continuous_random_variables/0401_continuous_random_variables.html">
   Continuous Random Variables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../04_continuous_random_variables/0402_probability_density_function.html">
   Probability Density Function
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../04_continuous_random_variables/0403_expectation.html">
   Expectation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../04_continuous_random_variables/0404_moments_and_variance.html">
   Moments and Variance
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../04_continuous_random_variables/0405_cumulative_distribution_function.html">
   Cumulative Distribution Function
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../04_continuous_random_variables/0406_mean_median_mode.html">
   Mean, Median and Mode
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../04_continuous_random_variables/0407_continuous_uniform_distribution.html">
   Continuous Uniform Distribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../04_continuous_random_variables/0408_exponential_distribution.html">
   Exponential Distribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../04_continuous_random_variables/0409_gaussian_distribution.html">
   Gaussian Distribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../04_continuous_random_variables/0410_skewness_and_kurtosis.html">
   Skewness and Kurtosis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../04_continuous_random_variables/0411_convolve_and_sum_of_random_variables.html">
   Convolution and Sum of Random Variables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../04_continuous_random_variables/0412_functions_of_random_variables.html">
   Functions of Random Variables
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Chapter 5. Joint Distributions
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../05_joint_distributions/braindump.html">
   Brain Dump
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Machine Learning Algorithms
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Naive Bayes Concept
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="naive_bayes_example_penguin.html">
   Example: Penguins Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="naive_bayes_application_mnist.html">
   Naive Bayes Application (MNIST)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  References and Resources
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../references_and_resources/bibliography.html">
   Bibliography
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../references_and_resources/resources.html">
   Resources
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/gao-hongnan/gaohn-probability-stats"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/gao-hongnan/gaohn-probability-stats/issues/new?title=Issue%20on%20page%20%2Fmachine_learning_algorithms/generative/naive_bayes/naive_bayes_concept.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../../_sources/machine_learning_algorithms/generative/naive_bayes/naive_bayes_concept.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#notations">
   Notations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#discriminative-vs-generative">
     Discriminative vs Generative
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#naive-bayes-setup">
   Naive Bayes Setup
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inference-prediction">
   Inference/Prediction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-naive-bayes-assumptions">
   The Naive Bayes Assumptions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#independent-and-identically-distributed-i-i-d">
     Independent and Identically Distributed (i.i.d.)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conditional-independence">
     Conditional Independence
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#parameter-vector">
   Parameter Vector
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inductive-bias-distribution-assumptions">
   Inductive Bias (Distribution Assumptions)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#targets-categorical-distribution">
     Targets (Categorical Distribution)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#discrete-features-categorical-distribution">
     Discrete Features (Categorical Distribution)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#continuous-features-gaussian-distribution">
     Continuous Features (Gaussian Distribution)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mixed-features-discrete-and-continuous">
     Mixed Features (Discrete and Continuous)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-fitting">
   Model Fitting
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fitting-algorithm">
     Fitting Algorithm
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#maximum-likelihood-estimation">
     Maximum Likelihood Estimation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#estimating-priors">
     Estimating Priors
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#maximum-likelihood-estimation-for-priors-categorical-distribution">
     Maximum Likelihood Estimation for Priors (Categorical Distribution)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#estimating-likelihood-gaussian-version">
     Estimating Likelihood (Gaussian Version)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#maximum-likelihood-estimate-for-likelihood-continuous-feature-parameters">
     Maximum Likelihood Estimate for Likelihood (Continuous Feature Parameters)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#brain-dump">
   Brain dump
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Naive Bayes Concept</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#notations">
   Notations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#discriminative-vs-generative">
     Discriminative vs Generative
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#naive-bayes-setup">
   Naive Bayes Setup
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inference-prediction">
   Inference/Prediction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-naive-bayes-assumptions">
   The Naive Bayes Assumptions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#independent-and-identically-distributed-i-i-d">
     Independent and Identically Distributed (i.i.d.)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conditional-independence">
     Conditional Independence
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#parameter-vector">
   Parameter Vector
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inductive-bias-distribution-assumptions">
   Inductive Bias (Distribution Assumptions)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#targets-categorical-distribution">
     Targets (Categorical Distribution)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#discrete-features-categorical-distribution">
     Discrete Features (Categorical Distribution)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#continuous-features-gaussian-distribution">
     Continuous Features (Gaussian Distribution)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mixed-features-discrete-and-continuous">
     Mixed Features (Discrete and Continuous)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-fitting">
   Model Fitting
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fitting-algorithm">
     Fitting Algorithm
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#maximum-likelihood-estimation">
     Maximum Likelihood Estimation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#estimating-priors">
     Estimating Priors
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#maximum-likelihood-estimation-for-priors-categorical-distribution">
     Maximum Likelihood Estimation for Priors (Categorical Distribution)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#estimating-likelihood-gaussian-version">
     Estimating Likelihood (Gaussian Version)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#maximum-likelihood-estimate-for-likelihood-continuous-feature-parameters">
     Maximum Likelihood Estimate for Likelihood (Continuous Feature Parameters)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#brain-dump">
   Brain dump
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="naive-bayes-concept">
<h1>Naive Bayes Concept<a class="headerlink" href="#naive-bayes-concept" title="Permalink to this headline">#</a></h1>
<section id="notations">
<h2>Notations<a class="headerlink" href="#notations" title="Permalink to this headline">#</a></h2>
<div class="proof definition admonition" id="underlying-distributions">
<p class="admonition-title"><span class="caption-number">Definition 57 </span> (Underlying Distributions)</p>
<section class="definition-content" id="proof-content">
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathcal{X}\)</span>: Input space consists of all possible inputs <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathcal{X}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>: Label space = <span class="math notranslate nohighlight">\(\{1, 2, \cdots, K\}\)</span> where <span class="math notranslate nohighlight">\(K\)</span> is the number of classes.</p></li>
<li><p>The mapping between <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> and <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> is given by <span class="math notranslate nohighlight">\(c: \mathcal{X} \rightarrow \mathcal{Y}\)</span> where <span class="math notranslate nohighlight">\(c\)</span> is called <em>concept</em> according to the PAC learning theory.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}\)</span>: The fixed but unknown distribution of the data. Usually, this refers
to the joint distribution of the input and the label,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \mathcal{D} &amp;= \mathbb{P}(\mathcal{X}, \mathcal{Y} ; \boldsymbol{\theta}) \\
  &amp;= \mathbb{P}_{\{\mathcal{X}, \mathcal{Y} ; \boldsymbol{\theta}\}}(\mathbf{x}, y)
  \end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathcal{X}\)</span> and <span class="math notranslate nohighlight">\(y \in \mathcal{Y}\)</span>, and <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> is the
parameter vector of the distribution <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>.</p>
</li>
</ul>
</section>
</div><div class="proof definition admonition" id="dataset-definition">
<p class="admonition-title"><span class="caption-number">Definition 58 </span> (Dataset)</p>
<section class="definition-content" id="proof-content">
<p>Now, consider a dataset <span class="math notranslate nohighlight">\(\mathcal{D}_{\{\mathbf{x}, y\}}\)</span> consisting of <span class="math notranslate nohighlight">\(N\)</span> samples (observations) and <span class="math notranslate nohighlight">\(D\)</span> predictors (features) drawn <strong>jointly</strong> and <strong>indepedently and identically distributed</strong> (i.i.d.) from <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>. Note we will refer to the dataset <span class="math notranslate nohighlight">\(\mathcal{D}_{\{\mathbf{x}, y\}}\)</span> with the same notation as the underlying distribution <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> from now on.</p>
<ul>
<li><p>The training dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> can also be represented compactly as a set:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{align*}
    \mathcal{D} \overset{\mathbf{def}}{=} \mathcal{D}_{\{\mathbf{x}, y\}} &amp;= \left\{\mathbf{x}^{(n)}, y^{(n)}\right\}_{n=1}^N \\
    &amp;= \left\{\left(\mathbf{x}^{(1)}, y^{(1)}\right), \left(\mathbf{x}^{(2)}, y^{(2)}\right), \cdots, \left(\mathbf{x}^{(N)}, y^{(N)}\right)\right\} \\
    &amp;= \left\{\mathbf{X}, \mathbf{y}\right\}
    \end{align*}
    \end{split}\]</div>
<p>where we often subscript <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(y\)</span> with <span class="math notranslate nohighlight">\(n\)</span> to denote the <span class="math notranslate nohighlight">\(n\)</span>-th sample from the dataset, i.e.
<span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> and <span class="math notranslate nohighlight">\(y^{(n)}\)</span>. Most of the times, <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> is bolded since
it represents a vector of <span class="math notranslate nohighlight">\(D\)</span> number of features, while <span class="math notranslate nohighlight">\(y^{(n)}\)</span> is not bolded since it is a scalar, though
it is not uncommon for <span class="math notranslate nohighlight">\(y^{(n)}\)</span> to be bolded as well if you represent it with K-dim one-hot vector.</p>
</li>
<li><p>For the n-th sample <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span>, we often denote the <span class="math notranslate nohighlight">\(d\)</span>-th feature as <span class="math notranslate nohighlight">\(x_d^{(n)}\)</span> and the representation of <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> as a vector as:</p>
<div class="math notranslate nohighlight">
\[
  \mathbf{x}^{(n)} \in \mathbb{R}^{D} = \begin{bmatrix} x_1^{(n)} &amp; x_2^{(n)} &amp; \cdots &amp; x_D^{(n)} \end{bmatrix}_{D \times 1}
  \]</div>
<p>is a sample of size <span class="math notranslate nohighlight">\(D\)</span>, drawn (jointly with <span class="math notranslate nohighlight">\(y\)</span>) <span class="math notranslate nohighlight">\(\textbf{i.i.d.}\)</span> from <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>.</p>
</li>
<li><p>We often add an extra feature <span class="math notranslate nohighlight">\(x_0^{(n)} = 1\)</span> to <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> to represent the bias term.
i.e.</p>
<div class="math notranslate nohighlight">
\[
  \mathbf{x}^{(n)} \in \mathbb{R}^{D+1} = \begin{bmatrix} x_0^{(n)} &amp; x_1^{(n)} &amp; x_2^{(n)} &amp; \cdots &amp; x_D^{(n)} \end{bmatrix}_{(D+1) \times 1}
  \]</div>
</li>
<li><p>For the n-th sample’s label <span class="math notranslate nohighlight">\(y^{(n)} \overset{\mathbf{def}}{=} c(\mathbf{x}^{(n)})\)</span>, if we were to represent it as K-dim one-hot vector, we would have:</p>
<div class="math notranslate nohighlight">
\[
  y^{(n)} \in \mathbb{R}^{K} = \begin{bmatrix} 0 &amp; 0 &amp; \cdots &amp; 1 &amp; \cdots &amp; 0 \end{bmatrix}_{K \times 1}
  \]</div>
<p>where the <span class="math notranslate nohighlight">\(1\)</span> is at the <span class="math notranslate nohighlight">\(k\)</span>-th position, and <span class="math notranslate nohighlight">\(k\)</span> is the class label of the n-th sample.</p>
</li>
<li><p>Everything defined above is for <strong>one single sample/data point</strong>, to represent it as a matrix, we can define
a design matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> and a label vector <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> as follows,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \begin{aligned}
  \mathbf{X} \in \mathbb{R}^{N \times D} &amp;= \begin{bmatrix} \mathbf{x}^{(1)} \\ \mathbf{x}^{(2)} \\ \vdots \\ \mathbf{x}^{(N)} \end{bmatrix} = \begin{bmatrix} x_1^{(1)} &amp; x_2^{(1)} &amp; \cdots &amp; x_D^{(1)} \\ x_1^{(2)} &amp; x_2^{(2)} &amp; \cdots &amp; x_D^{(2)} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ x_1^{(N)} &amp; x_2^{(N)} &amp; \cdots &amp; x_D^{(N)} \end{bmatrix}_{N \times D} \\
  \end{aligned}
  \end{split}\]</div>
<p>as the matrix of all samples. Note that each row is a sample and each column is a feature. We can append a column of 1’s to the first column of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> to represent the bias term.</p>
<p><strong>In this section, we also talk about random vectors <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> so we will replace the design matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> with <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> to avoid confusion.</strong></p>
<p>Subsequently, for the label vector <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>, we can define it as follows,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \begin{aligned}
  \mathbf{y} \in \mathbb{R}^{N} = \begin{bmatrix} y^{(1)} \\ y^{(2)} \\ \vdots \\ y^{(N)} \end{bmatrix}
  \end{aligned}
  \end{split}\]</div>
</li>
</ul>
</section>
</div><div class="proof example admonition" id="joint-distribution-example">
<p class="admonition-title"><span class="caption-number">Example 12 </span> (Joint Distribution Example)</p>
<section class="example-content" id="proof-content">
<p>For example, if the number of features, <span class="math notranslate nohighlight">\(D = 2\)</span>, then let’s say</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X}^{(n)} = \begin{bmatrix} X^{(n)}_1 &amp; X^{(n)}_2 \end{bmatrix} \in \mathbb{R}^2
\]</div>
<p>consists of two Gaussian random variables,
with <span class="math notranslate nohighlight">\(\mu_1\)</span> and <span class="math notranslate nohighlight">\(\mu_2\)</span> being the mean of the two distributions,
and <span class="math notranslate nohighlight">\(\sigma_1\)</span> and <span class="math notranslate nohighlight">\(\sigma_2\)</span> being the variance of the two distributions;
furthermore, <span class="math notranslate nohighlight">\(Y^{(n)}\)</span> is a Bernoulli random variable with parameter <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span>, then we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\boldsymbol{\theta} &amp;= \begin{bmatrix} \mu_1 &amp; \sigma_1 &amp; \mu_2 &amp; \sigma_2 &amp; \boldsymbol{\pi}\end{bmatrix} \\
&amp;= \begin{bmatrix} \boldsymbol{\mu} &amp; \boldsymbol{\sigma} &amp; \boldsymbol{\pi} \end{bmatrix}
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\mu} = \begin{bmatrix} \mu_1 &amp; \mu_2 \end{bmatrix}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\sigma} = \begin{bmatrix} \sigma_1 &amp; \sigma_2 \end{bmatrix}\)</span>.</p>
</section>
</div><div class="proof remark admonition" id="some-remarks">
<p class="admonition-title"><span class="caption-number">Remark 7 </span> (Some remarks)</p>
<section class="remark-content" id="proof-content">
<ul class="simple">
<li><p>From now on, we will refer the realization of <span class="math notranslate nohighlight">\(Y\)</span> as <span class="math notranslate nohighlight">\(k\)</span> instead.</p></li>
<li><p>For some sections, when I mention <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, it means the random vector which resides in the
<span class="math notranslate nohighlight">\(D\)</span>-dimensional space, not the design matrix. This also means that this random vector refers
to a single sample, not the entire dataset.</p></li>
</ul>
</section>
</div><div class="proof definition admonition" id="joint-and-conditional-probability">
<p class="admonition-title"><span class="caption-number">Definition 59 </span> (Joint and Conditional Probability)</p>
<section class="definition-content" id="proof-content">
<p>We are often interested in finding the probability of a label given a sample,</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\mathbb{P}(Y = k \mid \mathbf{X} = \mathbf{x}) &amp;= \mathbb{P}(Y = k \mid \mathbf{X} = \left(x_1, x_2, \ldots, x_D\right)) 
\end{aligned}
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X} \in \mathbb{R}^{D} = \begin{bmatrix} X_1 &amp; X_2 &amp; \cdots &amp; X_D \end{bmatrix} 
\]</div>
<p>is a random vector and its realizations,</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x} = \begin{bmatrix} x_1 &amp; x_2 &amp; \cdots &amp; x_D \end{bmatrix}
\]</div>
<p>and therefore, <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> can be characterized by an <span class="math notranslate nohighlight">\(D\)</span>-dimensional PDF</p>
<div class="math notranslate nohighlight">
\[
f_{\mathbf{X}}(\mathbf{x}) = f_{X_1, X_2, \ldots, X_D}(x_1, x_2, \ldots, x_D ; \boldsymbol{\theta})
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
Y \in \mathbb{Z} \quad \text{and} \quad k \in \mathbb{Z}
\]</div>
<p>is a discrete random variable (in our case classification) and its realization respectively, and therefore, <span class="math notranslate nohighlight">\(Y\)</span> can be characterized by a discrete PDF (PMF)</p>
<div class="math notranslate nohighlight">
\[
f_{Y}(k ; \boldsymbol{\pi}) \sim \text{Categorical}(\boldsymbol{\pi})
\]</div>
<p><strong>Note that we are talking about one single sample tuple <span class="math notranslate nohighlight">\(\left(\mathbf{x}, y\right)\)</span> here. I did not
index the sample tuple with <span class="math notranslate nohighlight">\(n\)</span> because this sample can be any sample in the unknown distribution <span class="math notranslate nohighlight">\(\mathbb{P}_{\mathcal{X}, \mathcal{Y}}(\mathbf{x}, y)\)</span>
and not only from our given dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>.</strong></p>
</section>
</div><div class="proof definition admonition" id="likelihood">
<p class="admonition-title"><span class="caption-number">Definition 60 </span> (Likelihood)</p>
<section class="definition-content" id="proof-content">
<p>We denote the likelihood function as <span class="math notranslate nohighlight">\(\mathbb{P}(\mathbf{X} = \mathbf{x} \mid Y = k)\)</span>,
which is the probability of observing <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> given that the sample belongs to class <span class="math notranslate nohighlight">\(Y = k\)</span>.</p>
</section>
</div><div class="proof definition admonition" id="prior">
<p class="admonition-title"><span class="caption-number">Definition 61 </span> (Prior)</p>
<section class="definition-content" id="proof-content">
<p>We denote the prior probability of class <span class="math notranslate nohighlight">\(k\)</span> as <span class="math notranslate nohighlight">\(\mathbb{P}(Y = k)\)</span>, which usually
follows a discrete distribution such as the Categorical distribution.</p>
</section>
</div><div class="proof definition admonition" id="posterior">
<p class="admonition-title"><span class="caption-number">Definition 62 </span> (Posterior)</p>
<section class="definition-content" id="proof-content">
<p>We denote the posterior probability of class <span class="math notranslate nohighlight">\(k\)</span> given <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> as <span class="math notranslate nohighlight">\(\mathbb{P}(Y = k \mid \mathbf{X} = \mathbf{x})\)</span>.</p>
</section>
</div><div class="proof definition admonition" id="marginal-distribution-and-normalization-constant">
<p class="admonition-title"><span class="caption-number">Definition 63 </span> (Marginal Distribution and Normalization Constant)</p>
<section class="definition-content" id="proof-content">
<p>We denote the normalizing constant as <span class="math notranslate nohighlight">\(\mathbb{P}(\mathbf{X} = \mathbf{x}) = \sum_{k=1}^K \mathbb{P}(Y = k) \mathbb{P}(\mathbf{X} = \mathbf{x} \mid Y = k)\)</span>.</p>
</section>
</div><section id="discriminative-vs-generative">
<h3>Discriminative vs Generative<a class="headerlink" href="#discriminative-vs-generative" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Discriminative classifiers model the conditional distribution <span class="math notranslate nohighlight">\(\mathbb{P}(Y = k \mid \mathbf{X} =  \mathbf{x})\)</span>.
This means we are modelling the conditional distribution of the target <span class="math notranslate nohighlight">\(Y\)</span> given the input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p></li>
<li><p>Generative classifiers model the conditional distribution <span class="math notranslate nohighlight">\(\mathbb{P}(\mathbf{X} = \mathbf{x} \mid Y = k)\)</span>.
This means we are modelling the conditional distribution of the input <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> given the target <span class="math notranslate nohighlight">\(Y\)</span>.
Then we can use Bayes’ rule to compute the conditional distribution of the target <span class="math notranslate nohighlight">\(Y\)</span> given the input <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.</p></li>
<li><p>Both the target <span class="math notranslate nohighlight">\(Y\)</span> and the input <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> are random variables in the generative model.
In the discriminative model, only the target <span class="math notranslate nohighlight">\(Y\)</span> is a random variable as the input <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is fixed (we do not need to estimate anything about the input <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>).</p></li>
<li><p>For example, Logistic Regression models the target <span class="math notranslate nohighlight">\(Y\)</span> as a function of predictor’s <span class="math notranslate nohighlight">\(\mathbf{X} = \begin{bmatrix}X_1 \\ X_2 \\ \vdots \\X_D \end{bmatrix}\)</span>.</p></li>
<li><p>Naive bayes models both the target <span class="math notranslate nohighlight">\(Y\)</span> and the predictors <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> as a function of each other.
This means we are modelling the joint distribution of the target <span class="math notranslate nohighlight">\(Y\)</span> and the predictors <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.</p></li>
</ul>
</section>
</section>
<section id="naive-bayes-setup">
<h2>Naive Bayes Setup<a class="headerlink" href="#naive-bayes-setup" title="Permalink to this headline">#</a></h2>
<p>Let</p>
<div class="math notranslate nohighlight">
\[
\mathcal{D} = \left \{ \left(\mathrm{X}^{(n)}, Y^{(n)} \right) \right \}_{n=1}^N = \left \{ \left(\mathrm{x}^{(n)}, y^{(n)} \right) \right \}_{n=1}^N
\]</div>
<p>be the dataset with <span class="math notranslate nohighlight">\(N\)</span> samples and <span class="math notranslate nohighlight">\(D\)</span> predictors.</p>
<p>All samples are assumed to be <strong>independent and identically distributed (i.i.d.)</strong> from the unknown but fixed joint distribution
<span class="math notranslate nohighlight">\(\mathbb{P}(\mathcal{X}, \mathcal{Y} ; \boldsymbol{\theta})\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\left \{ \left(\mathrm{X}^{(n)}, Y^{(n)} \right) \right \} \overset{\small{\text{i.i.d.}}}{\sim} \mathbb{P}(\mathcal{X}, \mathcal{Y} ; \boldsymbol{\theta}) \quad \text{for } n = 1, 2, \cdots, N
\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> is the parameter vector of the joint distribution. See <a class="reference internal" href="#joint-distribution-example">Example 12</a> for an example of such.</p>
</section>
<section id="inference-prediction">
<span id="naive-bayes-inference-prediction"></span><h2>Inference/Prediction<a class="headerlink" href="#inference-prediction" title="Permalink to this headline">#</a></h2>
<p>Before we look at the fitting/estimating process, let’s look at the inference/prediction process.</p>
<p>Suppose the problem at hand has <span class="math notranslate nohighlight">\(K\)</span> classes, <span class="math notranslate nohighlight">\(k = 1, 2, \cdots, K\)</span>, where <span class="math notranslate nohighlight">\(k\)</span> is the index of the class.</p>
<p>Then, to find the class of a new test sample <span class="math notranslate nohighlight">\(\mathbf{x}^{(q)} \in \mathbb{R}^{D}\)</span> with <span class="math notranslate nohighlight">\(D\)</span> features,
we can compute the conditional probability of each class <span class="math notranslate nohighlight">\(Y = k\)</span> given the sample <span class="math notranslate nohighlight">\(\mathbf{x}^{(q)}\)</span>:</p>
<div class="proof algorithm full-width admonition" id="naive-bayes-inference-algorithm">
<p class="admonition-title"><span class="caption-number">Algorithm 1 </span> (Naive Bayes Inference Algorithm)</p>
<section class="algorithm-content" id="proof-content">
<ul>
<li><p>Compute the conditional probability of each class <span class="math notranslate nohighlight">\(Y = k\)</span> given the sample <span class="math notranslate nohighlight">\(\mathbf{x}^{(q)}\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-conditional-naive-bayes">
<span class="eqno">(29)<a class="headerlink" href="#equation-eq-conditional-naive-bayes" title="Permalink to this equation">#</a></span>\[
  \mathbb{P}(Y = k \mid \mathbf{X} = \mathbf{x}^{(q)}) = \dfrac{\mathbb{P}(Y = k) \mathbb{P}(\mathbf{X} = \mathbf{x}^{(q)} \mid Y = k)}{\mathbb{P}(\mathbf{X} = \mathbf{x}^{(q)})} \quad \text{for } k = 1, 2, \cdots, K
  \]</div>
</li>
<li><p>Choose the class <span class="math notranslate nohighlight">\(k\)</span> that maximizes the conditional probability:</p>
<div class="math notranslate nohighlight" id="equation-eq-argmax-naive-bayes-1">
<span class="eqno">(30)<a class="headerlink" href="#equation-eq-argmax-naive-bayes-1" title="Permalink to this equation">#</a></span>\[
  \hat{y}^{(q)} = \arg\max_{k=1}^K \mathbb{P}(Y = k \mid \mathbf{X} = \mathbf{x}^{(q)})
  \]</div>
</li>
<li><p>The observant reader would have noticed that the normalizing constant
<span class="math notranslate nohighlight">\(\mathbb{P}\left(X = \mathbf{x}^{(q)}\right)\)</span> is the same for all <span class="math notranslate nohighlight">\(k\)</span>.
Therefore, we can ignore it and simply choose the class <span class="math notranslate nohighlight">\(k\)</span> that maximizes
the numerator of the conditional probability in <a class="reference internal" href="#equation-eq-conditional-naive-bayes">(29)</a>:</p>
<div class="math notranslate nohighlight" id="equation-eq-argmax-naive-bayes-2">
<span class="eqno">(31)<a class="headerlink" href="#equation-eq-argmax-naive-bayes-2" title="Permalink to this equation">#</a></span>\[
  \hat{y}^{(q)} = \arg\max_{k=1}^K \mathbb{P}(Y = k) \mathbb{P}(\mathbf{X} = \mathbf{x}^{(q)} \mid Y = k) 
  \]</div>
<p>since where the normalizing constant is ignored, the conditional probability</p>
<div class="math notranslate nohighlight" id="equation-eq-proportional-naive-bayes">
<span class="eqno">(32)<a class="headerlink" href="#equation-eq-proportional-naive-bayes" title="Permalink to this equation">#</a></span>\[
  \mathbb{P}(Y = k \mid \mathbf{X} = \mathbf{x}^{(q)}) \propto \mathbb{P}(Y = k) \mathbb{P}(\mathbf{X} = \mathbf{x}^{(q)} \mid Y = k) 
  \]</div>
<p>by a constant factor <span class="math notranslate nohighlight">\(\mathbb{P}(\mathbf{X} = \mathbf{x}^{(q)})\)</span>.</p>
</li>
<li><p>Expressing it in vector form, we have</p>
<div class="math notranslate nohighlight" id="equation-eq-argmax-naive-bayes-3">
<span class="eqno">(33)<a class="headerlink" href="#equation-eq-argmax-naive-bayes-3" title="Permalink to this equation">#</a></span>\[\begin{split}
  \begin{aligned}
  \hat{\mathbf{y}} &amp;= \arg\max_{k=1}^K \begin{bmatrix} \mathbb{P}(Y=1) \mathbb{P}(\mathbf{X} = \mathbf{x}\mid Y = 1) \\ \mathbb{P}(Y=2) \mathbb{P}(\mathbf{X} = \mathbf{x}\mid Y = 2) \\ \vdots \\ \mathbb{P}(Y=K) \mathbb{P}(\mathbf{X} = \mathbf{x} \mid Y = K) \end{bmatrix}_{K \times 1} \\  
  &amp;= \arg\max_{k=1}^K \begin{bmatrix} \mathbb{P}(Y=1) \\ \mathbb{P}(Y=2) \\ \cdots \\ \mathbb{P}(Y=K) \end{bmatrix}\circ \begin{bmatrix} \mathbb{P}(\mathbf{X} = \mathbf{x} \mid Y = 1) \\ \mathbb{P}(\mathbf{X} = \mathbf{x}\mid Y = 2) \\ \vdots \\ \mathbb{P}(\mathbf{X} = \mathbf{x} \mid Y = K) \end{bmatrix} \\
  &amp;= \arg\max_{k=1}^K \mathbf{M_1} \circ \mathbf{M_2} \\
  &amp;= \arg\max_{k=1}^K \mathbf{M_1} \circ \mathbf{M_3} \\
  \end{aligned}
  \end{split}\]</div>
<p>where</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-m1">
<span class="eqno">(34)<a class="headerlink" href="#equation-eq-naive-bayes-m1" title="Permalink to this equation">#</a></span>\[\begin{split}
  \mathbf{M_1} = \begin{bmatrix}
  \mathbb{P}(Y = 1) \\
  \mathbb{P}(Y = 2) \\
  \vdots \\
  \mathbb{P}(Y = K)
  \end{bmatrix}_{K \times 1}
  \end{split}\]</div>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-m2">
<span class="eqno">(35)<a class="headerlink" href="#equation-eq-naive-bayes-m2" title="Permalink to this equation">#</a></span>\[\begin{split}
  \mathbf{M_2} = \begin{bmatrix}
  \mathbb{P}(\mathbf{X} = \mathbf{x} \mid Y = 1) \\
  \mathbb{P}(\mathbf{X} = \mathbf{x} \mid Y = 2) \\
  \vdots \\
  \mathbb{P}(\mathbf{X} = \mathbf{x} \mid Y = K)
  \end{bmatrix}_{K \times 1}
  \end{split}\]</div>
<p>and</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-m3">
<span class="eqno">(36)<a class="headerlink" href="#equation-eq-naive-bayes-m3" title="Permalink to this equation">#</a></span>\[\begin{split}
  \mathbf{M_3} &amp;= \begin{bmatrix}
  \mathbb{P}(X_1 = x_1 \mid Y = 1 ; \theta_{11}) &amp; \mathbb{P}(X_2 = x_2 \mid Y = 1 ; \theta_{12}) &amp; \cdots &amp; \mathbb{P}(X_D = x_D \mid Y = 1 ; \theta_{1D}) \\
  \mathbb{P}(X_1 = x_1 \mid Y = 2 ; \theta_{21}) &amp; \mathbb{P}(X_2 = x_2 \mid Y = 2 ; \theta_{22}) &amp; \cdots &amp; \mathbb{P}(X_D = x_D \mid Y = 2 ; \theta_{2D}) \\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
  \mathbb{P}(X_1 = x_1 \mid Y = K ; \theta_{K1}) &amp; \mathbb{P}(X_2 = x_2 \mid Y = K ; \theta_{K2}) &amp; \cdots &amp; \mathbb{P}(X_D = x_D \mid Y = K ; \theta_{KD})
  \end{bmatrix}_{K \times D} \\
  \end{split}\]</div>
<p>Note superscript <span class="math notranslate nohighlight">\(q\)</span> is removed for simplicity, and <span class="math notranslate nohighlight">\(\circ\)</span> is the element-wise (Hadamard) product.
We will also explain why we replace <span class="math notranslate nohighlight">\(\mathbf{M_2}\)</span> with <span class="math notranslate nohighlight">\(\mathbf{M_3}\)</span> in <a class="reference internal" href="#naive-bayes-conditional-independence"><span class="std std-ref">Conditional Independence</span></a>.</p>
</li>
</ul>
</section>
</div><p>Now if we just proceed to estimate the conditional probability <span class="math notranslate nohighlight">\(\mathbb{P}(Y = k \mid \mathbf{X} = \mathbf{x}^{(q)})\)</span>, we will need to estimate the joint probability <span class="math notranslate nohighlight">\(\mathbb{P}(X = \mathbf{x}^{(q)}, Y = k)\)</span>, since by definition, we have</p>
<div class="math notranslate nohighlight" id="equation-eq-joint-naive-bayes-1">
<span class="eqno">(37)<a class="headerlink" href="#equation-eq-joint-naive-bayes-1" title="Permalink to this equation">#</a></span>\[
\mathbb{P}(X = \mathbf{x}^{(q)}, Y = k) = \mathbb{P}(Y = k) \mathbb{P}(X = \mathbf{x}^{(q)} \mid Y = k)
\]</div>
<p>which is intractable<a class="footnote-reference brackets" href="#intractable" id="id1">1</a>.</p>
<p>However, if we can <em><strong>estimate</strong></em> the conditional probability (likelihood) <span class="math notranslate nohighlight">\(\mathbb{P}(\mathbf{X} = \mathbf{x}^{(q)} \mid Y = k)\)</span>
and the prior probability <span class="math notranslate nohighlight">\(\mathbb{P}(Y = k)\)</span>, then we can use Bayes’ rule to
compute the posterior conditional probability <span class="math notranslate nohighlight">\(\mathbb{P}(Y = k \mid \mathbf{X} = \mathbf{x}^{(q)})\)</span>.</p>
</section>
<section id="the-naive-bayes-assumptions">
<h2>The Naive Bayes Assumptions<a class="headerlink" href="#the-naive-bayes-assumptions" title="Permalink to this headline">#</a></h2>
<p>In this section, we talk about some implicit and explicit assumptions of the Naive Bayes model.</p>
<section id="independent-and-identically-distributed-i-i-d">
<h3>Independent and Identically Distributed (i.i.d.)<a class="headerlink" href="#independent-and-identically-distributed-i-i-d" title="Permalink to this headline">#</a></h3>
<p>In supervised learning, implicitly or explicitly, one <em>always</em> assumes that the training set</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathcal{D} &amp;= \left\{\left(\mathbf{x}^{(1)}, y^{(1)}\right), \left(\mathbf{x}^{(2)}, y^{(2)}\right), \cdots, \left(\mathbf{x}^{(N)}, y^{(N)}\right)\right\} \\
\end{aligned}
\end{split}\]</div>
<p>is composed of <span class="math notranslate nohighlight">\(N\)</span> input/response tuples</p>
<div class="math notranslate nohighlight">
\[
\left({\mathbf{X}}^{(n)} = \mathbf{x}^{(n)}, Y^{(n)} = y^{(n)}\right)
\]</div>
<p>that are <em><strong>independently drawn from the same (identical) joint distribution</strong></em></p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}_{\{\mathcal{X}, \mathcal{Y} ; \boldsymbol{\theta}\}}(\mathbf{x}, y)
\]</div>
<p>with</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(\mathbf{X} = \mathbf{x}, Y = y ; \boldsymbol{\theta}) = \mathbb{P}(Y = y \mid \mathbf{X} = \mathbf{x}) \mathbb{P}(\mathbf{X} = \mathbf{x})
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbb{P}(Y = y \mid \mathbf{X} = \mathbf{x})\)</span> is the conditional probability of <span class="math notranslate nohighlight">\(Y\)</span> given <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>,
the relationship that the learner algorithm/concept <span class="math notranslate nohighlight">\(c\)</span> is trying to capture.</p>
<div class="proof definition admonition" id="iid-assumption">
<p class="admonition-title"><span class="caption-number">Definition 64 </span> (The i.i.d. Assumption)</p>
<section class="definition-content" id="proof-content">
<p>Mathematically, this i.i.d. assumption writes (also defined in <a class="reference internal" href="../../../03_discrete_random_variables/iid.html#def_iid">Definition 29</a>):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\left({\mathbf{X}}^{(n)}, Y^{(n)}\right) &amp;\sim \mathbb{P}_{\{\mathcal{X}, \mathcal{Y}, \boldsymbol{\theta}\}}(\mathbf{x}, y) \quad \text{and}\\
\left({\mathbf{X}}^{(n)}, Y^{(n)}\right) &amp;\text{ independent of } \left({\mathbf{X}}^{(m)}, Y^{(m)}\right) \quad \forall n \neq m \in \{1, 2, \ldots, N\}
\end{aligned}
\end{split}\]</div>
<p>and we sometimes denote</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\left(\mathbf{x}^{(n)}, y^{(n)}\right) \overset{\text{i.i.d.}}{\sim} \mathbb{P}_{\{\mathcal{X}, \mathcal{Y}, \boldsymbol{\theta}\}}(\mathbf{x}, y)
\end{aligned}
\]</div>
</section>
</div></section>
<section id="conditional-independence">
<span id="naive-bayes-conditional-independence"></span><h3>Conditional Independence<a class="headerlink" href="#conditional-independence" title="Permalink to this headline">#</a></h3>
<p>The core assumption of the Naive Bayes model is that the predictors <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>
are conditionally independent given the class label <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<p>But how did we arrive at the conditional independence assumption? Let’s look at what we wanted to achieve in the first place.</p>
<p>Recall that our goal in <a class="reference internal" href="#naive-bayes-inference-prediction"><span class="std std-ref">Inference/Prediction</span></a> is to find the class <span class="math notranslate nohighlight">\(k \in \{1, 2, \cdots, K\}\)</span> that maximizes the <strong>posterior</strong> probability
<span class="math notranslate nohighlight">\(\mathbb{P}(Y = k \mid \mathbf{X} = \mathbf{x}^{(q)} ; \boldsymbol{\theta})\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-eq-argmax-naive-bayes-4">
<span class="eqno">(38)<a class="headerlink" href="#equation-eq-argmax-naive-bayes-4" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\hat{y}^{(q)} &amp;= \arg \max_{k} \mathbb{P}(Y = k \mid \mathbf{X} = \mathbf{x}^{(q)} ; \boldsymbol{\theta}) \\
              &amp;= \arg \max_{k} \frac{\mathbb{P}(Y = k, \mathbf{X} = \mathbf{x}^{(q)} ; \boldsymbol{\theta})}{\mathbb{P}(\mathbf{X} = \mathbf{x}^{(q)} ; \boldsymbol{\theta})} \\
              &amp;= \arg \max_{k} \frac{\mathbb{P}(Y = k ; \boldsymbol{\pi}) \mathbb{P}(\mathbf{X} = \mathbf{x}^{(q)} \mid Y = k ; \boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}})}{\mathbb{P}(\mathbf{X} = \mathbf{x}^{(q)} ; \boldsymbol{\theta})}\\
\end{aligned}
\end{split}\]</div>
<p>We have seen earlier in <a class="reference internal" href="#naive-bayes-inference-algorithm">Algorithm 1</a> that since the denominator
is constant for all <span class="math notranslate nohighlight">\(k\)</span>, we can ignore it and just maximize the numerator.</p>
<div class="math notranslate nohighlight" id="equation-eq-argmax-naive-bayes-5">
<span class="eqno">(39)<a class="headerlink" href="#equation-eq-argmax-naive-bayes-5" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\hat{y}^{(q)} &amp;= \arg \max_{k} \mathbb{P}\left(Y = k ; \boldsymbol{\pi}\right) \mathbb{P}\left(\mathbf{X} = \mathbf{x}^{(q)} \mid Y = k ; \boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\right) \\
\end{aligned}
\end{split}\]</div>
<p>This suggests we need to find estimates for both the <strong>prior</strong> and the <strong>likelihood</strong>. This of course
involves us finding the <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\)</span> that maximize the likelihood function<a class="footnote-reference brackets" href="#likelihood-1" id="id2">2</a>, which we will talk about later.</p>
<p>In order to meaningfully optimize the expression, we need to decompose the expression <a class="reference internal" href="#equation-eq-argmax-naive-bayes-5">(39)</a>
into its components that contain the parameters we want to estimate.</p>
<div class="math notranslate nohighlight" id="equation-eq-joint-distribution">
<span class="eqno">(40)<a class="headerlink" href="#equation-eq-joint-distribution" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\mathbb{P}(Y = k ; \boldsymbol{\pi}) \mathbb{P}(\mathbf{X} \mid Y = k ; \boldsymbol{\theta}) &amp;= \mathbb{P}((Y, \mathbf{X}) ; \boldsymbol{\theta}, \boldsymbol{\pi}) \\
&amp;= \mathbb{P}(Y, X_1, X_2, \ldots X_D)
\end{aligned}
\end{split}\]</div>
<p>which is actually the joint distribution of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> and <span class="math notranslate nohighlight">\(Y\)</span><a class="footnote-reference brackets" href="#joint-distribution" id="id3">3</a>.</p>
<p>This joint distribution expression <a class="reference internal" href="#equation-eq-joint-distribution">(40)</a> can be further decomposed by the chain rule of probability<a class="footnote-reference brackets" href="#chain-rule-of-probability" id="id4">4</a> as</p>
<div class="math notranslate nohighlight" id="equation-eq-joint-distribution-decomposed">
<span class="eqno">(41)<a class="headerlink" href="#equation-eq-joint-distribution-decomposed" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\mathbb{P}(Y, X_1, X_2, \ldots X_D) &amp;= \mathbb{P}(Y) \mathbb{P}(X_1, X_2, \ldots X_D \mid Y) \\
&amp;= \mathbb{P}(Y) \mathbb{P}(X_1 \mid Y) \mathbb{P}(X_2 \mid Y, X_1) \cdots \mathbb{P}(X_D \mid Y, X_1, X_2, \ldots X_{D-1}) \\
&amp;= \mathbb{P}(Y) \prod_{d=1}^D \mathbb{P}(X_d \mid Y, X_1, X_2, \ldots X_{d-1}) \\
&amp;= \mathbb{P}(Y) \prod_{d=1}^{D} \mathbb{P}\left(X_d \middle \vert \bigcap_{d'=1}^{d-1} X_{d'}\right) 
\end{aligned}
\end{split}\]</div>
<p>This alone does not get us any further, we still need to estimate roughly <span class="math notranslate nohighlight">\(2^{D}\)</span> parameters<a class="footnote-reference brackets" href="#dparameters" id="id5">5</a>,
which is computationally expensive. Not to forget that we need to estimate for each class <span class="math notranslate nohighlight">\(k \in \{1, 2, 3, \ldots, K\}\)</span>
which has a complexity of <span class="math notranslate nohighlight">\(\sim \mathcal{O}(2^DK)\)</span>.</p>
<div class="proof remark admonition" id="2dparameters">
<p class="admonition-title"><span class="caption-number">Remark 8 </span> (Why <span class="math notranslate nohighlight">\(2^D\)</span> parameters?)</p>
<section class="remark-content" id="proof-content">
<p>Let’s simplify the problem by assuming each feature <span class="math notranslate nohighlight">\(X_d\)</span> and the class label <span class="math notranslate nohighlight">\(Y\)</span> are binary random variables,
i.e. <span class="math notranslate nohighlight">\(X_d \in \{0, 1\}\)</span> and <span class="math notranslate nohighlight">\(Y \in \{0, 1\}\)</span>.</p>
<p>Then <span class="math notranslate nohighlight">\(\mathbb{P}(Y, X_1, X_2, \ldots X_D)\)</span> is a joint distribution of <span class="math notranslate nohighlight">\(D+1\)</span> random variables, each with <span class="math notranslate nohighlight">\(2\)</span> values.</p>
<p>This means the sample space of <span class="math notranslate nohighlight">\(\mathbb{P}(Y, X_1, X_2, \ldots X_D)\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathcal{S} &amp;= \{(0, 1)\} \times \{(0, 1)\} \times \{(0, 1)\} \times \cdots \times \{(0, 1)\} \\
&amp;= \{(0, 0, 0, \ldots, 0), (0, 0, 0, \ldots, 1), (0, 0, 1, \ldots, 0), \ldots, (1, 1, 1, \ldots, 1)\}
\end{aligned}
\end{split}\]</div>
<p>which has <span class="math notranslate nohighlight">\(2^{D+1}\)</span> elements.
To really get the exact joint distribution, we need to estimate the probability of each element in the sample space, which is <span class="math notranslate nohighlight">\(2^{D+1}\)</span> parameters.</p>
<p>This has two caveats:</p>
<ol class="simple">
<li><p>There are too many parameters to estimate, which is computationally expensive. Imagine if <span class="math notranslate nohighlight">\(D\)</span> is 1000, we need to estimate <span class="math notranslate nohighlight">\(2^{1000}\)</span> parameters, which is infeasible.</p></li>
<li><p>Even if we can estimate all the parameters, we are essentially overfitting the data by memorizing the training data. There is no learning involved.</p></li>
</ol>
</section>
</div><p>This is where the “Naive” assumption comes in. The Naive Bayes’ classifier assumes that
the features are conditionally independent<a class="footnote-reference brackets" href="#id15" id="id6">6</a> given the class label.</p>
<p>More formally stated,</p>
<div class="proof definition admonition" id="conditional-independence">
<p class="admonition-title"><span class="caption-number">Definition 65 </span> (Conditional Independence)</p>
<section class="definition-content" id="proof-content">
<div class="math notranslate nohighlight" id="equation-eq-conditional-independence">
<span class="eqno">(42)<a class="headerlink" href="#equation-eq-conditional-independence" title="Permalink to this equation">#</a></span>\[
\mathbb{P}(X_d \mid Y = k, X_{d^{'}}) = \mathbb{P}(X_d \mid Y = k) \quad \text{for all } d \neq d^{'}
\]</div>
</section>
</div><p>with this assumption, we can further simplify expression <a class="reference internal" href="#equation-eq-joint-distribution-decomposed">(41)</a> as</p>
<div class="math notranslate nohighlight" id="equation-eq-conditional-independence-naive-bayes-1">
<span class="eqno">(43)<a class="headerlink" href="#equation-eq-conditional-independence-naive-bayes-1" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\mathbb{P}(Y, X_1, X_2, \ldots X_D) &amp;= \mathbb{P}(Y ; \boldsymbol{\pi}) \prod_{d=1}^D \mathbb{P}(X_d \mid Y ; \theta_{d}) \\
\end{aligned}
\end{split}\]</div>
<p>More precisely, after the simplification in <a class="reference internal" href="#equation-eq-conditional-independence-naive-bayes-1">(43)</a>,
the argmax expression in <a class="reference internal" href="#equation-eq-conditional-naive-bayes">(29)</a> can be written as</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-classifier-1">
<span class="eqno">(44)<a class="headerlink" href="#equation-eq-naive-bayes-classifier-1" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\mathbb{P}(Y = k \mid \mathbf{X} = \mathbf{x} ; \boldsymbol{\theta}) &amp; = \dfrac{\mathbb{P}(Y = k ; \boldsymbol{\pi}) \mathbb{P}(\mathbf{X} \mid Y = k ; \boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}})}{\mathbb{P}(\mathbf{X})} \\
&amp;= \dfrac{\mathbb{P}(Y, X_1, X_2, \ldots X_D)}{\mathbb{P}(\mathbf{X})} \\
&amp;= \dfrac{\mathbb{P}(Y = k ; \boldsymbol{\pi}) \prod_{d=1}^D \mathbb{P}(X_d = x_d \mid Y = k ; \theta_{kd})}{\mathbb{P}(\mathbf{X} = \mathbf{x})} \\
\end{aligned}
\end{split}\]</div>
<p>Consequently, our argmax expression in <a class="reference internal" href="#equation-eq-argmax-naive-bayes-2">(31)</a> can be written as</p>
<div class="math notranslate nohighlight" id="equation-eq-argmax-naive-bayes-6">
<span class="eqno">(45)<a class="headerlink" href="#equation-eq-argmax-naive-bayes-6" title="Permalink to this equation">#</a></span>\[
\arg \max_{k=1}^K \mathbb{P}(Y = k \mid \mathbf{X}) = \arg \max_{k=1}^K \mathbb{P}(Y = k ; \boldsymbol{\pi}) \prod_{d=1}^D \mathbb{P}(X_d = x_d \mid Y = k ; \theta_{kd})
\]</div>
<p>We also make some updates to the vector form <a class="reference internal" href="#equation-eq-argmax-naive-bayes-3">(33)</a> by updating <span class="math notranslate nohighlight">\(\mathbf{M_2}\)</span> to:</p>
<div class="full-width docutils">
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-m2-updated">
<span class="eqno">(46)<a class="headerlink" href="#equation-eq-naive-bayes-m2-updated" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{aligned}
  \mathbf{M_2} &amp;= \begin{bmatrix}
  \mathbb{P}(\mathbf{X} = \mathbf{x} \mid Y = 1) \\
  \mathbb{P}(\mathbf{X} = \mathbf{x} \mid Y = 2) \\
  \vdots \\
  \mathbb{P}(\mathbf{X} = \mathbf{x} \mid Y = K)
  \end{bmatrix}_{K \times 1} \\
  &amp;= \begin{bmatrix}
  \mathbb{P}(X_1 = x_1 \mid Y = 1 ; \theta_{11}) \mathbb{P}(X_2 = x_2 \mid Y = 1 ; \theta_{12}) \cdots \mathbb{P}(X_D = x_D \mid Y = 1 ; \theta_{1D}) \\
  \mathbb{P}(X_1 = x_1 \mid Y = 2 ; \theta_{21}) \mathbb{P}(X_2 = x_2 \mid Y = 2 ; \theta_{22}) \cdots \mathbb{P}(X_D = x_D \mid Y = 2 ; \theta_{2D}) \\
  \vdots \\
  \mathbb{P}(X_1 = x_1 \mid Y = K ; \theta_{K1}) \mathbb{P}(X_2 = x_2 \mid Y = K ; \theta_{K2}) \cdots \mathbb{P}(X_D = x_D \mid Y = K ; \theta_{KD})
  \end{bmatrix}_{K \times 1} \\
\end{aligned}
\end{split}\]</div>
<p>To easily recover each row of <span class="math notranslate nohighlight">\(\mathbf{M_2}\)</span>, it is efficient to define a <span class="math notranslate nohighlight">\(K \times D\)</span> matrix, denoted <span class="math notranslate nohighlight">\(\mathbf{M_3}\)</span></p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-m3-explained">
<span class="eqno">(47)<a class="headerlink" href="#equation-eq-naive-bayes-m3-explained" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{aligned}
  \mathbf{M_3} &amp;= \begin{bmatrix}
  \mathbb{P}(X_1 = x_1 \mid Y = 1 ; \theta_{11}) &amp; \mathbb{P}(X_2 = x_2 \mid Y = 1 ; \theta_{12}) &amp; \cdots &amp; \mathbb{P}(X_D = x_D \mid Y = 1 ; \theta_{1D}) \\
  \mathbb{P}(X_1 = x_1 \mid Y = 2 ; \theta_{21}) &amp; \mathbb{P}(X_2 = x_2 \mid Y = 2 ; \theta_{22}) &amp; \cdots &amp; \mathbb{P}(X_D = x_D \mid Y = 2 ; \theta_{2D}) \\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
  \mathbb{P}(X_1 = x_1 \mid Y = K ; \theta_{K1}) &amp; \mathbb{P}(X_2 = x_2 \mid Y = K ; \theta_{K2}) &amp; \cdots &amp; \mathbb{P}(X_D = x_D \mid Y = K ; \theta_{KD})
  \end{bmatrix}_{K \times D} \\
\end{aligned}
\end{split}\]</div>
</div>
<p>where we can easily recover each row of <span class="math notranslate nohighlight">\(\mathbf{M_2}\)</span> by taking the product of the corresponding row of <span class="math notranslate nohighlight">\(\mathbf{M_3}\)</span>.</p>
</section>
</section>
<section id="parameter-vector">
<h2>Parameter Vector<a class="headerlink" href="#parameter-vector" title="Permalink to this headline">#</a></h2>
<p>In the last section on <a class="reference internal" href="#naive-bayes-conditional-independence"><span class="std std-ref">Conditional Independence</span></a>, we indicated parameters in the expressions.
Here we discuss a little on this newly introduced notation.</p>
<p>Each <span class="math notranslate nohighlight">\(\pi_k\)</span> of <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> refers to the prior probability of class <span class="math notranslate nohighlight">\(k\)</span>, and <span class="math notranslate nohighlight">\(\theta_{kd}\)</span> refers to the parameter of the
class conditional density for class <span class="math notranslate nohighlight">\(k\)</span> and feature <span class="math notranslate nohighlight">\(d\)</span><a class="footnote-reference brackets" href="#kdparameters" id="id7">7</a>. Furthermore,
the boldsymbol <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> is the parameter vector,</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\theta} = \left(\boldsymbol{\pi}, \{\theta_{kd}\}_{k=1, d=1}^{K, D} \right) = \left(\boldsymbol{\pi}, \boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\right)
\]</div>
<div class="proof definition admonition" id="parameter-vector">
<p class="admonition-title"><span class="caption-number">Definition 66 </span> (The Parameter Vector)</p>
<section class="definition-content" id="proof-content">
<p>There is not much to say about the categorical component <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span>, since we are
just estimating the prior probabilities of the classes.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\pi} = \begin{bmatrix} \pi_1 \\ \pi_2 \\ \vdots \\ \pi_K \end{bmatrix}_{K \times 1}
\end{split}\]</div>
<p>The parameter vector (matrix) <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}=\{\theta_{kd}\}_{k=1, d=1}^{K, D}\)</span> is a bit more complicated.
It resides in the <span class="math notranslate nohighlight">\(\mathbb{R}^{K \times D}\)</span> space, where each element <span class="math notranslate nohighlight">\(\theta_{kd}\)</span> is the parameter
associated with feature <span class="math notranslate nohighlight">\(d\)</span> conditioned on class <span class="math notranslate nohighlight">\(k\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}} = \begin{bmatrix} 
\theta_{11} &amp; \theta_{12} &amp; \dots &amp; \theta_{1D} \\
\theta_{21} &amp; \theta_{22} &amp; \dots &amp; \theta_{2D} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\theta_{K1} &amp; \theta_{K2} &amp; \dots &amp; \theta_{KD}
\end{bmatrix}_{K \times D}
\end{split}\]</div>
<p>So if <span class="math notranslate nohighlight">\(K=3\)</span> and <span class="math notranslate nohighlight">\(D=2\)</span>, then the parameter vector <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> is a <span class="math notranslate nohighlight">\(3 \times 2\)</span> matrix, i.e.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}} = \begin{bmatrix}
\theta_{11} &amp; \theta_{12} \\
\theta_{21} &amp; \theta_{22} \\
\theta_{31} &amp; \theta_{32}
\end{bmatrix}_{3 \times 2}
\end{split}\]</div>
<p>This means we have effectively reduced our complexity from <span class="math notranslate nohighlight">\(\sim \mathcal{O}(2^D)\)</span> to <span class="math notranslate nohighlight">\(\sim \mathcal{O}(KD + 1)\)</span>
assuming the same setup in <a class="reference internal" href="#2dparameters">Remark 8</a>.</p>
<p>One big misconception is that the elements in <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\)</span> are scalar values.
This is not true, for example, let’s look at the first entry <span class="math notranslate nohighlight">\(\theta_{11}\)</span>, corresponding to
the parameter of class <span class="math notranslate nohighlight">\(K=1\)</span> and feature <span class="math notranslate nohighlight">\(D=1\)</span>, i.e. <span class="math notranslate nohighlight">\(\theta_{11}\)</span> is the parameter of the class conditional
density <span class="math notranslate nohighlight">\(\mathbb{P}(X_1 \mid Y = 1)\)</span>. Now <span class="math notranslate nohighlight">\(X_1\)</span> can take on any value in <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>, which is indeed a scalar,
we further assume that <span class="math notranslate nohighlight">\(X_1\)</span> takes on a univariate Gaussian distribution, then <span class="math notranslate nohighlight">\(\theta_{11}\)</span> is a vector of length 2, i.e.</p>
<div class="math notranslate nohighlight">
\[
\theta_{11} = \begin{bmatrix} \mu_{11} &amp; \sigma_{11} \end{bmatrix}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu_{11}\)</span> is the mean of the Gaussian distribution and <span class="math notranslate nohighlight">\(\sigma_{11}\)</span> is the standard deviation of the Gaussian distribution.
This is something we need to take note of.</p>
<p><strong>We have also reduced the problem of estimating the joint distribution to just individual conditional distributions.</strong></p>
<p>Overall, before this assumption, you can think of estimating the joint distribution of <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>,
and after this assumption, you can simply individually estimate each conditional distribution.</p>
</section>
</div><p>Notice that the shape of <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> is <span class="math notranslate nohighlight">\(K \times 1\)</span>, and the shape of <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\)</span> is <span class="math notranslate nohighlight">\(K \times D\)</span>.
This corresponds to the shape of the matrix <span class="math notranslate nohighlight">\(\mathbf{M_1}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{M_3}\)</span> as defined in
<a class="reference internal" href="#equation-eq-naive-bayes-m1">(34)</a> and <a class="reference internal" href="#equation-eq-naive-bayes-m3">(36)</a>, respectively. This is expected since
<span class="math notranslate nohighlight">\(\mathbf{M_1}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{M_3}\)</span> hold the PDFs while <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\)</span> hold the parameters
of these PDFs.</p>
</section>
<section id="inductive-bias-distribution-assumptions">
<h2>Inductive Bias (Distribution Assumptions)<a class="headerlink" href="#inductive-bias-distribution-assumptions" title="Permalink to this headline">#</a></h2>
<p>We still need to introduce some inductive bias into <a class="reference internal" href="#equation-eq-naive-bayes-classifier-1">(44)</a>, more concretely, we need to make some assumptions about the distribution
of <span class="math notranslate nohighlight">\(\mathbb{P}(Y)\)</span> and <span class="math notranslate nohighlight">\(\mathbb{P}(X_d \mid Y)\)</span>.</p>
<p>For the target variable, we typically model it as a categorical distribution,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(Y) \sim \mathrm{Categorical}(\boldsymbol{\pi})
\]</div>
<p>For the conditional distribution of the features, we typically model it according to what type of features we have.</p>
<p>For example, if we have binary features, then we can model it as a Bernoulli distribution,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(X_d \mid Y) \sim \mathrm{Bernoulli}(\theta_{dk})
\]</div>
<p>If we have categorical features, then we can model it as a multinomial/catgorical distribution,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(X_d \mid Y) \sim \mathrm{Multinomial}(\boldsymbol{\theta}_{dk})
\]</div>
<p>If we have continuous features, then we can model it as a Gaussian distribution,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(X_d \mid Y) \sim \mathcal{N}(\mu_{dk}, \sigma_{dk}^2)
\]</div>
<p>To reiterate, we want to make some inductive bias assumptions of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> conditional on <span class="math notranslate nohighlight">\(Y\)</span>,
as well as with <span class="math notranslate nohighlight">\(Y\)</span>. Note very carefully that we are not talking about the marginal distribution of
<span class="math notranslate nohighlight">\(\mathbf{X}\)</span> here, instead, we are talking about the conditional distribution of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> given <span class="math notranslate nohighlight">\(Y\)</span>. The distinction is subtle, but important.</p>
<section id="targets-categorical-distribution">
<h3>Targets (Categorical Distribution)<a class="headerlink" href="#targets-categorical-distribution" title="Permalink to this headline">#</a></h3>
<p>As mentioned earlier, both <span class="math notranslate nohighlight">\(Y^{(n)}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{X}^{(n)}\)</span> are random variables/vectors.
This means we need to estimate both of them.</p>
<p>We first conveniently assume that <span class="math notranslate nohighlight">\(Y^{(n)}\)</span> is a discrete random variable, and
follows the <strong><a class="reference external" href="https://en.wikipedia.org/wiki/Categorical_distribution">Category distribution</a></strong><a class="footnote-reference brackets" href="#categorical-distribution" id="id8">8</a>,
an extension of the Bernoulli distribution to multiple classes. Instead of a single parameter <span class="math notranslate nohighlight">\(p\)</span> (probability of success for Bernoulli),
the Category distribution has a vector <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> of <span class="math notranslate nohighlight">\(K\)</span> parameters.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\pi} = \begin{bmatrix} \pi_1 \\ \vdots \\ \pi_K \end{bmatrix}_{K \times 1}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\pi_k\)</span> is the probability of <span class="math notranslate nohighlight">\(Y^{(n)}\)</span> taking on value <span class="math notranslate nohighlight">\(k\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-eq-target-category-distribution">
<span class="eqno">(48)<a class="headerlink" href="#equation-eq-target-category-distribution" title="Permalink to this equation">#</a></span>\[\begin{split}
Y^{(n)} \overset{\small{\text{i.i.d.}}}{\sim} \text{Category}(\boldsymbol{\pi}) \quad \text{where } \boldsymbol{\pi} = \begin{bmatrix} \pi_1 \\ \vdots \\ \pi_K \end{bmatrix}_{K \times 1}
\end{split}\]</div>
<p>Consequently, we just need to estimate <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> to recover <span class="math notranslate nohighlight">\(\mathbf{M_1}\)</span> defined in <a class="reference internal" href="#equation-eq-naive-bayes-m1">(34)</a>.</p>
<p>Equivalently,</p>
<div class="math notranslate nohighlight" id="equation-eq-category-distribution">
<span class="eqno">(49)<a class="headerlink" href="#equation-eq-category-distribution" title="Permalink to this equation">#</a></span>\[
\mathbb{P}(Y^{(n)} = k) = \pi_k \quad \text{for } k = 1, 2, \cdots, K
\]</div>
<div class="proof definition admonition" id="categorical-distribution">
<p class="admonition-title"><span class="caption-number">Definition 67 </span> (Categorical Distribution)</p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(Y\)</span> be a discrete random variable with <span class="math notranslate nohighlight">\(K\)</span> number of states.
Then <span class="math notranslate nohighlight">\(Y\)</span> follows a categorical distribution with parameters <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> if</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(Y = k) = \pi_k \quad \text{for } k = 1, 2, \cdots, K
\]</div>
<p>Consequently, the PMF of the categorical distribution is defined more compactly as,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(Y = k) = \prod_{k=1}^K \pi_k^{I\{Y = k\}}
\]</div>
<p>where <span class="math notranslate nohighlight">\(I\{Y = k\}\)</span> is the indicator function that is equal to 1 if <span class="math notranslate nohighlight">\(Y = k\)</span> and 0 otherwise.</p>
</section>
</div><div class="proof definition admonition" id="categorical-multinomial-distribution">
<p class="admonition-title"><span class="caption-number">Definition 68 </span> (Categorical (Multinomial) Distribution)</p>
<section class="definition-content" id="proof-content">
<p>This formulation is adopted by Bishop’s<span id="id9">[<a class="reference internal" href="../../../references_and_resources/bibliography.html#id3" title="CHRISTOPHER M. BISHOP. Pattern recognition and machine learning. SPRINGER-VERLAG NEW YORK, 2016.">BISHOP, 2016</a>]</span>, the categorical distribution is defined as</p>
<div class="math notranslate nohighlight" id="equation-eq-categorical-distribution-bishop">
<span class="eqno">(50)<a class="headerlink" href="#equation-eq-categorical-distribution-bishop" title="Permalink to this equation">#</a></span>\[
\mathbb{P}(\mathbf{Y} = \mathbf{y}; \boldsymbol{\pi}) = \prod_{k=1}^K \pi_k^{y_k}
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{y} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_K \end{bmatrix}
\end{split}\]</div>
<p>is an one-hot encoded vector of size <span class="math notranslate nohighlight">\(K\)</span>,</p>
<p>The <span class="math notranslate nohighlight">\(y_k\)</span> is the <span class="math notranslate nohighlight">\(k\)</span>-th element of <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>, and is equal to 1 if <span class="math notranslate nohighlight">\(Y = k\)</span> and 0 otherwise.
The <span class="math notranslate nohighlight">\(\pi_k\)</span> is the <span class="math notranslate nohighlight">\(k\)</span>-th element of <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span>, and is the probability of <span class="math notranslate nohighlight">\(Y = k\)</span>.</p>
<p>This notation alongside with the indicator notation in the previous definition allows us to manipulate
the likelihood function easier.</p>
</section>
</div><div class="proof example admonition" id="categorical-distribution-example">
<p class="admonition-title"><span class="caption-number">Example 13 </span> (Categorical Distribution Example)</p>
<section class="example-content" id="proof-content">
<p>Consider rolling a fair six-sided die. Let <span class="math notranslate nohighlight">\(Y\)</span> be the random variable that represents the outcome
of the dice roll. Then <span class="math notranslate nohighlight">\(Y\)</span> follows a categorical distribution with parameters <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> where <span class="math notranslate nohighlight">\(\pi_k = \frac{1}{6}\)</span> for <span class="math notranslate nohighlight">\(k = 1, 2, \cdots, 6\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(Y = k) = \frac{1}{6} \quad \text{for } k = 1, 2, \cdots, 6
\]</div>
<p>For example, if we roll a 3, then <span class="math notranslate nohighlight">\(\mathbb{P}(Y = 3) = \frac{1}{6}\)</span>.</p>
<p>With the more compact notation, the indicator function is <span class="math notranslate nohighlight">\(I\{Y = k\} = 1\)</span> if <span class="math notranslate nohighlight">\(Y = 3\)</span> and <span class="math notranslate nohighlight">\(0\)</span> otherwise. Therefore, the PMF is</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(Y = k) = \prod_{k=1}^6 \frac{1}{6}^{I\{Y = k\}} = \left(\frac{1}{6}\right)^0 \cdot \left(\frac{1}{6}\right)^0 \cdot \left(\frac{1}{6}\right)^1 \cdot \left(\frac{1}{6}\right)^0 \cdot \left(\frac{1}{6}\right)^0 \cdot \left(\frac{1}{6}\right)^0 = \frac{1}{6}
\]</div>
<p>Using Bishop’s notation, the PMF is still the same, only the realization <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> is not a scalar,
but instead a vector of size <span class="math notranslate nohighlight">\(6\)</span>. In the case where <span class="math notranslate nohighlight">\(Y = 3\)</span>, the vector <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{y} = \begin{bmatrix} 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}
\end{split}\]</div>
</section>
</div></section>
<section id="discrete-features-categorical-distribution">
<h3>Discrete Features (Categorical Distribution)<a class="headerlink" href="#discrete-features-categorical-distribution" title="Permalink to this headline">#</a></h3>
<p>Now, our next task is find parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\)</span> to model the conditional distribution of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> given <span class="math notranslate nohighlight">\(Y = k\)</span>,
and consequently, recovering the matrix <span class="math notranslate nohighlight">\(\mathbf{M_3}\)</span> defined in <a class="reference internal" href="#equation-eq-naive-bayes-m3">(36)</a>.</p>
<p>In the case where (all) the features <span class="math notranslate nohighlight">\(X_d\)</span> are categorical (<span class="math notranslate nohighlight">\(D\)</span> number of features),
i.e. <span class="math notranslate nohighlight">\(X_d \in \{1, 2, \cdots, C\}\)</span>,
we can use the categorical distribution to model the (<span class="math notranslate nohighlight">\(D\)</span>-dimensional) conditional
distribution of <span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{D}\)</span> given <span class="math notranslate nohighlight">\(Y = k\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-categorical-feature-1">
<span class="eqno">(51)<a class="headerlink" href="#equation-eq-naive-bayes-categorical-feature-1" title="Permalink to this equation">#</a></span>\[
\begin{align*}
\mathbf{X} \mid Y = k &amp;\overset{\small{\text{i.i.d.}}}{\sim} \text{Category}\left(\boldsymbol{\pi}_{\{\mathbf{X} \mid Y\}}\right) \quad \text{for } k = 1, 2, \cdots, K
\end{align*}
\]</div>
<p>where</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-categorical-feature-2">
<span class="eqno">(52)<a class="headerlink" href="#equation-eq-naive-bayes-categorical-feature-2" title="Permalink to this equation">#</a></span>\[\begin{split}
\boldsymbol{\pi}_{\{\mathbf{X} \mid Y\}} = \begin{bmatrix} \pi_{1, 1} &amp; \dots &amp; \pi_{1, D} \\ \vdots &amp; \ddots &amp; \vdots \\ \pi_{K, 1} &amp; \dots &amp; \pi_{K, D} \end{bmatrix}_{K \times D} \\
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\boldsymbol{\pi}_{\{\mathbf{X} \mid Y\}}\)</span> is a matrix of size <span class="math notranslate nohighlight">\(K \times D\)</span> where each
element <span class="math notranslate nohighlight">\(\pi_{k, d}\)</span> is the parameter for the
probability distribution (PDF) of <span class="math notranslate nohighlight">\(X_d\)</span> given <span class="math notranslate nohighlight">\(Y = k\)</span>.</p>
<div class="math notranslate nohighlight">
\[
X_d \mid Y = k \overset{\small{\text{i.i.d.}}}{\sim} \text{Category}(\pi_{k, d})
\]</div>
<p>Furthermore,
each <span class="math notranslate nohighlight">\(\pi_{k, d}\)</span> is <strong>not a scalar</strong> but a <strong>vector of size <span class="math notranslate nohighlight">\(C\)</span></strong> holding the probability of <span class="math notranslate nohighlight">\(X_d = c\)</span> given <span class="math notranslate nohighlight">\(Y = k\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\pi_{k, d} = \begin{bmatrix} \pi_{k, d, 1} &amp; \dots &amp; \pi_{k, d, C} \end{bmatrix}
\end{align*}
\]</div>
<p>Then the (chained) multi-dimensional conditional PDF of <span class="math notranslate nohighlight">\(\mathbf{X} = \begin{bmatrix} X_1 &amp; \dots &amp; X_D \end{bmatrix}\)</span> given <span class="math notranslate nohighlight">\(Y = k\)</span> is</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-categorical-feature-3">
<span class="eqno">(53)<a class="headerlink" href="#equation-eq-naive-bayes-categorical-feature-3" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\mathbb{P}(\mathbf{X} = \mathbf{x} | Y = k ; \boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}) &amp;= \prod_{d=1}^D \text{Categorical}(X_d \mid Y = k; \pi_{k, d}) \\
&amp;= \prod_{d=1}^D \prod_{c=1}^C \pi_{k, d, c}^{x_{c, d}} \quad \text{for } c = 1, 2, \cdots, C \text{ and } k = 1, 2, \cdots, K
\end{align*}
\end{split}\]</div>
<p>As an example, if <span class="math notranslate nohighlight">\(C=3\)</span>, <span class="math notranslate nohighlight">\(D=2\)</span> and <span class="math notranslate nohighlight">\(K=4\)</span>, then the <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\)</span> is a <span class="math notranslate nohighlight">\(K \times D = 4 \times 2\)</span> matrix, but for
each entry <span class="math notranslate nohighlight">\(\pi_{k, d}\)</span>, is a <span class="math notranslate nohighlight">\(1 \times C\)</span> vector. If one really wants, we can also represent this as a
<span class="math notranslate nohighlight">\(4 \times 2 \times 3\)</span> tensor, especially in the case of implementing it in code.</p>
<p>To be more verbose, when we find</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X} \mid Y = k \overset{\small{\text{i.i.d.}}}{\sim} \text{Category}(\boldsymbol{\pi}_{\{\mathbf{X} \mid Y\}})
\]</div>
<p>we are actually finding for all <span class="math notranslate nohighlight">\(k = 1, 2, \cdots, K\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathbf{X} \mid Y = 1 &amp;\overset{\small{\text{i.i.d.}}}{\sim} \text{Category}(\boldsymbol{\pi}_{\{\mathbf{X} \mid Y=1\}}) \\
\mathbf{X} \mid Y = 2 &amp;\overset{\small{\text{i.i.d.}}}{\sim} \text{Category}(\boldsymbol{\pi}_{\{\mathbf{X} \mid Y=2\}}) \\
\vdots \\
\mathbf{X} \mid Y = K &amp;\overset{\small{\text{i.i.d.}}}{\sim} \text{Category}(\boldsymbol{\pi}_{\{\mathbf{X} \mid Y=K\}})
\end{align*}
\end{split}\]</div>
<p>Each row above corresponds to each row of the matrix <span class="math notranslate nohighlight">\(\mathbf{M_2}\)</span> defined in <a class="reference internal" href="#equation-eq-naive-bayes-m2">(35)</a>. We
can further decompose each <span class="math notranslate nohighlight">\(\mathbf{X} \mid Y = k\)</span> into <span class="math notranslate nohighlight">\(D\)</span> independent random variables, each of which
is modeled by a categorical distribution, thereby recovering each element of <span class="math notranslate nohighlight">\(\mathbf{M_3}\)</span> <a class="reference internal" href="#equation-eq-naive-bayes-m3">(36)</a>.</p>
<p>See <strong>Kevin Murphy’s Probabilistic Machine Learning: An Introduction</strong> pp 358 for more details.</p>
</section>
<section id="continuous-features-gaussian-distribution">
<span id="id10"></span><h3>Continuous Features (Gaussian Distribution)<a class="headerlink" href="#continuous-features-gaussian-distribution" title="Permalink to this headline">#</a></h3>
<p>Here, the task is still the same, to find parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\)</span> to model the conditional distribution of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> given <span class="math notranslate nohighlight">\(Y = k\)</span>,
and consequently, recovering the matrix <span class="math notranslate nohighlight">\(\mathbf{M_3}\)</span> defined in <a class="reference internal" href="#equation-eq-naive-bayes-m3">(36)</a>.</p>
<p>In the case where (all) the features <span class="math notranslate nohighlight">\(X_d\)</span> are continuous (<span class="math notranslate nohighlight">\(D\)</span> number of features),
we can use the Gaussian distribution to model the conditional distribution of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> given <span class="math notranslate nohighlight">\(Y = k\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-continuous-feature-1">
<span class="eqno">(54)<a class="headerlink" href="#equation-eq-naive-bayes-continuous-feature-1" title="Permalink to this equation">#</a></span>\[
\begin{align*}
\mathbf{X} \mid Y = k \overset{\small{\text{i.i.d.}}}{\sim} \mathcal{N}(\theta_{\{\mathbf{X} \mid Y\}}) \quad \text{for } k = 1, 2, \cdots, K
\end{align*}
\]</div>
<p>where</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-continuous-feature-2">
<span class="eqno">(55)<a class="headerlink" href="#equation-eq-naive-bayes-continuous-feature-2" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\theta_{\{\mathbf{X} \mid Y\}} &amp;= \begin{bmatrix} \theta_{1, 1} &amp; \dots &amp; \theta_{1, D} \\ \vdots &amp; \ddots &amp; \vdots \\ \theta_{K, 1} &amp; \dots &amp; \theta_{K, D} \end{bmatrix}_{K \times D} \\
&amp;= \begin{bmatrix} (\mu_{1, 1}, \sigma_{1, 1}^2) &amp; \dots &amp; (\mu_{1, D}, \sigma_{1, D}^2) \\ \vdots &amp; \ddots &amp; \vdots \\ (\mu_{K, 1}, \sigma_{K, 1}^2) &amp; \dots &amp; (\mu_{K, D}, \sigma_{K, D}^2) \end{bmatrix}_{K \times D}
\end{align*}
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\)</span> is a <span class="math notranslate nohighlight">\(K \times D\)</span> matrix, where each element
<span class="math notranslate nohighlight">\(\theta_{k, d}\)</span> is a tuple of the mean and variance of the
Gaussian distribution modeling the conditional distribution of <span class="math notranslate nohighlight">\(X_d\)</span> given <span class="math notranslate nohighlight">\(Y = k\)</span>.</p>
<p>To be more precise, each element in the matrix <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\)</span> is a tuple of the mean and variance of the
Gaussian distribution modeling the conditional distribution of <span class="math notranslate nohighlight">\(X_d\)</span> given <span class="math notranslate nohighlight">\(Y = k\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-continuous-feature-3">
<span class="eqno">(56)<a class="headerlink" href="#equation-eq-naive-bayes-continuous-feature-3" title="Permalink to this equation">#</a></span>\[
\begin{align*}
X_d \mid Y = k &amp;\overset{\small{\text{i.i.d.}}}{\sim} \mathcal{N}(\mu_{k, d}, \sigma_{k, d}^2) \quad \text{for } k = 1, 2, \cdots, K
\end{align*}
\]</div>
<p>Then the (chained) multivariate Gaussian distribution of <span class="math notranslate nohighlight">\(\mathbf{X} = \begin{bmatrix} X_1 &amp; \dots &amp; X_D \end{bmatrix}\)</span> given <span class="math notranslate nohighlight">\(Y = k\)</span> is</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-continuous-feature-4">
<span class="eqno">(57)<a class="headerlink" href="#equation-eq-naive-bayes-continuous-feature-4" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\mathbb{P}\left(\mathbf{X} = \mathbf{x} \mid Y = k ; \boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\right) &amp;= \prod_{d=1}^D \mathcal{N}(x_d \mid \mu_{k, d}, \sigma_{k, d}^2) \\
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu_{k, d}\)</span> and <span class="math notranslate nohighlight">\(\sigma_{k, d}^2\)</span> are the mean and variance of the
Gaussian distribution modeling the conditional distribution of <span class="math notranslate nohighlight">\(X_d\)</span> given <span class="math notranslate nohighlight">\(Y = k\)</span>.</p>
</section>
<section id="mixed-features-discrete-and-continuous">
<h3>Mixed Features (Discrete and Continuous)<a class="headerlink" href="#mixed-features-discrete-and-continuous" title="Permalink to this headline">#</a></h3>
<p>So far we have assumed that each feature <span class="math notranslate nohighlight">\(X_d\)</span> is either all discrete, or all continuous. This
need not be the case, and may not always be the case. In reality, we may have a mixture of both.</p>
<p>For example, if <span class="math notranslate nohighlight">\(X_1\)</span> corresponds to the smoking status of a person (i.e. whether they smoke or not),
then this feature is binary, and can be modeled by a Bernoulli distribution.
On the other hand, if <span class="math notranslate nohighlight">\(X_2\)</span> corresponds to the weight of a person, then this feature is continuous, and can be modeled by a Gaussian distribution.
The nice thing is since within each class <span class="math notranslate nohighlight">\(k\)</span>, the features <span class="math notranslate nohighlight">\(X_d\)</span> are independent of each other, we can model each feature <span class="math notranslate nohighlight">\(X_d\)</span> by its own distribution.</p>
<p>So, carrying over the example above, we have,</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-mixed-feature-1">
<span class="eqno">(58)<a class="headerlink" href="#equation-eq-naive-bayes-mixed-feature-1" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
X_1 \mid Y = k &amp;\overset{\small{\text{i.i.d.}}}{\sim} \text{Bernoulli}(\pi_{\{X_1 \mid Y=k\}}) \\
X_2 \mid Y = k &amp;\overset{\small{\text{i.i.d.}}}{\sim} \text{Gaussian}(\mu_{\{X_2 \mid Y=k\}}, \sigma_{\{X_2 \mid Y=k\}}^2)
\end{align*}
\end{split}\]</div>
<p>and subsequently, the chained PDF is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathbb{P}\left(X_1 = x_1, X_2 = x_2 \mid Y = k ; \boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\right) &amp;= \prod_{d=1}^D \mathbb{P}\left(X_d = x_d \mid Y = k ; \boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\right) \\
&amp;= \mathbb{P}\left(X_1 = x_1 \mid Y = k ; \boldsymbol{\pi}_{\{X_1 \mid Y\}}\right) \mathbb{P}\left(X_2 = x_2 \mid Y = k ; \boldsymbol{\theta}_{\{X_2 \mid Y\}}\right) \\
&amp;= \pi_{\{X_1 \mid Y=k\}}^{x_1} (1 - \pi_{\{X_1 \mid Y=k\}})^{1 - x_1} \mathcal{N}(x_2 \mid \mu_{\{X_2 \mid Y=k\}}, \sigma_{\{X_2 \mid Y=k\}}^2)
\end{align*}
\end{split}\]</div>
<p>See more details in <a class="reference external" href="https://dafriedman97.github.io/mlbook/content/c4/concept.html">Machine Learning from Scratch</a>.</p>
</section>
</section>
<section id="model-fitting">
<h2>Model Fitting<a class="headerlink" href="#model-fitting" title="Permalink to this headline">#</a></h2>
<p>We have so far laid out the model prediction process, the implicit and explicit assumptions, as well as
the model parameters.</p>
<p>Now, we need to figure out how to fit the model parameters to the data. After all, once we
find the model parameters that best fit the data, we can use the model to make predictions
using matrix <span class="math notranslate nohighlight">\(\mathbf{M_1}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{M_3}\)</span> as defined in <a class="reference internal" href="#naive-bayes-inference-algorithm">Algorithm 1</a>.</p>
<section id="fitting-algorithm">
<h3>Fitting Algorithm<a class="headerlink" href="#fitting-algorithm" title="Permalink to this headline">#</a></h3>
<div class="proof algorithm admonition" id="prf:naive-bayes-estimation-algorithm">
<p class="admonition-title"><span class="caption-number">Algorithm 2 </span> (Naive Bayes Estimation Algorithm)</p>
<section class="algorithm-content" id="proof-content">
<p>For each entry in matrix <span class="math notranslate nohighlight">\(\mathbf{M_1}\)</span>, we seek to find its corresponding parameter matrix <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-estimation-1">
<span class="eqno">(59)<a class="headerlink" href="#equation-eq-naive-bayes-estimation-1" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\boldsymbol{\pi} &amp;= \begin{bmatrix} \pi_1 \\ \vdots \\ \pi_K \end{bmatrix}_{K \times 1} \\
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\pi_k\)</span> is the probability of class <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p>For each entry in matrix <span class="math notranslate nohighlight">\(\mathbf{M_3}\)</span>, we seek to find its corresponding parameter matrix <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-estimation-2">
<span class="eqno">(60)<a class="headerlink" href="#equation-eq-naive-bayes-estimation-2" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}} &amp;= \begin{bmatrix} \boldsymbol{\theta}_{\{\mathbf{X} \mid Y=1\}} \\ \vdots \\ \boldsymbol{\theta}_{\{\mathbf{X} \mid Y=K\}} \end{bmatrix}_{K \times D} \\
&amp;= \begin{bmatrix} \theta_{\{X_1 \mid Y=1\}} &amp; \cdots &amp; \theta_{\{X_D \mid Y=1\}} \\ \vdots &amp; \ddots &amp; \vdots \\ \theta_{\{X_1 \mid Y=K\}} &amp; \cdots &amp; \theta_{\{X_D \mid Y=K\}} \end{bmatrix}_{K \times D} \\
&amp;= \begin{bmatrix} \theta_{11} &amp; \cdots &amp; \theta_{1D} \\ \vdots &amp; \ddots &amp; \vdots \\ \theta_{K1} &amp; \cdots &amp; \theta_{KD} \end{bmatrix}_{K \times D} \\
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\theta_{kd}\)</span> is the probability of feature <span class="math notranslate nohighlight">\(X_d\)</span> given class <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p>Both matrices <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\)</span> are estimated by maximizing the likelihood of the data,
using the Maximum Likelihood Estimation (MLE) method.</p>
</section>
</div></section>
<section id="maximum-likelihood-estimation">
<h3>Maximum Likelihood Estimation<a class="headerlink" href="#maximum-likelihood-estimation" title="Permalink to this headline">#</a></h3>
<p>First, read chapter 8.1 of <span id="id11">[<a class="reference internal" href="../../../references_and_resources/bibliography.html#id2" title="Stanley H. Chan. Introduction to probability for Data Science. Michigan Publishing, 2021.">Chan, 2021</a>]</span> for a refresher on MLE.</p>
<div class="proof remark admonition" id="remark-univariate-mle">
<p class="admonition-title"><span class="caption-number">Remark 9 </span> (Univariate Maximum Likelihood Estimation)</p>
<section class="remark-content" id="proof-content">
<p>In <a class="reference external" href="https://dafriedman97.github.io/mlbook/content/c4/concept.html#linear-discriminative-analysis-lda">LDA</a>,
<span class="math notranslate nohighlight">\(\mathbf{X} \mid Y=k\)</span>, the distribution of the features <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> conditioned on <span class="math notranslate nohighlight">\(Y=k\)</span>, has no
assumption of conditional independence. Therefore, we need to estimate the parameters of
<span class="math notranslate nohighlight">\(\mathbf{X} = \{\mathbf{X}_1, \dots, \mathbf{X}_D\}\)</span> jointly.</p>
<p>More concretely,</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\mathbf{X} \mid Y = k \overset{\text{i.i.d.}}{\sim} \mathcal{N}\left(\boldsymbol{\mu}_{\{X \mid Y=k\}}, \boldsymbol{\Sigma}_{\{X \mid Y=k\}}\right) \quad \forall k \in \{1, \dots, K\}
\end{align*}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{\{X \mid Y=k\}}\)</span> is the mean vector of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> given <span class="math notranslate nohighlight">\(Y=k\)</span>, and <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{\{X \mid Y=k\}}\)</span> is the covariance matrix of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> given <span class="math notranslate nohighlight">\(Y=k\)</span>.</p>
<p>However, in the case of Naive Bayes, the assumption of conditional independence allows us to estimate the parameters of <span class="math notranslate nohighlight">\(\mathbf{X} = \{\mathbf{X}_1, \dots, \mathbf{X}_D\}\)</span> univariately,
conditional on <span class="math notranslate nohighlight">\(Y=k\)</span>.</p>
<p>Looking at expression <a class="reference internal" href="#equation-eq-naive-bayes-estimation-2">(60)</a>, we can see that each element
is indeed univariate, and we can estimate the parameters of each element univariately.</p>
</section>
</div><p>Everything we have talked about is just 1 single sample, and that won’t work in the realm of
estimating the best parameters that fit the data. Since we are given a dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>
consisting of <span class="math notranslate nohighlight">\(N\)</span> samples, we can estimate the parameters of the model by maximizing the likelihood of the data.</p>
<div class="proof definition admonition" id="def:naive-bayes-likelihood">
<p class="admonition-title"><span class="caption-number">Definition 69 </span> (Likelihood Function of Naive Bayes)</p>
<section class="definition-content" id="proof-content">
<p>Given <strong>i.i.d.</strong> random variables<a class="footnote-reference brackets" href="#iid-tuple" id="id12">9</a> <span class="math notranslate nohighlight">\(\left(\mathbf{X}^{(1)}, Y^{(1)}\right), \left(\mathbf{X}^{(2)}, Y^{(2)}\right), \dots, \left(\mathbf{X}^{(N)}, Y^{(N)}\right)\)</span>,
we can write the likelihood function (joint probability distribution)
as the product of the individual PDF of each sample<a class="footnote-reference brackets" href="#iid-likelihood" id="id13">10</a>:</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-likelihood-1">
<span class="eqno">(61)<a class="headerlink" href="#equation-eq-naive-bayes-likelihood-1" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\mathcal{L}(\boldsymbol{\theta} \mid \mathcal{D}) \overset{\mathrm{def}}{=} \mathbb{P}(\mathcal{D} ; \boldsymbol{\theta}) &amp;= \mathbb{P}\left(\mathcal{D} ; \left\{\boldsymbol{\pi}, \boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}} \right\}\right) \\
&amp;\overset{(a)}{=} \mathbb{P}\left(\left(\mathbf{X}^{(1)}, Y^{(1)}\right), \left(\mathbf{X}^{(2)}, Y^{(2)}\right), \dots, \left(\mathbf{X}^{(N)}, Y^{(N)}\right) ; \left\{\boldsymbol{\pi}, \boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}} \right\}\right) \\
&amp;\overset{(b)}{=} \prod_{n=1}^N  \mathbb{P}\left(Y^{(n)}=k ; \boldsymbol{\pi}\right) \mathbb{P}\left(\mathrm{X}^{(n)} \mid Y^{(n)} = k ; \boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}} \right)  \\
&amp;\overset{(c)}{=} \prod_{n=1}^N  \left\{\mathbb{P}\left(Y^{(n)}=k ; \boldsymbol{\pi}\right) \prod_{d=1}^D \mathbb{P}\left(X_d^{(n)} \mid Y^{(n)} = k ; \boldsymbol{\theta}_{k, d}\right) \right\} \\
\end{align*}
\end{split}\]</div>
<p>where each <span class="math notranslate nohighlight">\(\left(\mathbf{X}^{(n)}, Y^{(n)}\right)\)</span> in equation <span class="math notranslate nohighlight">\((a)\)</span> is a sample from the dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>
and can be expressed more verbosely as a joint distribution <span class="math notranslate nohighlight">\(\left(\mathbf{X}^{(n)}, Y^{(n)}\right) = \left(\mathbf{X}_1^{(n)}, \dots, \mathbf{X}_D^{(n)}, Y^{(n)}\right)\)</span>
as in <a class="reference internal" href="#equation-eq-joint-distribution">(40)</a>.</p>
<p>Equation <span class="math notranslate nohighlight">\((b)\)</span> is the product of the individual PDF of each sample, where the multiplicand is as in <a class="reference internal" href="#equation-eq-joint-distribution">(40)</a>.</p>
<p>Equation <span class="math notranslate nohighlight">\((c)\)</span> is then a consequence of <a class="reference internal" href="#equation-eq-conditional-independence-naive-bayes-1">(43)</a>.</p>
</section>
</div><p>Then we can maximize</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-likelihood-target">
<span class="eqno">(62)<a class="headerlink" href="#equation-eq-naive-bayes-likelihood-target" title="Permalink to this equation">#</a></span>\[
\prod_{n=1}^N  \mathbb{P}\left(Y^{(n)}=k ; \boldsymbol{\pi}\right)
\]</div>
<p>and</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-likelihood-feature">
<span class="eqno">(63)<a class="headerlink" href="#equation-eq-naive-bayes-likelihood-feature" title="Permalink to this equation">#</a></span>\[
\prod_{n=1}^N  \prod_{d=1}^D \mathbb{P}\left(X_d^{(n)} \middle \vert Y^{(n)} = k ; \boldsymbol{\theta}_{k, d}\right)
\]</div>
<p>individually since the above can be decomposed<a class="footnote-reference brackets" href="#decomposed-likelihood" id="id14">11</a>.</p>
<div class="proof definition admonition" id="def:naive-bayes-log-likelihood">
<p class="admonition-title"><span class="caption-number">Definition 70 </span> (Log Likelihood Function of Naive Bayes)</p>
<section class="definition-content" id="proof-content">
<p>For numerical stability, we can take the log of the likelihood function:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\log \mathcal{L}(\boldsymbol{\theta} \mid \mathcal{D}) &amp;= \log \mathbb{P}\left(\mathcal{D} ; \left\{\boldsymbol{\pi}, \boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}} \right\}\right) \\
\end{align*}
\end{split}\]</div>
<p>where the log of the product of the individual PDF of each sample is the sum of the log of each PDF. We
will go into that later.</p>
</section>
</div><p>Stated formally,</p>
<div class="proof definition admonition" id="def:naive-bayes-max-priors">
<p class="admonition-title"><span class="caption-number">Definition 71 </span> (Maximize Priors)</p>
<section class="definition-content" id="proof-content">
<p>The notation for maximizing the prior probabilities is as follows:</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-max-priors">
<span class="eqno">(64)<a class="headerlink" href="#equation-eq-naive-bayes-max-priors" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\hat{\boldsymbol{\pi}} &amp;= \arg \max_{\boldsymbol{\pi}} \mathcal{L}(\boldsymbol{\pi} ; \mathcal{D}) \\
&amp;= \arg \max_{\boldsymbol{\pi}} \prod_{n=1}^N  \mathbb{P}\left(Y^{(n)}=k ; \boldsymbol{\pi}\right) \\
\end{align*}
\end{split}\]</div>
<p>A reminder that the shape of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\pi}}\)</span> is <span class="math notranslate nohighlight">\(K \times 1\)</span>.</p>
</section>
</div><p>Similarly, we can maximize the likelihood function of the feature parameters:</p>
<div class="proof definition admonition" id="def:naive-bayes-max-feature-params">
<p class="admonition-title"><span class="caption-number">Definition 72 </span> (Maximize Feature Parameters)</p>
<section class="definition-content" id="proof-content">
<p>The notation for maximizing the feature parameters is as follows:</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-max-feature-params">
<span class="eqno">(65)<a class="headerlink" href="#equation-eq-naive-bayes-max-feature-params" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\hat{\boldsymbol{\theta}}_{\{\mathbf{X} \mid Y\}} &amp;= \arg \max_{\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}} \mathcal{L}(\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}} ; \mathcal{D}) \\
&amp;= \arg \max_{\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}} \prod_{n=1}^N  \prod_{d=1}^D \mathbb{P}\left(X_d^{(n)} \mid Y^{(n)} = k ; \boldsymbol{\theta}_{k, d}\right) \\
\end{align*}
\end{split}\]</div>
<p>A reminder that the shape of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\theta}}_{\{\mathbf{X} \mid Y\}}\)</span> is <span class="math notranslate nohighlight">\(K \times D\)</span>.</p>
</section>
</div></section>
<section id="estimating-priors">
<h3>Estimating Priors<a class="headerlink" href="#estimating-priors" title="Permalink to this headline">#</a></h3>
<p>Before we start the formal estimation process, it is intuitive to think that the prior probabilities <span class="math notranslate nohighlight">\(\boldsymbol{\pi}_k\)</span> should be proportional to the number of samples in each class. In other words, if we have <span class="math notranslate nohighlight">\(N_1\)</span> samples in class 1, <span class="math notranslate nohighlight">\(N_2\)</span> samples in class 2, and so on, then we should have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\pi_1 &amp;\propto N_1 \\
\pi_2 &amp;\propto N_2 \\
\vdots &amp; \quad \vdots \\
\pi_K &amp;\propto N_K
\end{align*}
\end{split}\]</div>
<p>For instance, if we have a dataset with <span class="math notranslate nohighlight">\(N=100\)</span> samples with <span class="math notranslate nohighlight">\(K=3\)</span> classes, and <span class="math notranslate nohighlight">\(N_1 = 10\)</span>, <span class="math notranslate nohighlight">\(N_2 = 30\)</span> and <span class="math notranslate nohighlight">\(N_3 = 60\)</span>, then we should have <span class="math notranslate nohighlight">\(\pi_1 = \frac{10}{100} = 0.1\)</span>, <span class="math notranslate nohighlight">\(\pi_2 = \frac{30}{100} = 0.3\)</span> and <span class="math notranslate nohighlight">\(\pi_3 = \frac{60}{100} = 0.6\)</span>. This is just the relative frequency of each class and
seems to be a sensible choice.</p>
<p>It turns out our intuition matches the formal estimation process derived from the maximum likelihood estimation (MLE) principle.</p>
</section>
<section id="maximum-likelihood-estimation-for-priors-categorical-distribution">
<h3>Maximum Likelihood Estimation for Priors (Categorical Distribution)<a class="headerlink" href="#maximum-likelihood-estimation-for-priors-categorical-distribution" title="Permalink to this headline">#</a></h3>
<p>We have seen earlier that we can maximize the priors and likelihood (target and feature parameters) separately.</p>
<p>Let’s start with the priors. Let’s state the expression from <a class="reference internal" href="#equation-eq-naive-bayes-max-priors">(64)</a> in definition
<a class="reference internal" href="#def:naive-bayes-max-priors">Definition 71</a> again:</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-max-priors-repeated">
<span class="eqno">(66)<a class="headerlink" href="#equation-eq-naive-bayes-max-priors-repeated" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\hat{\boldsymbol{\pi}} &amp;= \arg \max_{\boldsymbol{\pi}} \mathcal{L}(\boldsymbol{\pi} ; \mathcal{D}) \\
&amp;= \arg \max_{\boldsymbol{\pi}} \prod_{n=1}^N  \mathbb{P}\left(Y^{(n)}=k ; \boldsymbol{\pi}\right) \\
\end{align*}
\end{split}\]</div>
<p>We need to write the multiplicand in <a class="reference internal" href="#equation-eq-naive-bayes-max-priors-repeated">(66)</a> in terms of
the PDF of the Category distribution, as decribed in <a class="reference internal" href="#equation-eq-categorical-distribution-bishop">(50)</a>.
Extending from <a class="reference internal" href="#equation-eq-naive-bayes-max-priors-repeated">(66)</a>, we have:</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-max-priors-2">
<span class="eqno">(67)<a class="headerlink" href="#equation-eq-naive-bayes-max-priors-2" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\hat{\boldsymbol{\pi}} &amp;= \arg \max_{\boldsymbol{\pi}} \mathcal{L}(\boldsymbol{\pi} ; \mathcal{D}) \\
&amp;= \arg \max_{\boldsymbol{\pi}} \prod_{n=1}^N  \mathbb{P}\left(Y^{(n)}=k ; \boldsymbol{\pi}\right) \\
&amp;\overset{\mathrm{(a)}}{=} \arg \max_{\boldsymbol{\pi}} \prod_{n=1}^N  \left(\prod_{k=1}^K \pi_k^{y^{(n)}_k}\right) \\
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\left(\prod_{k=1}^K \pi_k^{y^{(n)}_k} \right)\)</span> in equation <span class="math notranslate nohighlight">\((a)\)</span> is a consequence
of the definition of the Category distribution in <a class="reference internal" href="#categorical-multinomial-distribution">Definition 68</a>.</p>
<p>Subsequently, knowing maximizing the log likelihood is the same as maximizing the likelihood, we have:</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-max-priors-3">
<span class="eqno">(68)<a class="headerlink" href="#equation-eq-naive-bayes-max-priors-3" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\hat{\boldsymbol{\pi}} &amp;= \arg \max_{\boldsymbol{\pi}} \mathcal{L}(\boldsymbol{\pi} ; \mathcal{D}) \\
&amp;= \arg \max_{\boldsymbol{\pi}} \log \mathcal{L}(\boldsymbol{\pi} ; \mathcal{D}) \\
&amp;\overset{\mathrm{(b)}}{=} \arg \max_{\boldsymbol{\pi}} \sum_{n=1}^N \log \left(\prod_{k=1}^K \pi_k^{y^{(n)}_k} \right) \\
&amp;\overset{\mathrm{(c)}}{=} \arg \max_{\boldsymbol{\pi}} \sum_{n=1}^N \sum_{k=1}^K y^{(n)}_k \log \pi_k \\
&amp;\overset{\mathrm{(d)}}{=} \arg \max_{\boldsymbol{\pi}} \sum_{k=1}^K N_k \log \pi_k \\
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(N_k\)</span> is the number of samples that belong to the <span class="math notranslate nohighlight">\(k\)</span>-th category.</p>
<div class="proof remark admonition" id="notation-overload">
<p class="admonition-title"><span class="caption-number">Remark 10 </span> (Notation Overload)</p>
<section class="remark-content" id="proof-content">
<p>We note to ourselves that we are reusing, and hence abusing the notation <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> for the log-likelihood function to be the same as the likelihood function, this is just for the ease of re-defining a new symbol for the log-likelihood function, <span class="math notranslate nohighlight">\(\log \mathcal{L}\)</span>.</p>
</section>
</div><p>Equation <span class="math notranslate nohighlight">\((b)\)</span> is derived because placing the logarithm outside the product is equivalent to summing the logarithms of the terms in the product.</p>
<p>Equation <span class="math notranslate nohighlight">\((d)\)</span> is derived by expanding equation <span class="math notranslate nohighlight">\((c)\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\sum_{n=1}^N \sum_{k=1}^K y^{(n)}_k \log \pi_k &amp;= \sum_{n=1}^N \left( \sum_{k=1}^K y^{(n)}_k \log \pi_k \right) \\
&amp;= y^{(1)}_1 \log \pi_1 + y^{(1)}_2 \log \pi_2 + \dots + y^{(1)}_K \log \pi_K \\
&amp;+ y^{(2)}_1 \log \pi_1 + y^{(2)}_2 \log \pi_2 + \dots + y^{(2)}_K \log \pi_K \\
&amp;+ \qquad \vdots \qquad \\
&amp;+ y^{(N)}_1 \log \pi_1 + y^{(N)}_2 \log \pi_2 + \dots + y^{(N)}_K \log \pi_K \\
&amp;\overset{(e)}{=} \left( y^{(1)}_1 + y^{(2)}_1 + \dots + y^{(N)}_1 \right) \log \pi_1 \\
&amp;+ \left( y^{(1)}_2 + y^{(2)}_2 + \dots + y^{(N)}_2 \right) \log \pi_2 \\
&amp;+ \qquad \vdots \qquad \\
&amp;+ \left( y^{(1)}_K + y^{(2)}_K + \dots + y^{(N)}_K \right) \log \pi_K \\
&amp;\overset{(f)}{=} N_1 \log \pi_1 + N_2 \log \pi_2 + \dots + N_K \log \pi_K \\
&amp;= \sum_{k=1}^K N_k \log \pi_k \\
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\((e)\)</span> is derived by summing each column, and <span class="math notranslate nohighlight">\(N_k = y^{(1)}_k + y^{(2)}_k + \dots + y^{(N)}_k\)</span>
is nothing but the number of samples that belong to the <span class="math notranslate nohighlight">\(k\)</span>-th category. One just need to recall that
if we have say 6 samples of class <span class="math notranslate nohighlight">\((0, 1, 2, 0, 1, 1)\)</span> where <span class="math notranslate nohighlight">\(K=3\)</span>, then the one-hot encoded
representation of the samples will be</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\left[
\begin{array}{ccc}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
\end{array}
\right]
\end{align*}
\end{split}\]</div>
<p>and summing each column will give us <span class="math notranslate nohighlight">\(N_1 = 2\)</span>, <span class="math notranslate nohighlight">\(N_2 = 3\)</span>, and <span class="math notranslate nohighlight">\(N_3 = 1\)</span>.</p>
<p>Now we are finally ready to solve the estimation (optimization) problem for <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-max-priors-4">
<span class="eqno">(69)<a class="headerlink" href="#equation-eq-naive-bayes-max-priors-4" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\hat{\boldsymbol{\pi}} &amp;= \underset{\boldsymbol{\pi}}{\mathrm{argmax}} ~~ \sum_{k=1}^K N_k \log \pi_k \\
\end{align*}
\end{split}\]</div>
<p>subject to the constraint that</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-max-priors-constraint">
<span class="eqno">(70)<a class="headerlink" href="#equation-eq-naive-bayes-max-priors-constraint" title="Permalink to this equation">#</a></span>\[
\sum_{k=1}^K \pi_k = 1
\]</div>
<p>which is just saying the probabilities must sum up to 1.</p>
<p>We can also write the expression as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\max_{\boldsymbol{\pi}} &amp;~~ \sum_{k=1}^K N_k \log \pi_k \\
\text{subject to} &amp;~~ \sum_{k=1}^K \pi_k = 1
\end{aligned}
\end{split}\]</div>
<p>This is a constrained optimization problem, and we can solve it using the Lagrangian method.</p>
<div class="proof definition admonition" id="lagrangian-method">
<p class="admonition-title"><span class="caption-number">Definition 73 </span> (Lagrangian Method)</p>
<section class="definition-content" id="proof-content">
<p>The Lagrangian method is a method to solve constrained optimization problems. The idea is to
convert the constrained optimization problem into an unconstrained optimization problem by
introducing a Lagrangian multiplier <span class="math notranslate nohighlight">\(\lambda\)</span> and then solve the unconstrained optimization
problem.</p>
<p>Given a function <span class="math notranslate nohighlight">\(f(\mathrm{x})\)</span> and a constraint <span class="math notranslate nohighlight">\(g(\mathrm{x}) = 0\)</span>, the Lagrangian function,
<span class="math notranslate nohighlight">\(\mathcal{L}(\mathrm{x}, \lambda)\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathcal{L}(\mathrm{x}, \lambda) &amp;= f(\mathrm{x}) - \lambda g(\mathrm{x}) \\
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda\)</span> is the Lagrangian multiplier and may be either positive or negative. Then,
the critical points of the Lagrangian function are the same as the critical points of the
original constrained optimization problem, i.e. setting the gradient vector of the Lagrangian
function <span class="math notranslate nohighlight">\(\nabla \mathcal{L}(\mathrm{x}, \lambda) = 0\)</span> with respect to <span class="math notranslate nohighlight">\(\mathrm{x}\)</span> and <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
</section>
</div><p>One note is that the notation of <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> seems to be overloaded again with the Lagrangian function, we will have to change it to <span class="math notranslate nohighlight">\(\mathcal{L}_\lambda\)</span> to avoid confusion. So, to reiterate, solving the Lagrangian function is equivalent to solving the constrained optimization problem.</p>
<p>In our problem, we can convert it to Lagrangian form as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\hat{\boldsymbol{\pi}} &amp;= \underset{\boldsymbol{\pi}}{\mathrm{argmax}} ~~ \mathcal{L}_\lambda(\boldsymbol{\pi}, \lambda ; \mathcal{D}) \\
&amp;= \underset{\boldsymbol{\pi}}{\mathrm{argmax}} ~~ \underbrace{\mathcal{L}(\boldsymbol{\pi} ; \mathcal{D})}_{f(\boldsymbol{\pi})} - \lambda \left(\underbrace{\sum_{k=1}^K \pi_k - 1}_{g(\boldsymbol{\pi})} \right) \\
&amp;= \underset{\boldsymbol{\pi}}{\mathrm{argmax}} ~~ \sum_{k=1}^K N_k \log \pi_k - \lambda \left( \sum_{k=1}^K \pi_k - 1 \right) \\
\end{align*}
\end{split}\]</div>
<p>which is now an unconstrained optimization problem.</p>
<p>We can now solve it by setting the gradient vector of the Lagrangian function</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-max-priors-lagrangian-1">
<span class="eqno">(71)<a class="headerlink" href="#equation-eq-naive-bayes-max-priors-lagrangian-1" title="Permalink to this equation">#</a></span>\[
\nabla \mathcal{L}_\lambda(\boldsymbol{\pi}, \lambda ; \mathcal{D}) = 0
\]</div>
<p>with respect to <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> and <span class="math notranslate nohighlight">\(\lambda\)</span>, as follows,</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-max-priors-lagrangian-2">
<span class="eqno">(72)<a class="headerlink" href="#equation-eq-naive-bayes-max-priors-lagrangian-2" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\nabla \mathcal{L}_\lambda(\boldsymbol{\pi}, \lambda ; \mathcal{D}) &amp;\overset{\mathrm{def}}{=} \frac{\partial \mathcal{L}_\lambda(\boldsymbol{\pi}, \lambda ; \mathcal{D})}{\partial \boldsymbol{\pi}} = 0 \quad \text{and} \quad \frac{\partial \mathcal{L}_\lambda(\boldsymbol{\pi}, \lambda ; \mathcal{D})}{\partial \lambda} = 0 \\
&amp;\iff \frac{\partial}{\partial \boldsymbol{\pi}} \left( \sum_{k=1}^K N_k \log \pi_k - \lambda \left( \sum_{k=1}^K \pi_k - 1 \right) \right) = 0 \quad \text{and} \quad \frac{\partial \mathcal{L}_\lambda(\boldsymbol{\pi}, \lambda ; \mathcal{D})}{\partial \lambda} = 0 \\
\\
&amp;\iff \begin{bmatrix} \frac{\partial \mathcal{L}_\lambda}{\partial \pi_1} \\ \vdots \\ \frac{\partial \mathcal{L}_\lambda}{\partial \pi_K} \end{bmatrix} = \begin{bmatrix} 0 \\ \vdots \\ 0 \end{bmatrix} \quad \text{and} \quad \frac{\partial \mathcal{L}_\lambda(\boldsymbol{\pi}, \lambda ; \mathcal{D})}{\partial \lambda} = 0 \\
&amp;\iff \begin{bmatrix} \frac{\partial}{\partial \pi_1} \left( N_1 \log \pi_1 - \lambda \left( \pi_1 - 1 \right) \right) \\ \vdots \\ \frac{\partial}{\partial \pi_K} \left( N_K \log \pi_K - \lambda \left( \pi_K - 1 \right) \right) \end{bmatrix} = \begin{bmatrix} 0 \\ \vdots \\ 0 \end{bmatrix} \quad \text{and} \quad \frac{\partial \mathcal{L}_\lambda(\boldsymbol{\pi}, \lambda ; \mathcal{D})}{\partial \lambda} = 0 \\
&amp;\iff \begin{bmatrix} \frac{N_1}{\pi_1} - \lambda \\ \vdots \\ \frac{N_K}{\pi_K} - \lambda \end{bmatrix} = \begin{bmatrix} 0 \\ \vdots \\ 0 \end{bmatrix} \quad \text{and} \quad \sum_{k=1}^K \pi_k - 1 = 0 \\
\end{align*}
\end{split}\]</div>
<p>The reason we can unpack <span class="math notranslate nohighlight">\(\frac{\partial}{\partial \pi_k}\left( \sum_{k=1}^K N_k \log \pi_k - \lambda \left( \sum_{k=1}^K \pi_k - 1 \right) \right)\)</span> as <span class="math notranslate nohighlight">\(\frac{\partial}{\partial \pi_k} \left( N_k \log \pi_k - \lambda \left( \pi_k - 1 \right) \right)\)</span> is because we are dealing with partial derivatives, so other terms other than <span class="math notranslate nohighlight">\(\pi_k\)</span> are constant.</p>
<p>Finally, we have a system of equations for each <span class="math notranslate nohighlight">\(\pi_k\)</span> and if we can solve for <span class="math notranslate nohighlight">\(\pi_k\)</span> for each <span class="math notranslate nohighlight">\(k\)</span>, we can then find the best estimate of <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span>. It turns out to solve for <span class="math notranslate nohighlight">\(\pi_k\)</span>, we have to find <span class="math notranslate nohighlight">\(\lambda\)</span> first, and this can be solved by setting <span class="math notranslate nohighlight">\(\sum_{k=1}^K \pi_k - 1 = 0\)</span> and solving for <span class="math notranslate nohighlight">\(\lambda\)</span>, which is the last equation in the system of equations above. We first express each <span class="math notranslate nohighlight">\(\pi_k\)</span> in terms of <span class="math notranslate nohighlight">\(\lambda\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\frac{N_1}{\pi_1} - \lambda &amp;= 0 \implies \pi_1 = \frac{N_1}{\lambda} \\
\frac{N_2}{\pi_2} - \lambda &amp;= 0 \implies \pi_2 = \frac{N_2}{\lambda} \\
\vdots \\
\frac{N_K}{\pi_K} - \lambda &amp;= 0 \implies \pi_K = \frac{N_K}{\lambda} \\
\end{align*}
\end{split}\]</div>
<p>Then we substitute these expressions into the last equation in the system of equations above, and solve for <span class="math notranslate nohighlight">\(\lambda\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\sum_{k=1}^K \pi_k - 1 = 0 &amp;\implies \sum_{k=1}^K \frac{N_k}{\lambda} - 1 = 0 \\
&amp;\implies \sum_{k=1}^K \frac{N_k}{\lambda} = 1 \\
&amp;\implies \sum_{k=1}^K N_k = \lambda \\
&amp;\implies \lambda = \sum_{k=1}^K N_k \\
&amp;\implies \lambda = N \\
\end{align*}
\end{split}\]</div>
<p>and therefore, we can now solve for <span class="math notranslate nohighlight">\(\pi_k\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\hat{\pi}} = \begin{bmatrix} 
\pi_1 = \frac{N_1}{N} \\ \pi_2 = \frac{N_2}{N} \\ \vdots \\ \pi_K = \frac{N_K}{N} 
\end{bmatrix}_{K \times 1}
\implies \pi_k = \frac{N_k}{N} \quad \text{for} \quad k = 1, 2, \ldots, K
\end{split}\]</div>
<p>We conclude that the maximum likelihood estimate of <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span>
is the same as the empirical relative frequency of each class in the training data. This coincides with our intuition.</p>
<p>For completeness of expression,</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-max-priors-final">
<span class="eqno">(73)<a class="headerlink" href="#equation-eq-naive-bayes-max-priors-final" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\hat{\boldsymbol{\pi}} &amp;= \arg \max_{\boldsymbol{\pi}} \mathcal{L}\left( \boldsymbol{\pi} ; \mathcal{D} \right) \\
&amp;= \begin{bmatrix} \hat{\pi}_1 \\ \vdots \\ \hat{\pi}_K \end{bmatrix} \\
&amp;= \begin{bmatrix} \frac{N_1}{N} \\ \vdots \\ \frac{N_K}{N} \end{bmatrix}
\end{align*}
\end{split}\]</div>
</section>
<section id="estimating-likelihood-gaussian-version">
<h3>Estimating Likelihood (Gaussian Version)<a class="headerlink" href="#estimating-likelihood-gaussian-version" title="Permalink to this headline">#</a></h3>
<p>Intuition: The likelihood parameters are the mean and variance of each feature for each class.</p>
</section>
<section id="maximum-likelihood-estimate-for-likelihood-continuous-feature-parameters">
<h3>Maximum Likelihood Estimate for Likelihood (Continuous Feature Parameters)<a class="headerlink" href="#maximum-likelihood-estimate-for-likelihood-continuous-feature-parameters" title="Permalink to this headline">#</a></h3>
<p>Now that we have found the maximum likelihood estimate for the prior probabilities,
we now find the maximum likelihood estimate for the likelihood parameters.</p>
<p>Let’s look at the expression <a class="reference internal" href="#equation-eq-naive-bayes-max-feature-params">(65)</a> from <a class="reference internal" href="#def:naive-bayes-max-feature-params">Definition 72</a> again:</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-max-feature-params-repeated">
<span class="eqno">(74)<a class="headerlink" href="#equation-eq-naive-bayes-max-feature-params-repeated" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\hat{\boldsymbol{\theta}}_{\{\mathbf{X} \mid Y\}} &amp;= \arg \max_{\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}} \mathcal{L}(\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}} ; \mathcal{D}) \\
&amp;= \arg \max_{\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}} \prod_{n=1}^N  \prod_{d=1}^D \mathbb{P}\left(X_d^{(n)} \middle \vert Y^{(n)} = k ; \boldsymbol{\theta}_{k, d} \right) \\
\end{align*}
\end{split}\]</div>
<p>We will give a formulation for the case when all features <span class="math notranslate nohighlight">\(X_d\)</span> are continuous. As mentioned<br />
in <a class="reference internal" href="#continuous-features-gaussian-distribution"><span class="std std-ref">Continuous Features (Gaussian Distribution)</span></a>, we will assume that the features <span class="math notranslate nohighlight">\(X_d\)</span> given class <span class="math notranslate nohighlight">\(Y=k\)</span>
are distributed according to a Gaussian distribution.</p>
<div class="warning admonition">
<p class="admonition-title">Hand Wavy</p>
<p>This section will be a bit hand wavy as I did not derive it by hand, but one just need to remember we need
to find a total of <span class="math notranslate nohighlight">\(K \times D\)</span> parameters.</p>
</div>
<p>Before we write the multiplicand in <a class="reference internal" href="#equation-eq-naive-bayes-max-feature-params-repeated">(74)</a> in terms of the PDF
of the Gaussian distribution, we will follow Kevin Murphy’s method (pp 329) and represent</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-max-feature-params-kevin-murphy-1">
<span class="eqno">(75)<a class="headerlink" href="#equation-eq-naive-bayes-max-feature-params-kevin-murphy-1" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\arg \max_{\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}} \prod_{n=1}^N  \prod_{d=1}^D \mathbb{P}\left(X_d^{(n)} \middle \vert Y^{(n)} = k ; \boldsymbol{\theta}_{k, d} \right) &amp;= \arg \max_{\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}} \prod_{n=1}^N  \prod_{k=1}^K \prod_{d=1}^D \mathbb{P}\left(X_d^{(n)} \middle \vert Y^{(n)} = k ; \boldsymbol{\theta}_{k, d} \right)^{I\left\{ Y^{(n)} = k \right\}} \\
\end{align*}
\end{split}\]</div>
<p>Then he applied the log function to both sides of <a class="reference internal" href="#equation-eq-naive-bayes-max-feature-params-kevin-murphy-1">(75)</a>,</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-max-feature-params-kevin-murphy-2">
<span class="eqno">(76)<a class="headerlink" href="#equation-eq-naive-bayes-max-feature-params-kevin-murphy-2" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\arg \max_{\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}} \log \left( \prod_{n=1}^N  \prod_{k=1}^K \prod_{d=1}^D \mathbb{P}\left(X_d^{(n)} \middle \vert Y^{(n)} = k ; \boldsymbol{\theta}_{k, d} \right)^{I\left\{ Y^{(n)} = k \right\}} \right) &amp;= \arg \max_{\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}} \sum_{n=1}^N  \sum_{k=1}^K \sum_{d=1}^D I\left\{ Y^{(n)} = k \right\} \log \left( \mathbb{P}\left(X_d^{(n)} \middle \vert Y^{(n)} = k ; \boldsymbol{\theta}_{k, d} \right) \right) \\
&amp;= \arg \max_{\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}} \sum_{k=1}^K \sum_{d=1}^D \left [\sum_{n=1: Y^{(n)} = k}^N \log \left(\mathbb{P}\left(X_d^{(n)} \middle \vert Y = k ; \boldsymbol{\theta}_{k, d} \right) \right)\right]\\
\end{align*}
\end{split}\]</div>
<p>where the notation <span class="math notranslate nohighlight">\(n=1: Y^{(n)} = k\)</span> means that we are summing over all <span class="math notranslate nohighlight">\(n\)</span> where <span class="math notranslate nohighlight">\(Y^{(n)} = k\)</span>. In other words,
we are looking at all the data points where the class label is <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p>We can further simplify <a class="reference internal" href="#equation-eq-naive-bayes-max-feature-params-kevin-murphy-2">(76)</a> as:</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-max-feature-params-kevin-murphy-3">
<span class="eqno">(77)<a class="headerlink" href="#equation-eq-naive-bayes-max-feature-params-kevin-murphy-3" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\arg \max_{\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}} \sum_{k=1}^K \sum_{d=1}^D \left [\sum_{n=1: Y^{(n)} = k}^N \log \left(\mathbb{P}\left(X_d^{(n)} \middle \vert Y = k ; \boldsymbol{\theta}_{k, d} \right) \right)\right] &amp;= \arg \max_{\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}} \sum_{k=1}^K \sum_{d=1}^D \log \mathbb{P}\left(\mathcal{D}_{dk} ; \boldsymbol{\theta}_{k, d} \right) \\
&amp;= \arg \max_{\boldsymbol{\theta}_{k, d}} \log \mathbb{P}\left(\mathcal{D}_{11} ; \boldsymbol{\theta}_{1, 1} \right) + \log \mathbb{P}\left(\mathcal{D}_{12} ; \boldsymbol{\theta}_{1, 2} \right) + \cdots + \log \mathbb{P}\left(\mathcal{D}_{1D} ; \boldsymbol{\theta}_{1, D} \right) + \log \mathbb{P}\left(\mathcal{D}_{21} ; \boldsymbol{\theta}_{2, 1} \right) + \log \mathbb{P}\left(\mathcal{D}_{22} ; \boldsymbol{\theta}_{2, 2} \right) + \cdots + \log \mathbb{P}\left(\mathcal{D}_{2D} ; \boldsymbol{\theta}_{2, D} \right) + \cdots + \log \mathbb{P}\left(\mathcal{D}_{K1} ; \boldsymbol{\theta}_{K, 1} \right) + \log \mathbb{P}\left(\mathcal{D}_{K2} ; \boldsymbol{\theta}_{K, 2} \right) + \cdots + \log \mathbb{P}\left(\mathcal{D}_{KD} ; \boldsymbol{\theta}_{K, D} \right) \\
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{D}_{dk}\)</span> is the data set of all the data points of feature <span class="math notranslate nohighlight">\(d\)</span> and class <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p>Now we can <em><strong>individually maximize</strong></em> the parameters of each feature and class pair, i.e. estimate <span class="math notranslate nohighlight">\(\mathcal{D}_{dk}\)</span> for each <span class="math notranslate nohighlight">\(d\)</span> and <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p>For example, <span class="math notranslate nohighlight">\(\mathcal{D}_{12}\)</span> refers to all the data points of feature <span class="math notranslate nohighlight">\(1\)</span> and class <span class="math notranslate nohighlight">\(2\)</span>.</p>
<p>In terms of earlier in a similar vein from Definition 72 (Maximize Feature Parameters), but now instead of
multiplying the probabilities, we are summing the log probabilities.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\arg \max_{\theta_{1, 2}} \log \mathbb{P}\left(\mathcal{D}_{12} ; \theta_{1, 2} \right) &amp;= \arg \max_{\theta_{1, 2}} \sum_{n=1: Y^{(n)} = 2}^N \log \left(\mathbb{P}\left(X_1^{(n)} \middle \vert Y = 2 ; \theta_{1, 2} \right) \right) \\
\end{align*}
\end{split}\]</div>
<p>where we will attempt to find the best estimate <span class="math notranslate nohighlight">\(\theta_{1, 2} = \left(\mu_{2, 1}, \sigma_{2, 1} \right)\)</span> for the parameters of the Gaussian distribution of feature <span class="math notranslate nohighlight">\(1\)</span> and class <span class="math notranslate nohighlight">\(2\)</span>.</p>
<p>It turns out that the maximum likelihood estimate for the parameters of the Gaussian distribution is the sample mean and sample variance of the data set <span class="math notranslate nohighlight">\(\mathcal{D}_{12}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\arg \max_{\theta_{1, 2}} \log \mathbb{P}\left(\mathcal{D}_{12} ; \theta_{1, 2} \right) &amp;= \begin{bmatrix} \hat{\mu_{2, 1}} \\ \hat{\sigma}_{2, 1} \end{bmatrix} \\
&amp;= \begin{bmatrix} \frac{1}{N_2} \sum_{n=1}^{N_2} x_1^{(n)} \\ \sqrt{\frac{1}{N_2} \sum_{n=1}^{N_2} \left( x_1^{(n)} - \hat{\mu}_{2, 1} \right)^2} \end{bmatrix} \\
&amp;= \begin{bmatrix} \bar{x}_{2, 1} \\ s_{2, 1} \end{bmatrix} \\
\end{align*}
\end{split}\]</div>
<p>and for the general form</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\arg \max_{\theta_{k, d}} \log \mathbb{P}\left(\mathcal{D}_{dk} ; \theta_{k, d} \right) &amp;= \begin{bmatrix} \hat{\mu_{k, d}} \\ \hat{\sigma}_{k, d} \end{bmatrix} \\
&amp;= \begin{bmatrix} \frac{1}{N_k} \sum_{n=1}^{N_k} x_d^{(n)} \\ \sqrt{\frac{1}{N_k} \sum_{n=1}^{N_k} \left( x_d^{(n)} - \hat{\mu}_{k, d} \right)^2} \end{bmatrix} \\
&amp;= \begin{bmatrix} \bar{x}_{k, d} \\ s_{k, d} \end{bmatrix} \\
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{x}_{k, d}\)</span> is the sample mean of the data set <span class="math notranslate nohighlight">\(\mathcal{D}_{dk}\)</span> and <span class="math notranslate nohighlight">\(s_{k, d}\)</span> is the sample standard deviation of the data set <span class="math notranslate nohighlight">\(\mathcal{D}_{dk}\)</span>.</p>
<p>For completeness, the parameter matrix <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\)</span> defined in <a class="reference internal" href="#equation-eq-naive-bayes-estimation-2">(60)</a> becomes:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}} &amp;= \begin{bmatrix} \boldsymbol{\theta}_{11} &amp; \boldsymbol{\theta}_{12} &amp; \cdots &amp; \boldsymbol{\theta}_{1D} \\ \boldsymbol{\theta}_{21} &amp; \boldsymbol{\theta}_{22} &amp; \cdots &amp; \boldsymbol{\theta}_{2D} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \boldsymbol{\theta}_{K1} &amp; \boldsymbol{\theta}_{K2} &amp; \cdots &amp; \boldsymbol{\theta}_{KD} \end{bmatrix} \\
&amp;= \begin{bmatrix} \left(\bar{x}_{1, 1}, s_{1, 1} \right) &amp; \left(\bar{x}_{1, 2}, s_{1, 2} \right) &amp; \cdots &amp; \left(\bar{x}_{1, D}, s_{1, D} \right) \\ \left(\bar{x}_{2, 1}, s_{2, 1} \right) &amp; \left(\bar{x}_{2, 2}, s_{2, 2} \right) &amp; \cdots &amp; \left(\bar{x}_{2, D}, s_{2, D} \right) \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \left(\bar{x}_{K, 1}, s_{K, 1} \right) &amp; \left(\bar{x}_{K, 2}, s_{K, 2} \right) &amp; \cdots &amp; \left(\bar{x}_{K, D}, s_{K, D} \right) \end{bmatrix} \\
\end{align*}
\end{split}\]</div>
<hr class="docutils" />
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
&amp;= \arg \max_{\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}} \sum_{k=1}^K \sum_{d=1}^D \left [\sum_{n=1: Y^{(n)} = k}^N \log \left(\frac{1}{\sqrt{2 \pi \sigma_{k, d}^2}} \exp \left( -\frac{1}{2 \sigma_{k, d}^2} \left( X_d^{(n)} - \mu_{k, d} \right)^2 \right) \right)\right] \\
\end{align*}
\end{split}\]</div>
<p>See derivations from section 4.2.5 and 4.2.6 of Probabilistic Machine Learning: An Introduction by Kevin Murphy
for the univariate and multivariate Gaussian case respectively.</p>
</section>
</section>
<section id="brain-dump">
<h2>Brain dump<a class="headerlink" href="#brain-dump" title="Permalink to this headline">#</a></h2>
<p>Brain dump</p>
<ol class="simple">
<li><p>The data points <span class="math notranslate nohighlight">\(\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \ldots, \mathbf{x}^{(N)}\)</span> are <strong>i.i.d.</strong> (independent and identically distributed) realizations from the random variables (random vectors) <span class="math notranslate nohighlight">\(\mathbf{X}^{(1)}, \mathbf{X}^{(2)}, \ldots, \mathbf{X}^{(N)}\)</span>.</p></li>
<li><p>Each <span class="math notranslate nohighlight">\(\mathbf{X}^{(n)}\)</span> is a random vector, i.e. <span class="math notranslate nohighlight">\(\mathbf{X}^{(n)} \in \mathbb{R}^{D}\)</span>, where <span class="math notranslate nohighlight">\(D\)</span> is the dimensionality of the feature space. This means that <span class="math notranslate nohighlight">\(\mathbf{X}^{(n)}\)</span> can be characterized by an <span class="math notranslate nohighlight">\(D\)</span>-dimensional PDF <span class="math notranslate nohighlight">\(f_{\mathbf{X}^{(n)}}(\mathbf{x}^{(n)}) = f_{X_1^{(n)}, X_2^{(n)}, \ldots, X_D^{(n)}}(x_1^{(n)}, x_2^{(n)}, \ldots, x_D^{(n)})\)</span>.</p></li>
</ol>
<p>This means <span class="math notranslate nohighlight">\(\mathbf{X}^{(n)}\)</span> is a multi-dimensional joint distribution,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{X}^{(n)} \in \mathbb{R}^{D} = \begin{bmatrix} X_1^{(n)} \\ X_2^{(n)} \\ \vdots \\ X_D^{(n)} \end{bmatrix}
\end{aligned}
\end{split}\]</div>
<p>and can be characterized by an <span class="math notranslate nohighlight">\(D\)</span>-dimensional PDF</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
f_{\mathbf{X}^{(n)}}(\mathbf{x}^{(n)}) = f_{\mathbf{X}^{(n)}}\left(\mathbf{x}^{(n)}\right) = f_{X_1^{(n)}, X_2^{(n)}, \ldots, X_D^{(n)}}\left(x_1^{(n)}, x_2^{(n)}, \ldots, x_D^{(n)}\right)
\end{aligned}
\]</div>
<p>For example, if <span class="math notranslate nohighlight">\(D = 3\)</span>, and the realizations of <span class="math notranslate nohighlight">\(\mathbf{X}^{(n)}\)</span> are <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)} = (22, 80, 1)\)</span>,
then <span class="math notranslate nohighlight">\(f_{\mathbf{X}^{(n)}}\left(\mathbf{x}^{(n)}\right)\)</span> is the probability density of observing this 3-dimensional vector in <span class="math notranslate nohighlight">\(\mathbb{R}^{3}\)</span>, within the
sample space <span class="math notranslate nohighlight">\(\Omega_{\mathbf{X}} = \mathbb{R}  \times \mathbb{R} \times \mathbb{R}\)</span>.</p>
<p>The confusion in the <strong>i.i.d.</strong> assumption is that we are not talking about the individual random variables
<span class="math notranslate nohighlight">\(X_1^{(n)}, X_2^{(n)}, \ldots, X_D^{(n)}\)</span> here, but the entire random vector <span class="math notranslate nohighlight">\(\mathbf{X}^{(n)}\)</span>.</p>
<p>This means there is no assumption of <span class="math notranslate nohighlight">\(X_1^{(n)}, X_2^{(n)}, \ldots, X_D^{(n)}\)</span> being <strong>i.i.d.</strong>. Instead, the samples
<span class="math notranslate nohighlight">\(\mathbf{X}^{(1)}, \mathbf{X}^{(2)}, \ldots, \mathbf{X}^{(N)}\)</span> are <strong>i.i.d.</strong>.</p>
<ol class="simple">
<li><p>More confusion, is it iid with label <span class="math notranslate nohighlight">\(Y=k\)</span>, seems like it is together since we can indeed decompose
the <span class="math notranslate nohighlight">\(\mathbb{P}(Y=k \mid \mathbf{X} = \mathbf{x})\)</span> into proportional <span class="math notranslate nohighlight">\(\mathbb{P}(Y=k) \mathbb{P}(\mathbf{X} = \mathbf{x} \mid Y = k)\)</span>.</p></li>
</ol>
<p>In SE post, discriminative model do not need make assumptions of X and therefore they may or may not be i.i.d.</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="http://jrmeyer.github.io/machinelearning/2017/08/18/mle.html">http://jrmeyer.github.io/machinelearning/2017/08/18/mle.html</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=XtUNwVrWnPM">https://www.youtube.com/watch?v=XtUNwVrWnPM</a></p></li>
<li></li>
</ul>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="intractable"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Cite Dive into Deep Learning on this. Also, the joint probability is intractable because the number of parameters to estimate is exponential in the number of features. Use binary bits example, see my notes.</p>
</dd>
<dt class="label" id="likelihood-1"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Not to be confused with the likelihood term <span class="math notranslate nohighlight">\(\mathbb{P}(\mathbf{X} \mid Y)\)</span> in Bayes’ terminology.</p>
</dd>
<dt class="label" id="joint-distribution"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Joint_probability_distribution#Discrete_case">Joint Probability Distribution</a></p>
</dd>
<dt class="label" id="chain-rule-of-probability"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Chain_rule_(probability)">Chain Rule of Probability</a></p>
</dd>
<dt class="label" id="dparameters"><span class="brackets"><a class="fn-backref" href="#id5">5</a></span></dt>
<dd><p>Dive into Deep Learning, Section 22.9, this is only assuming that each feature <span class="math notranslate nohighlight">\(\mathbf{x}_d^{(n)}\)</span> is binary, i.e. <span class="math notranslate nohighlight">\(\mathbf{x}_d^{(n)} \in \{0, 1\}\)</span>.</p>
</dd>
<dt class="label" id="id15"><span class="brackets"><a class="fn-backref" href="#id6">6</a></span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Conditional_independence">Conditional Independence</a></p>
</dd>
<dt class="label" id="kdparameters"><span class="brackets"><a class="fn-backref" href="#id7">7</a></span></dt>
<dd><p>Probablistic Machine Learning: An Introduction, Section 9.3, pp 328</p>
</dd>
<dt class="label" id="categorical-distribution"><span class="brackets"><a class="fn-backref" href="#id8">8</a></span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Categorical_distribution">Category Distribution</a></p>
</dd>
<dt class="label" id="iid-tuple"><span class="brackets"><a class="fn-backref" href="#id12">9</a></span></dt>
<dd><p><span class="math notranslate nohighlight">\(\left(\mathbf{X}^{(n)}, Y^{(1)}\right)\)</span> is written as a tuple, when in fact they can be considered 1 single variable.</p>
</dd>
<dt class="label" id="iid-likelihood"><span class="brackets"><a class="fn-backref" href="#id13">10</a></span></dt>
<dd><p>Refer to page 470 of <span id="id16">[<a class="reference internal" href="../../../references_and_resources/bibliography.html#id2" title="Stanley H. Chan. Introduction to probability for Data Science. Michigan Publishing, 2021.">Chan, 2021</a>]</span>. Note that we cannot write it as a product if the data is not independent and identically distributed.</p>
</dd>
<dt class="label" id="decomposed-likelihood"><span class="brackets"><a class="fn-backref" href="#id14">11</a></span></dt>
<dd><p>Cite Kevin Murphy and Bishop.</p>
</dd>
</dl>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./machine_learning_algorithms/generative/naive_bayes"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../../../05_joint_distributions/braindump.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Brain Dump</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="naive_bayes_example_penguin.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Example: Penguins Dataset</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Gao Hongnan<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>