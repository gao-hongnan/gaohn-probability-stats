
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Naive Bayes Example &#8212; Probability &amp; Statistics</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script src="../../../_static/tabs.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"defeq": "\\overset{\\text{def}}{=}", "defa": "\\overset{\\text{(a)}}{=}", "defb": "\\overset{\\text{(b)}}{=}", "defc": "\\overset{\\text{(c)}}{=}", "defd": "\\overset{\\text{(d)}}{=}", "st": "\\mid", "mod": "\\mid", "S": "\\Omega", "s": "\\omega", "e": "\\exp", "P": "\\mathbb{P}", "R": "\\mathbb{R}", "expectation": "\\mathbb{E}", "v": "\\mathbf{v}", "a": "\\mathbf{a}", "b": "\\mathbf{b}", "c": "\\mathbf{c}", "u": "\\mathbf{u}", "w": "\\mathbf{w}", "x": "\\mathbf{x}", "y": "\\mathbf{y}", "z": "\\mathbf{z}", "0": "\\mathbf{0}", "1": "\\mathbf{1}", "A": "\\mathbf{A}", "B": "\\mathbf{B}", "C": "\\mathbf{C}", "E": "\\mathcal{F}", "lset": "\\left\\{", "rset": "\\right\\}", "lsq": "\\left[", "rsq": "\\right]", "lpar": "\\left(", "rpar": "\\right)", "lcurl": "\\left\\{", "rcurl": "\\right\\}", "pmf": "p_X", "pdf": "f_X", "pdftwo": "f_{X,Y}", "pdfjoint": "f_{\\mathbf{X}}", "cdf": "F_X", "pspace": "(\\Omega, \\mathcal{F}, \\mathbb{P})", "var": "\\operatorname{Var}", "std": "\\operatorname{Std}", "bern": "\\operatorname{Bernoulli}", "binomial": "\\operatorname{Binomial}", "geometric": "\\operatorname{Geometric}", "poisson": "\\operatorname{Poisson}", "uniform": "\\operatorname{Uniform}", "normal": "\\operatorname{Normal}", "gaussian": "\\operatorname{Gaussian}", "gaussiansymbol": "\\mathcal{N}", "exponential": "\\operatorname{Exponential}", "iid": "\\textbf{i.i.d.}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Bibliography" href="../../../references_and_resources/bibliography.html" />
    <link rel="prev" title="Naive Bayes" href="naive_bayes.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Probability & Statistics</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Chapter 1. Mathematical Preliminaries
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../01_mathematical_preliminaries/01_combinatorics.html">
   Permutations and Combinations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../01_mathematical_preliminaries/02_calculus.html">
   Calculus
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../01_mathematical_preliminaries/exercises.html">
   Exercises
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Chapter 2. Probability
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../02_probability/0202_probability_space.html">
   Probability Space
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../02_probability/0203_probability_axioms.html">
   Probability Axioms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../02_probability/0204_conditional_probability.html">
   Conditional Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../02_probability/0205_independence.html">
   Independence
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../02_probability/0206_bayes_theorem.html">
   Baye’s Theorem and the Law of Total Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../02_probability/summary.html">
   Summary
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Chapter 3. Discrete Random Variables
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../03_discrete_random_variables/0301_random_variables.html">
   Random Variables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../03_discrete_random_variables/0302_discrete_random_variables.html">
   Discrete Random Variables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../03_discrete_random_variables/0303_probability_mass_function.html">
   Probability Mass Function
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../03_discrete_random_variables/0304_cumulative_distribution_function.html">
   Cumulative Distribution Function
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../03_discrete_random_variables/0305_expectation.html">
   Expectation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../03_discrete_random_variables/0306_moments_and_variance.html">
   Moments and Variance
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../03_discrete_random_variables/0307_discrete_uniform_distribution_concept.html">
   Discrete Uniform Distribution
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../03_discrete_random_variables/0307_discrete_uniform_distribution_implementation.html">
     Implementation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../03_discrete_random_variables/0308_bernoulli_distribution_concept.html">
   Bernoulli Distribution
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../03_discrete_random_variables/0308_bernoulli_distribution_implementation.html">
     Implementation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../03_discrete_random_variables/iid.html">
   Independent and Identically Distributed (IID)
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../03_discrete_random_variables/0309_binomial_distribution_concept.html">
   Binomial Distribution
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../03_discrete_random_variables/0309_binomial_distribution_implementation.html">
     Implementation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../03_discrete_random_variables/0309_binomial_distribution_application.html">
     Real World Examples
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../03_discrete_random_variables/0310_geometric_distribution_concept.html">
   Geometric Distribution
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../03_discrete_random_variables/0311_poisson_distribution_concept.html">
   Poisson Distribution
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../03_discrete_random_variables/0311_poisson_distribution_implementation.html">
     Implementation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../03_discrete_random_variables/summary.html">
   Important
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../03_discrete_random_variables/exercises.html">
   Exercises
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Chapter 4. Continuous Random Variables
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../04_continuous_random_variables/from_discrete_to_continuous.html">
   From Discrete to Continuous
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../04_continuous_random_variables/0401_continuous_random_variables.html">
   Continuous Random Variables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../04_continuous_random_variables/0402_probability_density_function.html">
   Probability Density Function
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../04_continuous_random_variables/0403_expectation.html">
   Expectation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../04_continuous_random_variables/0404_moments_and_variance.html">
   Moments and Variance
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../04_continuous_random_variables/0405_cumulative_distribution_function.html">
   Cumulative Distribution Function
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../04_continuous_random_variables/0406_mean_median_mode.html">
   Mean, Median and Mode
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../04_continuous_random_variables/0407_continuous_uniform_distribution.html">
   Continuous Uniform Distribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../04_continuous_random_variables/0408_exponential_distribution.html">
   Exponential Distribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../04_continuous_random_variables/0409_gaussian_distribution.html">
   Gaussian Distribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../04_continuous_random_variables/0410_skewness_and_kurtosis.html">
   Skewness and Kurtosis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../04_continuous_random_variables/0411_convolve_and_sum_of_random_variables.html">
   Convolution and Sum of Random Variables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../04_continuous_random_variables/0412_functions_of_random_variables.html">
   Functions of Random Variables
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Chapter 5. Joint Distributions
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../05_joint_distributions/braindump.html">
   Brain Dump
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Machine Learning Algorithms
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="naive_bayes_concept.html">
   Naive Bayes Concept
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="naive_bayes.html">
   Naive Bayes
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Naive Bayes Example
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  References and Resources
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../references_and_resources/bibliography.html">
   Bibliography
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../references_and_resources/resources.html">
   Resources
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/gao-hongnan/gaohn-probability-stats/main?urlpath=tree/content/machine_learning_algorithms/generative/naive_bayes/naive_bayes_example.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/gao-hongnan/gaohn-probability-stats/blob/main/content/machine_learning_algorithms/generative/naive_bayes/naive_bayes_example.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/gao-hongnan/gaohn-probability-stats"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/gao-hongnan/gaohn-probability-stats/issues/new?title=Issue%20on%20page%20%2Fmachine_learning_algorithms/generative/naive_bayes/naive_bayes_example.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../../_sources/machine_learning_algorithms/generative/naive_bayes/naive_bayes_example.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#concept">
   Concept
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#discriminative-vs-generative">
     Discriminative vs Generative
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inference-prediction">
     Inference/Prediction
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fitting-estimating">
     Fitting/Estimating
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#estimation">
     Estimation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#estimating-priors">
       Estimating Priors
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#penguins-dataset">
   Penguins Dataset
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-prior">
   The Prior
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#classifying-one-penguin">
   Classifying one penguin
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#one-categorical-feature">
     One Categorical Feature
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#one-quantitative-predictor">
     One Quantitative Predictor
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#two-predictors">
     Two Predictors
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#plots">
       Plots!!!
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#conditional-independence">
       Conditional Independence
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#three-predictors">
     Three Predictors
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Naive Bayes Example</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#concept">
   Concept
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#discriminative-vs-generative">
     Discriminative vs Generative
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inference-prediction">
     Inference/Prediction
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fitting-estimating">
     Fitting/Estimating
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#estimation">
     Estimation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#estimating-priors">
       Estimating Priors
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#penguins-dataset">
   Penguins Dataset
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-prior">
   The Prior
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#classifying-one-penguin">
   Classifying one penguin
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#one-categorical-feature">
     One Categorical Feature
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#one-quantitative-predictor">
     One Quantitative Predictor
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#two-predictors">
     Two Predictors
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#plots">
       Plots!!!
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#conditional-independence">
       Conditional Independence
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#three-predictors">
     Three Predictors
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="naive-bayes-example">
<h1>Naive Bayes Example<a class="headerlink" href="#naive-bayes-example" title="Permalink to this headline">#</a></h1>
<section id="concept">
<h2>Concept<a class="headerlink" href="#concept" title="Permalink to this headline">#</a></h2>
<section id="discriminative-vs-generative">
<h3>Discriminative vs Generative<a class="headerlink" href="#discriminative-vs-generative" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>We will add a subscript <span class="math notranslate nohighlight">\(n\)</span> to denote the <span class="math notranslate nohighlight">\(n\)</span>-th sample from the dataset. It still is referring to a single sample.</p></li>
<li><p>Discriminative classifiers model the conditional distribution <span class="math notranslate nohighlight">\(\mathbb{P}(Y_n = k \mid X_n =  \mathrm{x}_n)\)</span>. This means we are modelling the conditional distribution of the target <span class="math notranslate nohighlight">\(Y_n\)</span> given the input <span class="math notranslate nohighlight">\(\mathrm{x}_n\)</span>.</p></li>
<li><p>Generative classifiers model the conditional distribution <span class="math notranslate nohighlight">\(\mathbb{P}(X_n = \mathrm{x}_n \mid Y_n = k)\)</span>. This means we are modelling the conditional distribution of the input <span class="math notranslate nohighlight">\(\mathrm{x}_n\)</span> given the target <span class="math notranslate nohighlight">\(Y_n\)</span>. Then we can use Bayes’ rule to compute the conditional distribution of the target <span class="math notranslate nohighlight">\(Y_n\)</span> given the input <span class="math notranslate nohighlight">\(\mathrm{x}_n\)</span>.</p></li>
<li><p>Both the target <span class="math notranslate nohighlight">\(Y_n\)</span> and the input <span class="math notranslate nohighlight">\(X_n\)</span> are random variables in the generative model. In the discriminative model, only the target <span class="math notranslate nohighlight">\(Y_n\)</span> is a random variable as the input <span class="math notranslate nohighlight">\(X_n\)</span> is fixed (we do not need to estimate anything about the input <span class="math notranslate nohighlight">\(X\)</span>).</p></li>
</ul>
</section>
<section id="inference-prediction">
<h3>Inference/Prediction<a class="headerlink" href="#inference-prediction" title="Permalink to this headline">#</a></h3>
<p>Suppose the problem at hand has <span class="math notranslate nohighlight">\(K\)</span> classes, <span class="math notranslate nohighlight">\(k = 1, 2, \cdots, K\)</span>, where <span class="math notranslate nohighlight">\(k\)</span> is the index of the class.</p>
<p>Then, to find the class of a new test sample <span class="math notranslate nohighlight">\(\mathrm{x}_q\)</span>, we can compute the conditional probability of each class <span class="math notranslate nohighlight">\(Y = k\)</span> given the sample <span class="math notranslate nohighlight">\(\mathrm{x}_q\)</span>:</p>
<div class="proof algorithm admonition" id="naive-bayes-inference-algorithm">
<p class="admonition-title"><span class="caption-number">Algorithm 1 </span> (Naive Bayes Inference Algorithm)</p>
<section class="algorithm-content" id="proof-content">
<ol>
<li><p>Compute the conditional probability of each class <span class="math notranslate nohighlight">\(Y = k\)</span> given the sample <span class="math notranslate nohighlight">\(\mathrm{x}_q\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-conditional-naive-bayes">
<span class="eqno">(34)<a class="headerlink" href="#equation-eq-conditional-naive-bayes" title="Permalink to this equation">#</a></span>\[
    \mathbb{P}(Y = k \mid X = \mathrm{x}_q) = \dfrac{\mathbb{P}(X = \mathrm{x}_q \mid Y = k) \mathbb{P}(Y = k)}{\mathbb{P}(X = \mathrm{x}_q)} \quad \text{for } k = 1, 2, \cdots, K
    \]</div>
</li>
<li><p>Choose the class <span class="math notranslate nohighlight">\(k\)</span> that maximizes the conditional probability:</p>
<div class="math notranslate nohighlight" id="equation-eq-argmax-naive-bayes-1">
<span class="eqno">(35)<a class="headerlink" href="#equation-eq-argmax-naive-bayes-1" title="Permalink to this equation">#</a></span>\[
    \hat{y}_q = \arg\max_{k=1}^K \mathbb{P}(Y = k \mid X = \mathrm{x}_q)
    \]</div>
</li>
</ol>
<p>The observant reader would have noticed that the normalizing constant <span class="math notranslate nohighlight">\(\mathbb{P}(X = \mathrm{x}_q)\)</span> is the same for all <span class="math notranslate nohighlight">\(k\)</span>. Therefore, we can ignore it and simply choose the class <span class="math notranslate nohighlight">\(k\)</span> that maximizes the numerator of the conditional probability.</p>
<div class="math notranslate nohighlight" id="equation-eq-argmax-naive-bayes-2">
<span class="eqno">(36)<a class="headerlink" href="#equation-eq-argmax-naive-bayes-2" title="Permalink to this equation">#</a></span>\[
\hat{y}_q = \arg\max_{k=1}^K \mathbb{P}(X = \mathrm{x}_q \mid Y = k) \mathbb{P}(Y = k)
\]</div>
<p>since <span class="math notranslate nohighlight">\(\mathbb{P}(Y = k \mid X = \mathrm{x}_q) \propto \mathbb{P}(X = \mathrm{x}_q \mid Y = k) \mathbb{P}(Y = k)\)</span>
by a constant factor <span class="math notranslate nohighlight">\(\mathbb{P}(X = \mathrm{x}_q)\)</span>.</p>
</section>
</div><p>Now if we just proceed to estimate the conditional probability <span class="math notranslate nohighlight">\(\mathbb{P}(Y = k \mid X = \mathrm{x}_q)\)</span>, we will need to estimate the joint probability <span class="math notranslate nohighlight">\(\mathbb{P}(X = \mathrm{x}_q, Y = k)\)</span>, which is intractable<a class="footnote-reference brackets" href="#id4" id="id1">1</a>.</p>
<p>However, if we can <em><strong>estimate</strong></em> the conditional probability (likelihood) <span class="math notranslate nohighlight">\(\mathbb{P}(X = \mathrm{x}_q \mid Y = k)\)</span> and the prior probability <span class="math notranslate nohighlight">\(\mathbb{P}(Y = k)\)</span>, then we can use Bayes’ rule to compute the posterior conditional probability <span class="math notranslate nohighlight">\(\mathbb{P}(Y = k \mid X = \mathrm{x}_q)\)</span>.</p>
</section>
<section id="fitting-estimating">
<h3>Fitting/Estimating<a class="headerlink" href="#fitting-estimating" title="Permalink to this headline">#</a></h3>
<p>This section’s content is adapted from <a class="reference external" href="https://dafriedman97.github.io/mlbook/content/c4/concept.html">Machine Learning from Scratch</a>.</p>
<p>Similarly, we also make some inductive bias assumptions of <span class="math notranslate nohighlight">\(\mathrm{X}_n\)</span> conditional on <span class="math notranslate nohighlight">\(Y_n\)</span>. Note very carefully that we are not talking about the marginal distribution of <span class="math notranslate nohighlight">\(\mathrm{X}_n\)</span> here, instead, we are talking about the conditional distribution of <span class="math notranslate nohighlight">\(\mathrm{X}_n\)</span> given <span class="math notranslate nohighlight">\(Y_n\)</span>. The distinction is subtle, but important.</p>
<p>In general, we assume all samples <span class="math notranslate nohighlight">\(X_n\)</span> come from the same <em>family</em> of distributions. This means that for samples <span class="math notranslate nohighlight">\(n = 1\)</span> to <span class="math notranslate nohighlight">\(n = N\)</span>, and class <span class="math notranslate nohighlight">\(k = 1\)</span> to <span class="math notranslate nohighlight">\(k = K\)</span>, we have the following:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathrm{X}_n|(Y_n = 1) &amp;\sim \text{Distribution}(\boldsymbol{\theta}_1) \\
\mathrm{X}_n|(Y_n = 2) &amp;\sim \text{Distribution}(\boldsymbol{\theta}_2) \\
\vdots &amp; \quad \vdots \\
\mathrm{X}_n|(Y_n = K) &amp;\sim \text{Distribution}(\boldsymbol{\theta}_K)
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_k\)</span> is the parameter vector of <span class="math notranslate nohighlight">\(\mathrm{X}_n\)</span> conditioned on the <span class="math notranslate nohighlight">\(k\)</span>-th class. For instance, if we are using a Multivariate Gaussian distribution, then <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_k = \begin{bmatrix} \boldsymbol{\mu_k} &amp; \boldsymbol{\Sigma_k} \end{bmatrix}\)</span>.</p>
<p>However, it is possible for the individual variables within the random vector <span class="math notranslate nohighlight">\(\mathrm{X}_n\)</span> to follow different distributions. For instance, if <span class="math notranslate nohighlight">\(\mathrm{X}_n = \begin{bmatrix} X_{n1} &amp; X_{n2} \end{bmatrix}^\top\)</span>, we might have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
X_{n1}|(Y_n = k) &amp;\sim \text{Binomial}(n, p_{k}) \\
X_{n2}|(Y_n = k) &amp;\sim \mathcal{N}(\boldsymbol{\mu_k}, \boldsymbol{\Sigma_k})
\end{align*}
\end{split}\]</div>
<p>The machine learning fitting process is then to estimate the parameters of these distributions. More concretely, we need to estimate <span class="math notranslate nohighlight">\(\boldsymbol{\pi}_k\)</span> for <span class="math notranslate nohighlight">\(k = 1, \dots, K\)</span>, as well as <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_k\)</span> for <span class="math notranslate nohighlight">\(k = 1, \dots, K\)</span> for what might be the possible distributions of <span class="math notranslate nohighlight">\(\mathrm{X}_n \mid Y_n\)</span>. In this example above, we would need to estimate <span class="math notranslate nohighlight">\(p_k\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_k\)</span> for <span class="math notranslate nohighlight">\(k = 1, \dots, K\)</span> for the Binomial and Multivariate Gaussian distributions.</p>
<p>Once that’s done, we can estimate <span class="math notranslate nohighlight">\(\mathbb{P}(Y_n = k)\)</span> and <span class="math notranslate nohighlight">\(\mathbb{P}(\mathrm{X}_n \mid Y_n = k)\)</span> for <span class="math notranslate nohighlight">\(k = 1, \dots, K\)</span>. We can then use these estimates to make predictions about the class of a new sample <span class="math notranslate nohighlight">\(\mathrm{X}_n\)</span> using Bayes’ rule in equation <a class="reference internal" href="#equation-eq-argmax-naive-bayes-2">(36)</a>.</p>
</section>
<section id="estimation">
<h3>Estimation<a class="headerlink" href="#estimation" title="Permalink to this headline">#</a></h3>
<section id="estimating-priors">
<h4>Estimating Priors<a class="headerlink" href="#estimating-priors" title="Permalink to this headline">#</a></h4>
<p>Before we start the formal estimation process, it is intuitive to think that the prior probabilities <span class="math notranslate nohighlight">\(\boldsymbol{\pi}_k\)</span> should be proportional to the number of samples in each class. In other words, if we have <span class="math notranslate nohighlight">\(N_1\)</span> samples in class 1, <span class="math notranslate nohighlight">\(N_2\)</span> samples in class 2, and so on, then we should have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\pi_1 &amp;\propto N_1 \\
\pi_2 &amp;\propto N_2 \\
\vdots &amp; \quad \vdots \\
\pi_K &amp;\propto N_K
\end{align*}
\end{split}\]</div>
<p>For instance, if we have a dataset with <span class="math notranslate nohighlight">\(N=100\)</span> samples with <span class="math notranslate nohighlight">\(K=3\)</span> classes, and <span class="math notranslate nohighlight">\(N_1 = 10\)</span>, <span class="math notranslate nohighlight">\(N_2 = 30\)</span> and <span class="math notranslate nohighlight">\(N_3 = 60\)</span>, then we should have <span class="math notranslate nohighlight">\(\pi_1 = \frac{10}{100} = 0.1\)</span>, <span class="math notranslate nohighlight">\(\pi_2 = \frac{30}{100} = 0.3\)</span> and <span class="math notranslate nohighlight">\(\pi_3 = \frac{60}{100} = 0.6\)</span>. This is just the relative frequency of each class.</p>
<p>It turns out our intuition matches the formal estimation process.</p>
<p>Let <span class="math notranslate nohighlight">\(\mathcal{D} = \left \{ \left(\mathrm{X}^{(n)}, Y^{(n)} \right) \right \}_{n=1}^N = \left \{ \left(\mathrm{x}^{(n)}, y^{(n)} \right) \right \}_{n=1}^N\)</span> be the dataset
with <span class="math notranslate nohighlight">\(N\)</span> samples and <span class="math notranslate nohighlight">\(D\)</span> predictors. All samples are assumed to be <strong>independent and identically distributed (i.i.d.)</strong> from the unknown but fixed joint distribution
<span class="math notranslate nohighlight">\(\mathbb{D} = \mathbb{P}(\mathcal{X}, \mathcal{Y} ; \boldsymbol{\theta})\)</span> where <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> is the parameter vector of the joint distribution. In other words, <span class="math notranslate nohighlight">\(\mathrm{X}^{(1)}, \mathrm{X}^{(2)}, \dots, \mathrm{X}^{(N)}\)</span> are all i.i.d. from <span class="math notranslate nohighlight">\(\mathbb{D}\)</span>, as well as <span class="math notranslate nohighlight">\(\mathrm{Y}^{(1)}, \mathrm{Y}^{(2)}, \dots, \mathrm{Y}^{(N)}\)</span>.</p>
<p>We denote the (joint) probability distribution of the observed data <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> as <span class="math notranslate nohighlight">\(\mathbb{P}(\mathcal{D} ; \boldsymbol{\theta})\)</span>.</p>
<p>For example, if <span class="math notranslate nohighlight">\(D = 2\)</span>, and <span class="math notranslate nohighlight">\(X^{(n)}_1\)</span> and <span class="math notranslate nohighlight">\(X^{(n)}_2\)</span> are both multivariate Gaussian random variables,
with <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_1\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_2\)</span> being the mean vectors of the two distributions,
and <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_1\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_2\)</span> being the covariance matrices of the two distributions;
furthermore, <span class="math notranslate nohighlight">\(Y^{(n)}\)</span> is a Bernoulli random variable with parameter <span class="math notranslate nohighlight">\(\pi\)</span>, then we have <span class="math notranslate nohighlight">\(\boldsymbol{\theta} = \begin{bmatrix} \boldsymbol{\mu}_1 &amp; \boldsymbol{\Sigma}_1 &amp; \boldsymbol{\mu}_2 &amp; \boldsymbol{\Sigma}_2 &amp; \pi \end{bmatrix}\)</span>.</p>
<p>In this context, since we are estimating <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> but do not really know the parameters of <span class="math notranslate nohighlight">\(\mathrm{X}\)</span> just yet, we can simplify the expression to just</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(\mathcal{D} ; \left(\boldsymbol{\theta}, \boldsymbol{\pi} \right))
\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\pi} = \begin{bmatrix} \pi_1 &amp; \pi_2 &amp; \dots &amp; \pi_K \end{bmatrix}\)</span> and
<span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> is the parameter vector of <span class="math notranslate nohighlight">\(\mathrm{X}\)</span>.</p>
<p>Since each sample is <strong>i.i.d.</strong>, we can write the joint probability distribution as the product of the individual probabilities of each sample:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathbb{P}(\mathcal{D} ; \left(\boldsymbol{\pi}, \boldsymbol{\theta} \right)) &amp;= \prod_{n=1}^N \mathbb{P}(\mathrm{X}^{(n)}, Y^{(n)} ;  \left(\boldsymbol{\theta}, \boldsymbol{\pi} \right)) \\
&amp;= \prod_{n=1}^N \mathbb{P}(\mathrm{X}^{(n)} ; \boldsymbol{\theta}) \mathbb{P}(Y^{(n)} ; \boldsymbol{\pi})
\end{align*}
\end{split}\]</div>
<p>There should be no confusion that both <span class="math notranslate nohighlight">\(\mathrm{X}\)</span> and <span class="math notranslate nohighlight">\(\mathrm{Y}\)</span> are included in the joint distribution,
since the dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> is a joint distribution of <span class="math notranslate nohighlight">\(\mathrm{X}\)</span> and <span class="math notranslate nohighlight">\(\mathrm{Y}\)</span>, and not just <span class="math notranslate nohighlight">\(\mathrm{X}\)</span>(?) (Verify this.)</p>
<p>Now, we are only interested in the term that depends on <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span>, so we can drop the term that depends on <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathbb{P}(\mathcal{D} ; \boldsymbol{\pi}) &amp;= \prod_{n=1}^N \mathbb{P}(Y^{(n)} ; \boldsymbol{\pi}) \\
\end{align*}
\end{split}\]</div>
<p>The <strong>likelihood function</strong> is defined as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathcal{L}(\left \{ \boldsymbol{\theta}, \boldsymbol{\pi} \right \} ; \mathcal{D}) &amp;\overset{\mathrm{def}}{=} \mathbb{P}(\mathcal{D} ; \left(\boldsymbol{\theta}, \boldsymbol{\pi} \right)) \\
&amp;= \prod_{n=1}^N \mathbb{P}(\mathrm{X}^{(n)}, Y^{(n)} ;  \left(\boldsymbol{\theta}, \boldsymbol{\pi} \right)) \\
\end{align*}
\end{split}\]</div>
<p>but since we are only interested in the term that depends on <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span>, our likelihood function is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathcal{L}(\boldsymbol{\pi} ; \mathcal{D}) &amp;\overset{\mathrm{def}}{=} \mathbb{P}(\mathcal{D} ; \boldsymbol{\pi}) \\
&amp;= \prod_{n=1}^N \mathbb{P}(Y^{(n)} ; \boldsymbol{\pi}) \\
&amp;\overset{\mathrm{(a)}}{=} \prod_{n=1}^N \left(\prod_{k=1}^K \pi_k^{y^{(n)}_k} \right) \\
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\left(\prod_{k=1}^K \pi_k^{y^{(n)}_k} \right)\)</span> in equation <span class="math notranslate nohighlight">\((a)\)</span> is a consequence
of the definition of the Category distribution.</p>
<p>Subsequently, we can take the log of the likelihood function to get the <strong>log-likelihood function</strong> (for the ease of computation):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathcal{L}(\boldsymbol{\pi} ; \mathcal{D}) &amp;\overset{\mathrm{def}}{=} \log \mathcal{L}(\boldsymbol{\pi} ; \mathcal{D}) \\
&amp;= \sum_{n=1}^N \log \left(\prod_{k=1}^K \pi_k^{y^{(n)}_k} \right) \\
&amp;\overset{(b)}{=} \sum_{n=1}^N \sum_{k=1}^K y^{(n)}_k \log \pi_k \\
&amp;\overset{(c)}{=} \sum_{k=1}^K N_k \log \pi_k \\
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(N_k\)</span> is the number of samples that belong to the <span class="math notranslate nohighlight">\(k\)</span>-th category.</p>
<div class="proof remark admonition" id="notation-overload">
<p class="admonition-title"><span class="caption-number">Remark 10 </span> (Notation Overload)</p>
<section class="remark-content" id="proof-content">
<p>We note to ourselves that we are reusing, and hence abusing the notation <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> for the log-likelihood function to be the same as the likelihood function, this is just for the ease of re-defining a new symbol for the log-likelihood function, <span class="math notranslate nohighlight">\(\log \mathcal{L}\)</span>.</p>
</section>
</div><p>Equation <span class="math notranslate nohighlight">\((c)\)</span> is derived by expanding equation <span class="math notranslate nohighlight">\((b)\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\sum_{n=1}^N \sum_{k=1}^K y^{(n)}_k \log \pi_k &amp;= \sum_{n=1}^N \left( \sum_{k=1}^K y^{(n)}_k \log \pi_k \right) \\
&amp;= y^{(1)}_1 \log \pi_1 + y^{(1)}_2 \log \pi_2 + \dots + y^{(1)}_K \log \pi_K \\
&amp;+ y^{(2)}_1 \log \pi_1 + y^{(2)}_2 \log \pi_2 + \dots + y^{(2)}_K \log \pi_K \\
&amp;+ \qquad \vdots \qquad \\
&amp;+ y^{(N)}_1 \log \pi_1 + y^{(N)}_2 \log \pi_2 + \dots + y^{(N)}_K \log \pi_K \\
&amp;\overset{(d)}{=} \left( y^{(1)}_1 + y^{(2)}_1 + \dots + y^{(N)}_1 \right) \log \pi_1 \\
&amp;+ \left( y^{(1)}_2 + y^{(2)}_2 + \dots + y^{(N)}_2 \right) \log \pi_2 \\
&amp;+ \qquad \vdots \qquad \\
&amp;+ \left( y^{(1)}_K + y^{(2)}_K + \dots + y^{(N)}_K \right) \log \pi_K \\
&amp;\overset{(e)}{=} N_1 \log \pi_1 + N_2 \log \pi_2 + \dots + N_K \log \pi_K \\
&amp;= \sum_{k=1}^K N_k \log \pi_k \\
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\((d)\)</span> is derived by summing each column, and <span class="math notranslate nohighlight">\(N_k = y^{(1)}_k + y^{(2)}_k + \dots + y^{(N)}_k\)</span>
is nothing but the number of samples that belong to the <span class="math notranslate nohighlight">\(k\)</span>-th category. One just need to recall that
if we have say 6 samples of class <span class="math notranslate nohighlight">\((0, 1, 2, 0, 1, 1)\)</span> where <span class="math notranslate nohighlight">\(K=3\)</span>, then the one-hot encoded
representation of the samples will be</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\left[
\begin{array}{ccc}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
\end{array}
\right]
\end{align*}
\end{split}\]</div>
<p>and summing each column will give us <span class="math notranslate nohighlight">\(N_1 = 2\)</span>, <span class="math notranslate nohighlight">\(N_2 = 3\)</span>, and <span class="math notranslate nohighlight">\(N_3 = 1\)</span>.</p>
<p>Now we are finally ready to solve the estimation (optimization) problem for <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\hat{\boldsymbol{\pi}} &amp;= \underset{\boldsymbol{\pi}}{\mathrm{argmax}} ~~ \mathcal{L}(\boldsymbol{\pi} ; \mathcal{D}) \\
&amp;= \underset{\boldsymbol{\pi}}{\mathrm{argmax}} ~~ \sum_{k=1}^K N_k \log \pi_k \\
\end{align*}
\end{split}\]</div>
<p>subject to the constraint that</p>
<div class="math notranslate nohighlight">
\[
\sum_{k=1}^K \pi_k = 1
\]</div>
<p>which is just saying the probabilities must sum up to 1.</p>
<p>We can also write the expression as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\max_{\boldsymbol{\pi}} &amp;~~ \sum_{k=1}^K N_k \log \pi_k \\
\text{subject to} &amp;~~ \sum_{k=1}^K \pi_k = 1
\end{aligned}
\end{split}\]</div>
<p>This is a constrained optimization problem, and we can solve it using the Lagrangian method.</p>
<div class="proof definition admonition" id="lagrangian-method">
<p class="admonition-title"><span class="caption-number">Definition 69 </span> (Lagrangian Method)</p>
<section class="definition-content" id="proof-content">
<p>The Lagrangian method is a method to solve constrained optimization problems. The idea is to
convert the constrained optimization problem into an unconstrained optimization problem by
introducing a Lagrangian multiplier <span class="math notranslate nohighlight">\(\lambda\)</span> and then solve the unconstrained optimization
problem.</p>
<p>Given a function <span class="math notranslate nohighlight">\(f(\mathrm{x})\)</span> and a constraint <span class="math notranslate nohighlight">\(g(\mathrm{x}) = 0\)</span>, the Lagrangian function,
<span class="math notranslate nohighlight">\(\mathcal{L}(\mathrm{x}, \lambda)\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathcal{L}(\mathrm{x}, \lambda) &amp;= f(\mathrm{x}) - \lambda g(\mathrm{x}) \\
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda\)</span> is the Lagrangian multiplier and may be either positive or negative. Then,
the critical points of the Lagrangian function are the same as the critical points of the
original constrained optimization problem, i.e. setting the gradient vector of the Lagrangian
function <span class="math notranslate nohighlight">\(\nabla \mathcal{L}(\mathrm{x}, \lambda) = 0\)</span> with respect to <span class="math notranslate nohighlight">\(\mathrm{x}\)</span> and <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
</section>
</div><p>One note is that the notation of <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> seems to be overloaded again with the Lagrangian function, we will have to change it to <span class="math notranslate nohighlight">\(\mathcal{L}_\lambda\)</span> to avoid confusion. So, to reiterate, solving the Lagrangian function is equivalent to solving the constrained optimization problem.</p>
<p>In our problem, we can convert it to Lagrangian form as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\hat{\boldsymbol{\pi}} &amp;= \underset{\boldsymbol{\pi}}{\mathrm{argmax}} ~~ \mathcal{L}_\lambda(\boldsymbol{\pi}, \lambda ; \mathcal{D}) \\
&amp;= \underset{\boldsymbol{\pi}}{\mathrm{argmax}} ~~ \underbrace{\mathcal{L}(\boldsymbol{\pi} ; \mathcal{D})}_{f(\boldsymbol{\pi})} - \lambda \left(\underbrace{\sum_{k=1}^K \pi_k - 1}_{g(\boldsymbol{\pi})} \right) \\
&amp;= \underset{\boldsymbol{\pi}}{\mathrm{argmax}} ~~ \sum_{k=1}^K N_k \log \pi_k - \lambda \left( \sum_{k=1}^K \pi_k - 1 \right) \\
\end{align*}
\end{split}\]</div>
<p>which is now an unconstrained optimization problem. We can now solve it by setting the gradient vector of the Lagrangian function <span class="math notranslate nohighlight">\(\nabla \mathcal{L}_\lambda(\boldsymbol{\pi}, \lambda ; \mathcal{D}) = 0\)</span> with respect to <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> and <span class="math notranslate nohighlight">\(\lambda\)</span>, as follows,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\nabla \mathcal{L}_\lambda(\boldsymbol{\pi}, \lambda ; \mathcal{D}) &amp;\overset{\mathrm{def}}{=} \frac{\partial \mathcal{L}_\lambda(\boldsymbol{\pi}, \lambda ; \mathcal{D})}{\partial \boldsymbol{\pi}} = 0 \quad \text{and} \quad \frac{\partial \mathcal{L}_\lambda(\boldsymbol{\pi}, \lambda ; \mathcal{D})}{\partial \lambda} = 0 \\
&amp;\iff \frac{\partial}{\partial \boldsymbol{\pi}} \left( \sum_{k=1}^K N_k \log \pi_k - \lambda \left( \sum_{k=1}^K \pi_k - 1 \right) \right) = 0 \quad \text{and} \quad \frac{\partial \mathcal{L}_\lambda(\boldsymbol{\pi}, \lambda ; \mathcal{D})}{\partial \lambda} = 0 \\
\\
&amp;\iff \begin{bmatrix} \frac{\partial \mathcal{L}}{\partial \pi_1} \\ \vdots \\ \frac{\partial \mathcal{L}}{\partial \pi_K} \\ \frac{\partial \mathcal{L}}{\partial \lambda} \end{bmatrix} = \begin{bmatrix} 0 \\ \vdots \\ 0 \\ 0 \end{bmatrix} \\
&amp;\iff \begin{bmatrix} \frac{\partial}{\partial \pi_1} \left( N_1 \log \pi_1 - \lambda \left( \pi_1 - 1 \right) \right) \\ \vdots \\ \frac{\partial}{\partial \pi_K} \left( N_K \log \pi_K - \lambda \left( \pi_K - 1 \right) \right) \\ \frac{\partial \mathcal{L}_\lambda(\boldsymbol{\pi}, \lambda ; \mathcal{D})}{\partial \lambda} \end{bmatrix} = \begin{bmatrix} 0 \\ \vdots \\ 0 \\ 0 \end{bmatrix} \\
&amp;\iff \begin{bmatrix} \frac{N_1}{\pi_1} - \lambda \\ \vdots \\ \frac{N_K}{\pi_K} - \lambda \\ \sum_{k=1}^K \pi_k - 1 \end{bmatrix} = \begin{bmatrix} 0 \\ \vdots \\ 0 \\ 0 \end{bmatrix} \\
\end{align*}
\end{split}\]</div>
<p>The reason we can unpack <span class="math notranslate nohighlight">\(\frac{\partial}{\partial \pi_k}\left( \sum_{k=1}^K N_k \log \pi_k - \lambda \left( \sum_{k=1}^K \pi_k - 1 \right) \right)\)</span> as <span class="math notranslate nohighlight">\(\frac{\partial}{\partial \pi_k} \left( N_k \log \pi_k - \lambda \left( \pi_k - 1 \right) \right)\)</span> is because we are dealing with partial derivatives, so other terms other than <span class="math notranslate nohighlight">\(\pi_k\)</span> are constant.</p>
<p>Finally, we have a system of equations for each <span class="math notranslate nohighlight">\(\pi_k\)</span> and if we can solve for <span class="math notranslate nohighlight">\(\pi_k\)</span> for each <span class="math notranslate nohighlight">\(k\)</span>, we can then find the best estimate of <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span>. It turns out we have to find <span class="math notranslate nohighlight">\(\lambda\)</span> first, and this can be solved by setting <span class="math notranslate nohighlight">\(\sum_{k=1}^K \pi_k - 1 = 0\)</span> and solving for <span class="math notranslate nohighlight">\(\lambda\)</span>, which is the last equation in the system of equations above. We first express each <span class="math notranslate nohighlight">\(\pi_k\)</span> in terms of <span class="math notranslate nohighlight">\(\lambda\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\frac{N_1}{\pi_1} - \lambda &amp;= 0 \implies \pi_1 = \frac{N_1}{\lambda} \\
\frac{N_2}{\pi_2} - \lambda &amp;= 0 \implies \pi_2 = \frac{N_2}{\lambda} \\
\vdots \\
\frac{N_K}{\pi_K} - \lambda &amp;= 0 \implies \pi_K = \frac{N_K}{\lambda} \\
\end{align*}
\end{split}\]</div>
<p>Then we substitute these expressions into the last equation in the system of equations above, and solve for <span class="math notranslate nohighlight">\(\lambda\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\sum_{k=1}^K \pi_k - 1 = 0 &amp;\implies \sum_{k=1}^K \frac{N_k}{\lambda} - 1 = 0 \\
&amp;\implies \sum_{k=1}^K \frac{N_k}{\lambda} = 1 \\
&amp;\implies \sum_{k=1}^K N_k = \lambda \\
&amp;\implies \lambda = \sum_{k=1}^K N_k \\
&amp;\implies \lambda = N \\
\end{align*}
\end{split}\]</div>
<p>and therefore, we can now solve for <span class="math notranslate nohighlight">\(\pi_k\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\hat{\pi}} = \left .
  \begin{cases}
    \pi_1 = \frac{N_1}{N} \\
    \pi_2 = \frac{N_2}{N} \\
    \vdots \quad \vdots \quad \vdots \\
    \pi_K = \frac{N_K}{N} \\
  \end{cases}
  \right\} \implies \pi_k = \frac{N_k}{N} \quad \text{for} \quad k = 1, 2, \ldots, K
\end{split}\]</div>
<p>We conclude that the maximum likelihood estimate of <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> is <span class="math notranslate nohighlight">\(\boldsymbol{\pi} = \left( \frac{N_1}{N}, \frac{N_2}{N}, \ldots, \frac{N_K}{N} \right)\)</span>, which is the same as the empirical relative frequency of each class in the training data. This coincides with our intuition.</p>
<p>For completeness of expression,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\hat{\boldsymbol{\pi}} &amp;= \arg \max_{\boldsymbol{\pi}} \mathcal{L}\left( \boldsymbol{\pi} ; \mathcal{D} \right) \\
&amp;= \begin{bmatrix} \hat{\pi}_1 \\ \vdots \\ \hat{\pi}_K \end{bmatrix} \\
&amp;= \begin{bmatrix} \frac{N_1}{N} \\ \vdots \\ \frac{N_K}{N} \end{bmatrix}
\end{align*}
\end{split}\]</div>
<ol class="simple">
<li><p>iid assumption very impt, see chans book, if not we cannot multiply the likelihoods?</p></li>
<li><p>Each <span class="math notranslate nohighlight">\(X_n\)</span> is iid random vector of <span class="math notranslate nohighlight">\(N\)</span> samples.?</p></li>
<li><p><strong>IMPORTANT, you are using <span class="math notranslate nohighlight">\(N\)</span> samples to estimate, so you can use multiplication of <span class="math notranslate nohighlight">\(N\)</span> samples PDF here, but note each sample is a random vector, not a random variable, so u r not estimating a single mean or variance, but a vector of means and variances.</strong></p></li>
<li><p>Whether to use superscript to mean sample to avoid confusion?</p></li>
</ol>
</section>
</section>
</section>
<section id="penguins-dataset">
<h2>Penguins Dataset<a class="headerlink" href="#penguins-dataset" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import pandas as pd
import numpy as np
from scipy import stats
import seaborn as sns 
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
sns.set_style(&#39;white&#39;)
</pre></div>
</div>
</div>
</div>
<p>There exist multiple penguin species throughout Antarctica, including the Adelie, Chinstrap, and Gentoo. When encountering one of these penguins on an Antarctic trip, we might classify its species</p>
<div class="math notranslate nohighlight">
\[\begin{split}
Y= \begin{cases}A &amp; \text { Adelie } \\ C &amp; \text { Chinstrap } \\ G &amp; \text { Gentoo }\end{cases}
\end{split}\]</div>
<p>by examining various physical characteristics, such as whether the penguin weighs more than the average <span class="math notranslate nohighlight">\(4200 \mathrm{~g}\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
X_1= \begin{cases}1 &amp; \text { above-average weight } \\ 0 &amp; \text { below-average weight }\end{cases}
\end{split}\]</div>
<p>as well as measurements of the penguin’s bill</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
&amp; X_2=\text { bill length }(\text { in } \mathrm{mm}) \\
&amp; X_3=\text { flipper length }(\text { in } \mathrm{mm})
\end{aligned}
\end{split}\]</div>
<p>The penguins_bayes data, originally made available by Gorman, Williams, and Fraser (2014) and distributed by Horst, Hill, and Gorman (2020), contains the above species and feature information for a sample of 344 Antarctic penguins:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>penguins = sns.load_dataset(&#39;penguins&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># display(penguins.head())

penguins.head().style.set_table_attributes(&#39;style=&quot;font-size: 13px&quot;&#39;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style type="text/css">
</style>
<table id="T_82625" style="font-size: 13px">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_82625_level0_col0" class="col_heading level0 col0" >species</th>
      <th id="T_82625_level0_col1" class="col_heading level0 col1" >island</th>
      <th id="T_82625_level0_col2" class="col_heading level0 col2" >bill_length_mm</th>
      <th id="T_82625_level0_col3" class="col_heading level0 col3" >bill_depth_mm</th>
      <th id="T_82625_level0_col4" class="col_heading level0 col4" >flipper_length_mm</th>
      <th id="T_82625_level0_col5" class="col_heading level0 col5" >body_mass_g</th>
      <th id="T_82625_level0_col6" class="col_heading level0 col6" >sex</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_82625_level0_row0" class="row_heading level0 row0" >0</th>
      <td id="T_82625_row0_col0" class="data row0 col0" >Adelie</td>
      <td id="T_82625_row0_col1" class="data row0 col1" >Torgersen</td>
      <td id="T_82625_row0_col2" class="data row0 col2" >39.100000</td>
      <td id="T_82625_row0_col3" class="data row0 col3" >18.700000</td>
      <td id="T_82625_row0_col4" class="data row0 col4" >181.000000</td>
      <td id="T_82625_row0_col5" class="data row0 col5" >3750.000000</td>
      <td id="T_82625_row0_col6" class="data row0 col6" >Male</td>
    </tr>
    <tr>
      <th id="T_82625_level0_row1" class="row_heading level0 row1" >1</th>
      <td id="T_82625_row1_col0" class="data row1 col0" >Adelie</td>
      <td id="T_82625_row1_col1" class="data row1 col1" >Torgersen</td>
      <td id="T_82625_row1_col2" class="data row1 col2" >39.500000</td>
      <td id="T_82625_row1_col3" class="data row1 col3" >17.400000</td>
      <td id="T_82625_row1_col4" class="data row1 col4" >186.000000</td>
      <td id="T_82625_row1_col5" class="data row1 col5" >3800.000000</td>
      <td id="T_82625_row1_col6" class="data row1 col6" >Female</td>
    </tr>
    <tr>
      <th id="T_82625_level0_row2" class="row_heading level0 row2" >2</th>
      <td id="T_82625_row2_col0" class="data row2 col0" >Adelie</td>
      <td id="T_82625_row2_col1" class="data row2 col1" >Torgersen</td>
      <td id="T_82625_row2_col2" class="data row2 col2" >40.300000</td>
      <td id="T_82625_row2_col3" class="data row2 col3" >18.000000</td>
      <td id="T_82625_row2_col4" class="data row2 col4" >195.000000</td>
      <td id="T_82625_row2_col5" class="data row2 col5" >3250.000000</td>
      <td id="T_82625_row2_col6" class="data row2 col6" >Female</td>
    </tr>
    <tr>
      <th id="T_82625_level0_row3" class="row_heading level0 row3" >3</th>
      <td id="T_82625_row3_col0" class="data row3 col0" >Adelie</td>
      <td id="T_82625_row3_col1" class="data row3 col1" >Torgersen</td>
      <td id="T_82625_row3_col2" class="data row3 col2" >nan</td>
      <td id="T_82625_row3_col3" class="data row3 col3" >nan</td>
      <td id="T_82625_row3_col4" class="data row3 col4" >nan</td>
      <td id="T_82625_row3_col5" class="data row3 col5" >nan</td>
      <td id="T_82625_row3_col6" class="data row3 col6" >nan</td>
    </tr>
    <tr>
      <th id="T_82625_level0_row4" class="row_heading level0 row4" >4</th>
      <td id="T_82625_row4_col0" class="data row4 col0" >Adelie</td>
      <td id="T_82625_row4_col1" class="data row4 col1" >Torgersen</td>
      <td id="T_82625_row4_col2" class="data row4 col2" >36.700000</td>
      <td id="T_82625_row4_col3" class="data row4 col3" >19.300000</td>
      <td id="T_82625_row4_col4" class="data row4 col4" >193.000000</td>
      <td id="T_82625_row4_col5" class="data row4 col5" >3450.000000</td>
      <td id="T_82625_row4_col6" class="data row4 col6" >Female</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>display(penguins.info())
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 344 entries, 0 to 343
Data columns (total 7 columns):
 #   Column             Non-Null Count  Dtype  
---  ------             --------------  -----  
 0   species            344 non-null    object 
 1   island             344 non-null    object 
 2   bill_length_mm     342 non-null    float64
 3   bill_depth_mm      342 non-null    float64
 4   flipper_length_mm  342 non-null    float64
 5   body_mass_g        342 non-null    float64
 6   sex                333 non-null    object 
dtypes: float64(4), object(3)
memory usage: 18.9+ KB
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>None
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>display(penguins.describe())
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>bill_length_mm</th>
      <th>bill_depth_mm</th>
      <th>flipper_length_mm</th>
      <th>body_mass_g</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>342.000000</td>
      <td>342.000000</td>
      <td>342.000000</td>
      <td>342.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>43.921930</td>
      <td>17.151170</td>
      <td>200.915205</td>
      <td>4201.754386</td>
    </tr>
    <tr>
      <th>std</th>
      <td>5.459584</td>
      <td>1.974793</td>
      <td>14.061714</td>
      <td>801.954536</td>
    </tr>
    <tr>
      <th>min</th>
      <td>32.100000</td>
      <td>13.100000</td>
      <td>172.000000</td>
      <td>2700.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>39.225000</td>
      <td>15.600000</td>
      <td>190.000000</td>
      <td>3550.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>44.450000</td>
      <td>17.300000</td>
      <td>197.000000</td>
      <td>4050.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>48.500000</td>
      <td>18.700000</td>
      <td>213.000000</td>
      <td>4750.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>59.600000</td>
      <td>21.500000</td>
      <td>231.000000</td>
      <td>6300.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Among these penguins, 152 are Adelies, 68 are Chinstraps, and 124 are Gentoos. We’ll assume throughout that the proportional breakdown of these species in our dataset reflects the species breakdown in the wild. That is, our prior assumption about any new penguin is that it’s most likely an Adelie and least likely a Chinstrap:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>display(penguins[&#39;species&#39;].value_counts(normalize=False))
display(penguins[&#39;species&#39;].value_counts(normalize=True))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Adelie       152
Gentoo       124
Chinstrap     68
Name: species, dtype: int64
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Adelie       0.441860
Gentoo       0.360465
Chinstrap    0.197674
Name: species, dtype: float64
</pre></div>
</div>
</div>
</div>
<p>We drop <code class="docutils literal notranslate"><span class="pre">sex</span></code> column for simplicity as there are quite a few missing values in it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>penguins = penguins.drop(columns=[&quot;sex&quot;], inplace=False).reset_index(drop=True)
</pre></div>
</div>
</div>
</div>
<p>We also drop the other NA rows, there are only 2 of them.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>penguins = penguins.dropna().reset_index(drop=True) # drop rows with NAs, only 2 rows
penguins
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>species</th>
      <th>island</th>
      <th>bill_length_mm</th>
      <th>bill_depth_mm</th>
      <th>flipper_length_mm</th>
      <th>body_mass_g</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Adelie</td>
      <td>Torgersen</td>
      <td>39.1</td>
      <td>18.7</td>
      <td>181.0</td>
      <td>3750.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Adelie</td>
      <td>Torgersen</td>
      <td>39.5</td>
      <td>17.4</td>
      <td>186.0</td>
      <td>3800.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Adelie</td>
      <td>Torgersen</td>
      <td>40.3</td>
      <td>18.0</td>
      <td>195.0</td>
      <td>3250.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Adelie</td>
      <td>Torgersen</td>
      <td>36.7</td>
      <td>19.3</td>
      <td>193.0</td>
      <td>3450.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Adelie</td>
      <td>Torgersen</td>
      <td>39.3</td>
      <td>20.6</td>
      <td>190.0</td>
      <td>3650.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>337</th>
      <td>Gentoo</td>
      <td>Biscoe</td>
      <td>47.2</td>
      <td>13.7</td>
      <td>214.0</td>
      <td>4925.0</td>
    </tr>
    <tr>
      <th>338</th>
      <td>Gentoo</td>
      <td>Biscoe</td>
      <td>46.8</td>
      <td>14.3</td>
      <td>215.0</td>
      <td>4850.0</td>
    </tr>
    <tr>
      <th>339</th>
      <td>Gentoo</td>
      <td>Biscoe</td>
      <td>50.4</td>
      <td>15.7</td>
      <td>222.0</td>
      <td>5750.0</td>
    </tr>
    <tr>
      <th>340</th>
      <td>Gentoo</td>
      <td>Biscoe</td>
      <td>45.2</td>
      <td>14.8</td>
      <td>212.0</td>
      <td>5200.0</td>
    </tr>
    <tr>
      <th>341</th>
      <td>Gentoo</td>
      <td>Biscoe</td>
      <td>49.9</td>
      <td>16.1</td>
      <td>213.0</td>
      <td>5400.0</td>
    </tr>
  </tbody>
</table>
<p>342 rows × 6 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>display(penguins[&#39;species&#39;].value_counts(normalize=False))
display(penguins[&#39;species&#39;].value_counts(normalize=True))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Adelie       151
Gentoo       123
Chinstrap     68
Name: species, dtype: int64
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Adelie       0.441520
Gentoo       0.359649
Chinstrap    0.198830
Name: species, dtype: float64
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>print(f&quot;The mean body mass of penguins is {penguins[&#39;body_mass_g&#39;].mean():.2f} grams.&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The mean body mass of penguins is 4201.75 grams.
</pre></div>
</div>
</div>
</div>
<p>We create a new categorical feature <code class="docutils literal notranslate"><span class="pre">overweight</span></code> which is 1 if the <code class="docutils literal notranslate"><span class="pre">body_mass_g</span></code> is over the
mean, and 0 otherwise. This feature corresponds to our earlier defined random variable <span class="math notranslate nohighlight">\(X_1\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>penguins[&quot;overweight&quot;] = (penguins[&quot;body_mass_g&quot;] &gt; penguins[&quot;body_mass_g&quot;].mean()).astype(int)
display(penguins[&quot;overweight&quot;].value_counts())
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0    193
1    149
Name: overweight, dtype: int64
</pre></div>
</div>
</div>
</div>
</section>
<section id="the-prior">
<h2>The Prior<a class="headerlink" href="#the-prior" title="Permalink to this headline">#</a></h2>
<p>The prior distribution for <span class="math notranslate nohighlight">\(Y\)</span> is a <a class="reference external" href="https://en.wikipedia.org/wiki/Categorical_distribution">categorical distribution</a> with three categories, one for each species. The prior probabilities are given by the relative frequencies of each species in the dataset:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
&amp; \text { A } \sim \text { Categorical }(\pi_A=0.44) \\
&amp; \text { C } \sim \text { Categorical }(\pi_C=0.20) \\
&amp; \text { G } \sim \text { Categorical }(\pi_G=0.36)
\end{aligned}
\end{split}\]</div>
<p>which implies</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{P}(Y=A) &amp;= 0.44 \\
\mathbb{P}(Y=C) &amp;= 0.20 \\
\mathbb{P}(Y=G) &amp;= 0.36
\end{aligned}
\end{split}\]</div>
<p>Given a new penguin not in the dataset, the <strong>prior assumption</strong> says that the probability
of it being an Adelie is <span class="math notranslate nohighlight">\(0.44\)</span>, the probability of it being a Chinstrap is <span class="math notranslate nohighlight">\(0.20\)</span>, and the probability of it being a Gentoo is <span class="math notranslate nohighlight">\(0.36\)</span>.</p>
<p>Reference: <a class="reference external" href="https://www.bayesrulesbook.com/chapter-2.html#building-a-bayesian-model-for-events">The Bayes Rules Book</a>.</p>
</section>
<section id="classifying-one-penguin">
<h2>Classifying one penguin<a class="headerlink" href="#classifying-one-penguin" title="Permalink to this headline">#</a></h2>
<p>Consider a new penguin with the following features:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">body_mass_g</span></code> <span class="math notranslate nohighlight">\(&lt; 4200 \mathrm{~g}\)</span> (<code class="docutils literal notranslate"><span class="pre">overweight</span></code> = 0 if <code class="docutils literal notranslate"><span class="pre">body_mass_g</span></code> <span class="math notranslate nohighlight">\(&lt; 4200 \mathrm{~g}\)</span>, 1 otherwise)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bill_length_mm</span></code> <span class="math notranslate nohighlight">\(50 \mathrm{~mm}\)</span></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">flipper_length_mm</span></code> <span class="math notranslate nohighlight">\(195 \mathrm{~mm}\)</span></p></li>
</ul>
<p>Then we want to find out the posterior distribution of the species of this penguin, given the features.
In other words, what is the probability of this penguin being an Adelie, Chinstrap, or Gentoo, given the features?</p>
<section id="one-categorical-feature">
<h3>One Categorical Feature<a class="headerlink" href="#one-categorical-feature" title="Permalink to this headline">#</a></h3>
<p>Let’s start with the first feature, <code class="docutils literal notranslate"><span class="pre">body_mass_g</span></code> and since the penguin weights less than the average weight of <span class="math notranslate nohighlight">\(4200 \mathrm{~g}\)</span>, it belongs to the category <span class="math notranslate nohighlight">\(0\)</span> of the feature <code class="docutils literal notranslate"><span class="pre">overweight</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>total = penguins.groupby(&quot;species&quot;).size().reset_index(name=&quot;counts&quot;)
display(total)

overweight = (
    penguins[penguins[&quot;overweight&quot;] == 1]
    .groupby(&quot;species&quot;)[&quot;overweight&quot;]
    .sum()
    .reset_index()
)
display(overweight)

overweight[&quot;overweight&quot;] = [
    i / j * 100 for i, j in zip(overweight[&quot;overweight&quot;], total[&quot;counts&quot;])
]
total[&quot;counts&quot;] = [i / j * 100 for i, j in zip(total[&quot;counts&quot;], total[&quot;counts&quot;])]

display(total)
display(overweight)  # percentage of overweight penguins for each species
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>species</th>
      <th>counts</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Adelie</td>
      <td>151</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Chinstrap</td>
      <td>68</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Gentoo</td>
      <td>123</td>
    </tr>
  </tbody>
</table>
</div></div><div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>species</th>
      <th>overweight</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Adelie</td>
      <td>25</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Chinstrap</td>
      <td>7</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Gentoo</td>
      <td>117</td>
    </tr>
  </tbody>
</table>
</div></div><div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>species</th>
      <th>counts</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Adelie</td>
      <td>100.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Chinstrap</td>
      <td>100.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Gentoo</td>
      <td>100.0</td>
    </tr>
  </tbody>
</table>
</div></div><div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>species</th>
      <th>overweight</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Adelie</td>
      <td>16.556291</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Chinstrap</td>
      <td>10.294118</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Gentoo</td>
      <td>95.121951</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># bar chart 1 -&gt; top bars (group of &#39;overweight=No&#39;)
bar1 = sns.barplot(x=&quot;species&quot;, y=&quot;counts&quot;, data=total, color=&quot;darkblue&quot;)

# bar chart 2 -&gt; bottom bars (group of &#39;overweight=Yes&#39;)
bar2 = sns.barplot(x=&quot;species&quot;, y=&quot;overweight&quot;, data=overweight, color=&quot;lightblue&quot;)

# add legend
top_bar = mpatches.Patch(color=&quot;darkblue&quot;, label=&quot;overweight = No&quot;)
bottom_bar = mpatches.Patch(color=&quot;lightblue&quot;, label=&quot;overweight = Yes&quot;)
plt.legend(handles=[top_bar, bottom_bar])

# show the graph
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/naive_bayes_example_33_0.png" src="../../../_images/naive_bayes_example_33_0.png" />
</div>
</div>
<p>Before we do anything, we are given one categorical variable and for this penguin, it falls under
category <span class="math notranslate nohighlight">\(0\)</span> of the <code class="docutils literal notranslate"><span class="pre">overweight</span></code> feature (i.e. underweight).</p>
<p>From the conditionals below, the Chinstrap species have the highest probability of underweight penguins by
the relative freqeuncy table. That is, for each species, we compute the relative frequency within each species as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{P}[X_1=0|Y=A] &amp;= 0.8344 \\
\mathbb{P}[X_1=0|Y=C] &amp;= 0.8971 \\
\mathbb{P}[X_1=0|Y=G] &amp;= 0.0488 \\
\end{aligned}
\end{split}\]</div>
<p>Note that we are abusing the notation <span class="math notranslate nohighlight">\(\mathbb{P}\)</span> here, since we are not talking about the probability of a random variable, but rather the relative frequency of a feature within a species.
Also note the above expressions is the conditional probability of <span class="math notranslate nohighlight">\(X_1=0\)</span> given <span class="math notranslate nohighlight">\(Y\)</span> is a certain species, also termed as the <strong>likelihood</strong> of <span class="math notranslate nohighlight">\(Y\)</span> being a certain species given <span class="math notranslate nohighlight">\(X_1=0\)</span>. Again, we are being loose with notation here as everything here is empirical!</p>
<p>So one might say that the Chinstrap species is the least likely to be overweight, and the Gentoo species is the most likely to be overweight. Well, this makes intuitive sense since we are talking
about “likelihood” here: of all <span class="math notranslate nohighlight">\(P(X_1=0|Y=A)\)</span>, <span class="math notranslate nohighlight">\(P(X_0=1|Y=C)\)</span>, and <span class="math notranslate nohighlight">\(P(X_1=0|Y=G)\)</span>, the Chinstrap species is the least likely to be overweight.</p>
<p>Yet before we can make any conclusions, we need to take into account the prior probabilities of each species. We should weight the relative frequencies by the prior probabilities of each species. Intuitively, since Chinstrap is also the <em><strong>rarest</strong></em> species, it diminishes the likelihood of the penguin being a Chinstrap.</p>
<p>We need to use both the prior and the likelihood to compute the posterior distribution of the species of the penguin.
The posterior distribution is the probability of the species of the penguin given the features.</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\mathbb{P}[Y=y|X_1=x_1] = \frac{\mathbb{P}[X_1=x_1|Y=y] \mathbb{P}[Y=y]}{\mathbb{P}[X_1=x_1]}
\end{aligned}
\]</div>
<p>For example, if our given feature of the test penguin is <span class="math notranslate nohighlight">\(X_1=0\)</span>, then we need
to find the posterior distribution of <strong>all</strong> species given <span class="math notranslate nohighlight">\(X_1=0\)</span>. That is, we need to compute</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{P}[Y=A|X_1=0] &amp;= \frac{\mathbb{P}[X_1=0|Y=A] \mathbb{P}[Y=A]}{\mathbb{P}[X_1=0]} \\
\mathbb{P}[Y=C|X_1=0] &amp;= \frac{\mathbb{P}[X_1=0|Y=C] \mathbb{P}[Y=C]}{\mathbb{P}[X_1=0]} \\
\mathbb{P}[Y=G|X_1=0] &amp;= \frac{\mathbb{P}[X_1=0|Y=G] \mathbb{P}[Y=G]}{\mathbb{P}[X_1=0]}
\end{aligned}
\end{split}\]</div>
<p>and get the <strong>argmax</strong> of the above three expressions. The argmax is the species with the highest probability.
Note this makes sense because <span class="math notranslate nohighlight">\(\mathbb{P}[Y=y|X_1=x_1]\)</span> is a legitimate probability measure, since it sums to <span class="math notranslate nohighlight">\(1\)</span>.</p>
<p>The <strong>argmax</strong> expression is as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\text{argmax}_{y \in \{A, C, G\}} \mathbb{P}[Y=y|X_1=0] &amp;= \text{argmax}_{y \in \{A, C, G\}} \frac{\mathbb{P}[X_1=0|Y=y] \mathbb{P}[Y=y]}{\mathbb{P}[X_1=0]} \\
&amp;= \text{argmax}_{y \in \{A, C, G\}} \frac{\mathbb{P}[X_1=0|Y=y] \mathbb{P}[Y=y]}{\sum_{y' \in \{A, C, G\}} \mathbb{P}[X_1=0|Y=y'] \mathbb{P}[Y=y']}
\end{aligned}
\end{split}\]</div>
<p>Note in the Bayes Rules Book, the authors used the notation <span class="math notranslate nohighlight">\(f\)</span> instead of <span class="math notranslate nohighlight">\(\mathbb{P}\)</span>,
they are the same thing in this context since both <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are discrete random variables.
Consequently, the right hand side of the equation <span class="math notranslate nohighlight">\(\mathbb{P}[X_1=x_1|Y=y]\)</span> is
gives us a concrete value. However, if <span class="math notranslate nohighlight">\(X_1\)</span> were a continuous random variable, as we will see in the
next section, then the expression <span class="math notranslate nohighlight">\(\mathbb{P}[X_1=x_1|Y=y]\)</span> would not make much sense since
we the conditional probability of a continuous random variable at a point <span class="math notranslate nohighlight">\(x_1\)</span> is <span class="math notranslate nohighlight">\(0\)</span> by definition.</p>
<p>We reconcile this by expressing Bayes Rule in terms of the probability density function (PDF) of the posterior distribution.</p>
<p>The conditional PDF of the posterior distribution is</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
f_{Y|X_1}(y|x_1) = \frac{f_{X_1|Y}(x_1|y) f_{Y}(y)}{f_{X_1}(x_1)}
\end{aligned}
\]</div>
<p>We make a bold statement now (not proven but will do so) that the <span class="math notranslate nohighlight">\(\hat{y}\)</span> that maximizes
the posterior distribution in terms of Bayes Rules is the same as the <span class="math notranslate nohighlight">\(\hat{y}\)</span> that maximizes
the posterior distribution in terms of the conditional PDF.</p>
<p>If this statement is true, then</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\text{argmax}_{y \in \{A, C, G\}} f_{Y|X_1}(y|x_1) &amp;= \text{argmax}_{y \in \{A, C, G\}} \frac{f_{X_1|Y}(x_1|y) f_{Y}(y)}{f_{X_1}(x_1)} \\
&amp;= \text{argmax}_{y \in \{A, C, G\}} \frac{f_{X_1|Y}(x_1|y) f_{Y}(y)}{\sum_{y' \in \{A, C, G\}} f_{X_1|Y}(x_1|y') f_{Y}(y')}
\end{aligned}
\end{split}\]</div>
<p>We pause here and move on to the discrete case, but in any case, we can also use
the conditional PMF here, where the author sort of abused the notation here by using <span class="math notranslate nohighlight">\(f\)</span> instead of <span class="math notranslate nohighlight">\(\mathbb{P}\)</span>.</p>
<p>The table below breaks down the joint distribution table of <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X_1\)</span>, since
both are categorical, it is easy to compute the joint distribution table.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(Y\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(X_1 = 0\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(X_1 = 1\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\sum\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>A</p></td>
<td><p>126</p></td>
<td><p>25</p></td>
<td><p>151</p></td>
</tr>
<tr class="row-odd"><td><p>C</p></td>
<td><p>61</p></td>
<td><p>7</p></td>
<td><p>68</p></td>
</tr>
<tr class="row-even"><td><p>G</p></td>
<td><p>6</p></td>
<td><p>117</p></td>
<td><p>123</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\sum\)</span></p></td>
<td><p>193</p></td>
<td><p>149</p></td>
<td><p>342</p></td>
</tr>
</tbody>
</table>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(Y\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(X_1 = 0\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(X_1 = 1\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\sum\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>A</p></td>
<td><p>0.365</p></td>
<td><p>0.072</p></td>
<td><p>0.437</p></td>
</tr>
<tr class="row-odd"><td><p>C</p></td>
<td><p>0.178</p></td>
<td><p>0.020</p></td>
<td><p>0.198</p></td>
</tr>
<tr class="row-even"><td><p>G</p></td>
<td><p>0.017</p></td>
<td><p>0.342</p></td>
<td><p>0.359</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\sum\)</span></p></td>
<td><p>0.560</p></td>
<td><p>0.434</p></td>
<td><p>1.000</p></td>
</tr>
</tbody>
</table>
<p>The image is a sketch of the joint distribution table of <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X_1\)</span>.</p>
<figure class="align-default" id="penguin-overweight-joint">
<a class="reference internal image-reference" href="machine_learning_algorithms/generative/assets/penguin_overweight_joint.jpg"><img alt="machine_learning_algorithms/generative/assets/penguin_overweight_joint.jpg" src="machine_learning_algorithms/generative/assets/penguin_overweight_joint.jpg" style="height: 400px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8 </span><span class="caption-text">Joint distribution diagram of <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X_1\)</span>.</span><a class="headerlink" href="#penguin-overweight-joint" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>g = sns.JointGrid(data=penguins, x=&quot;overweight&quot;, y=&quot;species&quot;)
low, high = 0, 1
# bins =  np.arange(0, high + 1.5) - 0.5 # [-0.5, 0.5, 1.5]
g.plot_joint(sns.histplot,discrete=True, cbar=True, color=&quot;red&quot;)
g.plot_marginals(sns.histplot, discrete=True, color=&quot;red&quot;);
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/naive_bayes_example_38_0.png" src="../../../_images/naive_bayes_example_38_0.png" />
</div>
</div>
<p>We can also use seaborns <code class="docutils literal notranslate"><span class="pre">jointgrid</span></code> to do a joint distribution plot, the darker tone of red
corresponds to a higher impulse of the joint distribution.</p>
<p>Indeed we can see that Adelie + overweight = 0 and Gentoo + overweight = 1 are the
most red tone, indicating that <span class="math notranslate nohighlight">\(\mathbb{P}[X_1=0, Y=A]\)</span> and <span class="math notranslate nohighlight">\(\mathbb{P}[X_1=1, Y=G]\)</span> are the highest
impulses of the joint distribution.</p>
<p>In our table they correspond to 0.365 and 0.342 respectively.</p>
<p>The top and right are the marginal distribution of <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X_1\)</span> respectively. They are
just relative frequencies.</p>
<p>Note the above all uses histo plot as default, where we used normal histograms to
“estimate” the PDF of the joint distribution. We can also use a kernel density estimation (KDE) to
do so, which is a bit more smooth and accurate. Note everything here is JUST an estimation. We will
use it in continuous distributions later.</p>
<p>To see the empirical conditional distribution of <span class="math notranslate nohighlight">\(X_1=0\)</span> given <span class="math notranslate nohighlight">\(Y=C\)</span> for example,
it is simply given by the table above. To calculate we ask ourselves the following question:</p>
<p>what is <span class="math notranslate nohighlight">\(P(X_1=0|Y=C)\)</span>? We simply look at the table EARLIER and see that it is <span class="math notranslate nohighlight">\(0.8971\)</span>. But to
read it off this joint distribution table, you first need to recognize that when <span class="math notranslate nohighlight">\(Y=C\)</span>, we have shrinked our table to only the rows where <span class="math notranslate nohighlight">\(Y=C\)</span> (2nd row). Then we look at the column where <span class="math notranslate nohighlight">\(X_1=0\)</span> and see that it is 61, then we divide by the sum of the row, which is 68, to get <span class="math notranslate nohighlight">\(0.8971\)</span>.</p>
<p>In this scenario, the author mentioned that we can actually directly compute the posterior probability from the empirical joint distribution table.</p>
<p>For example, if we want to calculate the posterior probability of <span class="math notranslate nohighlight">\(Y=A\)</span> given <span class="math notranslate nohighlight">\(X_1=0\)</span>,
we simply look at the column <span class="math notranslate nohighlight">\(X_1=0\)</span> and calculate the relative frequency of <span class="math notranslate nohighlight">\(Y=A\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}[Y=A|X_1=0] = \frac{126}{193} = 0.652
\]</div>
<p>or equivalently</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}[Y=A|X_1=0] = \frac{0.365}{0.560} = 0.652
\]</div>
<p>Note that we are still talking about empirical here, so nothing is really proven yet.</p>
<p>In a similar fashion, we have</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}[Y=C|X_1=0] = \frac{61}{193} = 0.315
\]</div>
<div class="math notranslate nohighlight">
\[
\mathbb{P}[Y=G|X_1=0] = \frac{6}{193} = 0.031
\]</div>
<p>We can confirm this by computing the posterior distribution using the formula above.</p>
<p>Firstly, our prior says that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{P}[Y=A] &amp;= \frac{151}{342} = 0.4415 \\
\mathbb{P}[Y=C] &amp;= \frac{68}{342} = 0.1988 \\
\mathbb{P}[Y=G] &amp;= \frac{123}{342} = 0.3596 \\
\end{aligned}
\end{split}\]</div>
<p>These values will handle the <span class="math notranslate nohighlight">\(\mathbb{P}[Y=y]\)</span> term in the formula above.
I have to emphasize again that all of these are empirical probabilities, the actual
probability of the species of the penguin is not known, i.e <span class="math notranslate nohighlight">\(\mathbb{P}[Y=y]\)</span> is not known
but we can reasonably estimate it using statistics, and in this case we estimate
it using the relative frequency of each species, we will see later that the
relative frequency is a good estimator of the actual probability by Maximum Likelihood Estimation.</p>
<p>Next, we find the likelihood terms of <span class="math notranslate nohighlight">\(\mathbb{P}[X_1=0|Y=y]\)</span> for each species <span class="math notranslate nohighlight">\(y\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{P}[X_1=0|Y=A] &amp;= \frac{126}{151} = 0.8344 \\
\mathbb{P}[X_1=0|Y=C] &amp;= \frac{61}{68} = 0.8971 \\
\mathbb{P}[X_1=0|Y=G] &amp;= \frac{6}{123} = 0.0488 \\
\end{aligned}
\end{split}\]</div>
<p>Again, these are empirical probabilities, we are not sure if these are the actual probabilities of the features given the species, but we can reasonably estimate it using statistics, and
in this case we again use relative frequency to estimate it. We will see later this is modelled
by the Multinomial distribution with parameter <span class="math notranslate nohighlight">\(\pi_y\)</span>.</p>
<p>See WIKIPEDIA: When k is 2 and n is 1, the multinomial distribution is the Bernoulli distribution. When k is 2 and n is bigger than 1, it is the binomial distribution. When k is bigger than 2 and n is 1, it is the categorical distribution.</p>
<p>Plugging these <strong>priors</strong> and <strong>likelihoods</strong> into the formula above, we get the
denominator, the normalizing constant, which is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{P}[X_1=0] &amp;= \mathbb{P}[X_1=0|Y=A] \mathbb{P}[Y=A] \\
&amp;+ \mathbb{P}[X_1=0|Y=C] \mathbb{P}[Y=C] \\
&amp;+ \mathbb{P}[X_1=0|Y=G] \mathbb{P}[Y=G] \\
&amp;= \dfrac{193}{342} = 0.565
\end{aligned}
\end{split}\]</div>
<p>We pause a while to note that <span class="math notranslate nohighlight">\(\mathbb{P}[X_1]\)</span> is the probability of observing <span class="math notranslate nohighlight">\(X_1=x_1\)</span>,
which is independent of <span class="math notranslate nohighlight">\(Y\)</span>, so we can compute it using the law of total probability.</p>
<p>Of course, this is also merely the relative frequency of <span class="math notranslate nohighlight">\(X_1=0\)</span> in the dataset, which is expected
by intuition, but we will never know the actual probability of <span class="math notranslate nohighlight">\(X_1=0\)</span>. We can also
estimate it using Binomial distribution if we assume that it is a Bernoulli process, but
if it is continuous, then it is often harder to estimate it. We will see later that
we can omit the denominator since it is “constant” for all <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<p>Finally, by Bayes’ rule, we get the posterior probability of <span class="math notranslate nohighlight">\(Y=A\)</span> given <span class="math notranslate nohighlight">\(X_1=0\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{P}[Y=A|X_1=0] &amp;= \dfrac{\mathbb{P}[X_1=0|Y=A] \mathbb{P}[Y=A]}{\mathbb{P}[X_1=0]} \\
&amp;= \dfrac{151/342 \times 126/151}{193/342} \\
&amp;= \dfrac{126}{193} = 0.6528
\end{aligned}
\end{split}\]</div>
<p>which is the same as the one we calculated earlier using the empirical joint distribution table.</p>
<p>In a similar fashion, we get the posterior probability of <span class="math notranslate nohighlight">\(Y=C\)</span> given <span class="math notranslate nohighlight">\(X_1=0\)</span>, and <span class="math notranslate nohighlight">\(Y=G\)</span> given <span class="math notranslate nohighlight">\(X_1=0\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{P}[Y=C|X_1=0] &amp;= \dfrac{68/342 \times 61/68}{193/342} = \dfrac{61}{193} = 0.3161 \\
\mathbb{P}[Y=G|X_1=0] &amp;= \dfrac{123/342 \times 6/123}{193/342} = \dfrac{6}{193} = 0.0311
\end{aligned}
\end{split}\]</div>
<p>And the argmax of these three posterior probabilities is <span class="math notranslate nohighlight">\(Y=A\)</span>, so we predict that the penguin is of species <span class="math notranslate nohighlight">\(A\)</span>.
We observe that to get the argmax, we do not need the denominator, because it is the same for all <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<p>So we can omit the denominator, and we get the same result, it may no longer sum to 1, but
it does not affect the final result.</p>
<p>We also saw here that if our prior is very low, then even though the likelihood is high, the posterior can still be
low if <code class="docutils literal notranslate"><span class="pre">prior</span> <span class="pre">&lt;&lt;</span> <span class="pre">likelihood</span></code>. This is the reason why we need to use prior knowledge to help us make better predictions.</p>
</section>
<section id="one-quantitative-predictor">
<h3>One Quantitative Predictor<a class="headerlink" href="#one-quantitative-predictor" title="Permalink to this headline">#</a></h3>
<p>We now ignore the earlier categorical predictor <span class="math notranslate nohighlight">\(X_1\)</span> and focus on the quantitative predictor <span class="math notranslate nohighlight">\(X_2\)</span>.
This penguin has a <code class="docutils literal notranslate"><span class="pre">bill_length_mm</span></code> of 50mm, so we want to predict the species of the penguin given this information.</p>
<p>We know that as we move on to continuous space, we can no longer use “relative frequency” to estimate the probability of a continuous variable happening, as we will see later.</p>
<p>Let’s do some EDA.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>bill_length_mm = penguins[[&#39;bill_length_mm&#39;, &#39;species&#39;]]
bill_length_mm
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>bill_length_mm</th>
      <th>species</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>39.1</td>
      <td>Adelie</td>
    </tr>
    <tr>
      <th>1</th>
      <td>39.5</td>
      <td>Adelie</td>
    </tr>
    <tr>
      <th>2</th>
      <td>40.3</td>
      <td>Adelie</td>
    </tr>
    <tr>
      <th>3</th>
      <td>36.7</td>
      <td>Adelie</td>
    </tr>
    <tr>
      <th>4</th>
      <td>39.3</td>
      <td>Adelie</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>337</th>
      <td>47.2</td>
      <td>Gentoo</td>
    </tr>
    <tr>
      <th>338</th>
      <td>46.8</td>
      <td>Gentoo</td>
    </tr>
    <tr>
      <th>339</th>
      <td>50.4</td>
      <td>Gentoo</td>
    </tr>
    <tr>
      <th>340</th>
      <td>45.2</td>
      <td>Gentoo</td>
    </tr>
    <tr>
      <th>341</th>
      <td>49.9</td>
      <td>Gentoo</td>
    </tr>
  </tbody>
</table>
<p>342 rows × 2 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># plotting both distibutions on the same figure
_ = sns.kdeplot(data=penguins, x=&quot;bill_length_mm&quot;, fill=True, common_norm=False, alpha=.5, linewidth=0, legend=True)
# plot vertical line 
_ = plt.axvline(x=penguins[&quot;bill_length_mm&quot;].mean(), color=&#39;red&#39;, linestyle=&#39;--&#39;)
# plt.legend()
plt.show();
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/naive_bayes_example_48_0.png" src="../../../_images/naive_bayes_example_48_0.png" />
</div>
</div>
<p>The kdeplot above is for an univariate, empirical estimation of the whole dataset, not
conditional on any class label.</p>
<p>To see the conditional distribution, we can use the <code class="docutils literal notranslate"><span class="pre">hue</span></code> parameter in <code class="docutils literal notranslate"><span class="pre">seaborn.kdeplot</span></code> to plot the conditional distribution of <span class="math notranslate nohighlight">\(X_2\)</span> given <span class="math notranslate nohighlight">\(Y=y\)</span>. You just imagine that given
say Adelie has happened, then we zoom into the reduced sample space of Adelie, and plot the distribution of <span class="math notranslate nohighlight">\(X_2\)</span> in this <strong>reduced sample space</strong>. Things get a bit more complicated when we have more than one predictor, but the idea is the same, we will see later as well.</p>
<p>I want to emphasize that for 1 predictor conditioned on another random variable,
in this case <span class="math notranslate nohighlight">\(X_2\)</span> conditioned on <span class="math notranslate nohighlight">\(Y\)</span>, the conditional distribution is a <strong>marginal distribution</strong>,
or we can view it as <strong>univariate distribution</strong>. This is because once we <span class="math notranslate nohighlight">\(Y=y\)</span> has happened,
there is no randomness left in <span class="math notranslate nohighlight">\(Y\)</span>, we are only looking at the distribution of <span class="math notranslate nohighlight">\(X_2\)</span> in the reduced sample space <span class="math notranslate nohighlight">\(\mathcal{\Omega}_{X_2|Y}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># plotting both distibutions on the same figure
x2 = 50
_ = sns.kdeplot(
    data=penguins,
    x=&quot;bill_length_mm&quot;,
    hue=&quot;species&quot;,
    fill=True,
    common_norm=False,
    alpha=0.5,
    linewidth=0,
    legend=True,
)
# plot vertical line
_ = plt.axvline(x2, color=&quot;r&quot;, linestyle=&quot;--&quot;, label=&quot;50 mm&quot;)
plt.title(&quot;Density plot of bill length&quot;)
# plt.legend()
plt.show();
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/naive_bayes_example_50_0.png" src="../../../_images/naive_bayes_example_50_0.png" />
</div>
</div>
<p>Before we move on, some EDA on the joint distribution of <span class="math notranslate nohighlight">\(X_2\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.
Note very carefully that this is a <strong>joint distribution</strong> of <span class="math notranslate nohighlight">\(X_2\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, not a <strong>conditional distribution</strong> of <span class="math notranslate nohighlight">\(X_2\)</span> given <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>g = sns.JointGrid(data=penguins, x=&quot;bill_length_mm&quot;, y=&quot;species&quot;)
g.plot_joint(sns.histplot, color = &quot;red&quot;, cbar=True)

sns.jointplot(data=penguins, x=&quot;bill_length_mm&quot;, y=&quot;species&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;seaborn.axisgrid.JointGrid at 0x7fcb5089feb0&gt;
</pre></div>
</div>
<img alt="../../../_images/naive_bayes_example_52_1.png" src="../../../_images/naive_bayes_example_52_1.png" />
<img alt="../../../_images/naive_bayes_example_52_2.png" src="../../../_images/naive_bayes_example_52_2.png" />
</div>
</div>
<p>We see that the for this penguin with <code class="docutils literal notranslate"><span class="pre">bill_length_mm</span></code> of 50mm, our gut feeling tells us
that it most likely is not an Adelie, since the distribution of <span class="math notranslate nohighlight">\(X_2\)</span> for Adelie is
much less than 50 mm. It could be a Chinstrap or a Gentoo, but we are not sure which one,
though Chinstrap is tends to be longer for bill length, but the earlier section has
told us that we should weigh the prior probability of each species into consideration.</p>
<p>We can again use Bayes’ rule to compute the posterior probability of <span class="math notranslate nohighlight">\(Y=A\)</span> given <span class="math notranslate nohighlight">\(X_2=50\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\mathbb{P}[Y=y|X_2=x_2] = \frac{\mathbb{P}[X_2=x_2|Y=y] \mathbb{P}[Y=y]}{\mathbb{P}[X_2=x_2]}
\end{aligned}
\]</div>
<p>For example, since our given feature is <code class="docutils literal notranslate"><span class="pre">bill_length_mm</span></code> of 50mm, we see</p>
<div class="math notranslate nohighlight">
\[\begin{split} 
\begin{aligned}
\mathbb{P}[Y=A|X_2=50] &amp;= \dfrac{\mathbb{P}[X_2=50|Y=A] \mathbb{P}[Y=A]}{\mathbb{P}[X_2=50]} \\
\mathbb{P}[Y=C|X_2=50] &amp;= \dfrac{\mathbb{P}[X_2=50|Y=C] \mathbb{P}[Y=C]}{\mathbb{P}[X_2=50]} \\
\mathbb{P}[Y=G|X_2=50] &amp;= \dfrac{\mathbb{P}[X_2=50|Y=G] \mathbb{P}[Y=G]}{\mathbb{P}[X_2=50]}
\end{aligned}
\end{split}\]</div>
<p>Again, we use argmax to solve the problem,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\text{argmax}_{y \in \{A, C, G\}} \mathbb{P}[Y=y|X_2=50] &amp;= \text{argmax}_{y \in \{A, C, G\}} \dfrac{\mathbb{P}[X_2=50|Y=y] \mathbb{P}[Y=y]}{\mathbb{P}[X_2=50]} \\
&amp;= \text{argmax}_{y \in \{A, C, G\}} \dfrac{\mathbb{P}[X_2=50|Y=y] \mathbb{P}[Y=y]}{\sum_{y' \in \{A, C, G\}} \mathbb{P}[X_2=50|Y=y'] \mathbb{P}[Y=y']}
\end{aligned}
\end{split}\]</div>
<p>Now we met our first hurdle here since <span class="math notranslate nohighlight">\(\mathbb{P}[X_2=50|Y=y]\)</span> is not easy to compute since
<span class="math notranslate nohighlight">\(X_2\)</span> is a continuous variable, we cannot construct a empirical joint distribution table like
how we did earlier (<code class="docutils literal notranslate"><span class="pre">species</span></code> vs <code class="docutils literal notranslate"><span class="pre">bill_length_mm</span></code> does not work here).</p>
<p>Further, we haven’t assumed a model for <span class="math notranslate nohighlight">\(X_2\)</span> yet from which to define the likelihood <span class="math notranslate nohighlight">\(\mathbb{P}[X_2=50|Y=y]\)</span> or <span class="math notranslate nohighlight">\(\mathcal{L}(X_2=50|Y=y)\)</span>. We can do like previously to assume <strong>naively (pun intended)</strong> that the distribution of <span class="math notranslate nohighlight">\(X_2\)</span> <strong>GIVEN Y</strong> is Gaussian, note carefully
that this is a <strong>conditional distribution</strong> of <span class="math notranslate nohighlight">\(X_2\)</span> given <span class="math notranslate nohighlight">\(Y\)</span>, we can also say in same
term that <span class="math notranslate nohighlight">\(X_2\)</span> is <strong>continuous</strong> and <strong>conditionally normal</strong>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
X_2 \mid Y=A \sim \mathcal{N}(\mu_A, \sigma_A^2) \\
X_2 \mid Y=C \sim \mathcal{N}(\mu_C, \sigma_C^2) \\
X_2 \mid Y=G \sim \mathcal{N}(\mu_G, \sigma_G^2)
\end{aligned}
\end{split}\]</div>
<p>Notice here that it is possible that the three conditional distributions are different with
different <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>. Technically, we can even assume that the three conditional
distributions come from different families, but we will stick to the Gaussian family for now.
We usually call this the <strong>Gaussian Naive Bayes</strong> model.</p>
<p>Intuitively, we can see that the three species are not well separated in the joint distribution space, so we cannot expect a good performance from the Gaussian Naive Bayes model. For example,
50 mm could well be either Gentoo or Chinstrap. (This understanding may be wrong?) In any case,
in our example, the conditional distribution of <span class="math notranslate nohighlight">\(X_2\)</span> given <span class="math notranslate nohighlight">\(Y\)</span> appears quite Gaussian.
Even if not so, the Central Limit Theorem tells us that the sum of many independent random
variables tends to be Gaussian, so we can still use the Gaussian Naive Bayes model
if we have enough data (?).</p>
<p>The next question is how do we estimate the <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> for each <span class="math notranslate nohighlight">\(X_2\)</span> given <span class="math notranslate nohighlight">\(Y\)</span>?
We can use the <strong>Maximum Likelihood Estimation</strong> (MLE) method to estimate the parameters (clarify
whether it is MLE of MAP?)</p>
<p>It turns out that the MLE of <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> for each <span class="math notranslate nohighlight">\(X_2\)</span> given <span class="math notranslate nohighlight">\(Y\)</span> is the sample mean
and sample standard deviation of <span class="math notranslate nohighlight">\(X_2\)</span> given <span class="math notranslate nohighlight">\(Y\)</span>. They are unbiased estimators of the true
<span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>! (<strong>FILL In the blanks for the categorical part on Binomial and multinomial</strong>)
See bishops proof also.</p>
<p>The below table summarizes the sample (empirical) mean and sample standard deviation of <span class="math notranslate nohighlight">\(X_2\)</span> given <span class="math notranslate nohighlight">\(Y\)</span>.
For example, the sample mean of <span class="math notranslate nohighlight">\(X_2\)</span> given <span class="math notranslate nohighlight">\(Y=A\)</span> is 38.8 mm, and the sample standard deviation
is 2.66 mm.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Species</p></th>
<th class="head"><p>Sample Mean</p></th>
<th class="head"><p>Sample Standard Deviation</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>A</p></td>
<td><p>38.8</p></td>
<td><p>2.66</p></td>
</tr>
<tr class="row-odd"><td><p>C</p></td>
<td><p>48.8</p></td>
<td><p>3.34</p></td>
</tr>
<tr class="row-even"><td><p>G</p></td>
<td><p>47.5</p></td>
<td><p>3.08</p></td>
</tr>
</tbody>
</table>
<p>Remember again, the sample mean and sample standard deviation are unbiased estimators of the true
<span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>, and can be shown by MLE.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>bill_length_mean_std = bill_length_mm.groupby(&#39;species&#39;).agg([&#39;mean&#39;, &#39;std&#39;])
bill_length_mean_std
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }

    .dataframe thead tr:last-of-type th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="2" halign="left">bill_length_mm</th>
    </tr>
    <tr>
      <th></th>
      <th>mean</th>
      <th>std</th>
    </tr>
    <tr>
      <th>species</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Adelie</th>
      <td>38.791391</td>
      <td>2.663405</td>
    </tr>
    <tr>
      <th>Chinstrap</th>
      <td>48.833824</td>
      <td>3.339256</td>
    </tr>
    <tr>
      <th>Gentoo</th>
      <td>47.504878</td>
      <td>3.081857</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Let’s plot the normal distribution with the sample mean and sample standard deviation for each <span class="math notranslate nohighlight">\(X_2\)</span> given <span class="math notranslate nohighlight">\(Y\)</span>.
We also round off the sample mean and sample standard deviation to 2 decimal places for convenience and
stay true to the original book.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># rv_adelie = stats.norm(bill_length_mean_std.loc[&#39;Adelie&#39;][&#39;bill_length_mm&#39;][&#39;mean&#39;], bill_length_mean_std.loc[&#39;Adelie&#39;][&#39;bill_length_mm&#39;][&#39;std&#39;])
# rv_chinstrap = stats.norm(bill_length_mean_std.loc[&#39;Chinstrap&#39;][&#39;bill_length_mm&#39;][&#39;mean&#39;], bill_length_mean_std.loc[&#39;Chinstrap&#39;][&#39;bill_length_mm&#39;][&#39;std&#39;])
# rv_gentoo = stats.norm(bill_length_mean_std.loc[&#39;Gentoo&#39;][&#39;bill_length_mm&#39;][&#39;mean&#39;], bill_length_mean_std.loc[&#39;Gentoo&#39;][&#39;bill_length_mm&#39;][&#39;std&#39;])

rv_adelie = stats.norm(38.8, 2.66)
rv_chinstrap = stats.norm(48.8, 3.34)
rv_gentoo = stats.norm(47.5, 3.08)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>fig, ax = plt.subplots(1, 1)
ax.plot(np.linspace(30, 60, 100), rv_adelie.pdf(np.linspace(30, 60, 100)), &#39;r-&#39;, lw=5, alpha=0.6, label=&#39;Adelie&#39;)
ax.plot(np.linspace(30, 60, 100), rv_chinstrap.pdf(np.linspace(30, 60, 100)), &#39;b-&#39;, lw=5, alpha=0.6, label=&#39;Chinstrap&#39;)
ax.plot(np.linspace(30, 60, 100), rv_gentoo.pdf(np.linspace(30, 60, 100)), &#39;g-&#39;, lw=5, alpha=0.6, label=&#39;Gentoo&#39;)
# plot vertical line
_ = plt.axvline(x2, color=&quot;y&quot;, linestyle=&quot;--&quot;, label=&quot;50 mm&quot;)
_ = plt.axvline(x2+0.5, color=&quot;y&quot;, linestyle=&quot;--&quot;, label=&quot;50.5 mm&quot;)
_ = plt.axvline(x2-0.5, color=&quot;y&quot;, linestyle=&quot;--&quot;, label=&quot;49.5 mm&quot;)
ax.set_xlabel(&#39;bill_length_mm&#39;)
ax.set_ylabel(&#39;Probability density&#39;)
ax.legend()
plt.show();
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/naive_bayes_example_62_0.png" src="../../../_images/naive_bayes_example_62_0.png" />
</div>
</div>
<p>As we can see, the this naive assumption of Gaussian distribution for <span class="math notranslate nohighlight">\(X_2\)</span> given <span class="math notranslate nohighlight">\(Y\)</span> is not
perfect, but it isn’t too bad either. Note the distinction in the plot here and previously.
The previous diagram is the empirical density plot of the raw data, while this one is the
density plot of the conditional gaussian distribution of <span class="math notranslate nohighlight">\(X_2\)</span> given <span class="math notranslate nohighlight">\(Y\)</span> where we have
used the sample mean and sample standard deviation as the parameters of the Gaussian <span class="math notranslate nohighlight">\(X_2 \mid Y \sim \mathcal{N}(\mu, \sigma^2)\)</span>.</p>
<p>Now we can finally solve our “hurdle” problem of computing <span class="math notranslate nohighlight">\(\mathbb{P}[X_2=50|Y=y]\)</span>.
We can use the Gaussian density function to compute the probability density of <span class="math notranslate nohighlight">\(X_2=50\)</span> given <span class="math notranslate nohighlight">\(Y=y\)</span>.
Geometrically, we can see that the probability density of <span class="math notranslate nohighlight">\(X_2=50\)</span> given <span class="math notranslate nohighlight">\(Y=y\)</span> is the area under the
curve of the Gaussian density function at <span class="math notranslate nohighlight">\(X_2=50\)</span> for each <span class="math notranslate nohighlight">\(Y=y\)</span> around a small neighborhood <span class="math notranslate nohighlight">\(\delta\)</span>
of <span class="math notranslate nohighlight">\(X_2=50\)</span>. Illustrated in diagram above, the <span class="math notranslate nohighlight">\(\delta=0.5\)</span> mm and the area under the curve is
computed by the integral of the Gaussian density function from <span class="math notranslate nohighlight">\(X_2=49.5\)</span> to <span class="math notranslate nohighlight">\(X_2=50.5\)</span>. In reality however,
the <span class="math notranslate nohighlight">\(\delta\)</span> is infinitesimally small.</p>
<p>In practice, we just need to compute the probability density of <span class="math notranslate nohighlight">\(X_2=50\)</span> given <span class="math notranslate nohighlight">\(Y=y\)</span> at <span class="math notranslate nohighlight">\(X_2=50\)</span> and
not the actual probability. Note very carefully that PDF is not probability, it is the probability density!!!</p>
<p>So one natural question becomes, as mentioned earlier, we should compute
the posterior conditional PDF and maximize over it instead of the posterior conditional probability
and maximize over it since probability at a point is 0 for continuous variables.</p>
<p>And so our</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
f_{X_2 \mid Y}(x_2=50 \mid y=A) &amp;= \frac{1}{\sqrt{2\pi}\sigma_A} \exp \left( -\frac{(x_2 - \mu_A)^2}{2\sigma_A^2} \right) \\
&amp;= \frac{1}{\sqrt{2\pi}\sigma_A} \exp \left( -\frac{(50 - \mu_A)^2}{2\sigma_A^2} \right) \\
&amp;= \frac{1}{\sqrt{38.8} \cdot 2.66} \exp \left( -\frac{(50 - 38.8)^2}{2 \cdot 2.66^2} \right) \\
&amp;= 0.0000212 
\end{aligned}
\end{split}\]</div>
<p>Similarly,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
f_{X_2 \mid Y}(x_2=50 \mid y=C) &amp;= 0.112 \\
f_{X_2 \mid Y}(x_2=50 \mid y=G) &amp;= 0.09317 \\
\end{aligned}
\end{split}\]</div>
<p>Note that <span class="math notranslate nohighlight">\(\mu_A\)</span> and <span class="math notranslate nohighlight">\(\sigma_A\)</span> are the sample mean and sample standard deviation of <span class="math notranslate nohighlight">\(X_2\)</span> given <span class="math notranslate nohighlight">\(Y=A\)</span>.
Notation wise it is fine but you can also view it as <span class="math notranslate nohighlight">\(\mu_{A|X_2}\)</span> and <span class="math notranslate nohighlight">\(\sigma_{A|X_2}\)</span>.</p>
<p>READ HERE, DON’T BE LAZY:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.probabilitycourse.com/chapter5/5_2_3_conditioning_independence.php">https://www.probabilitycourse.com/chapter5/5_2_3_conditioning_independence.php</a></p></li>
<li><p><a class="reference external" href="https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/maximum-likelihood.html#summary">https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/maximum-likelihood.html#summary</a></p></li>
<li><p>Chan’s book.</p></li>
<li><p>Write out into content for my notes!!!</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>px2_given_adelie = rv_adelie.pdf(x2)
px2_given_chinstrap = rv_chinstrap.pdf(x2)
px2_given_gentoo = rv_gentoo.pdf(x2)

print(f&#39;P(X_2=50|Adelie) = {px2_given_adelie:.7f}&#39;)
print(f&#39;P(X_2=50|Chinstrap) = {px2_given_chinstrap:.3f}&#39;)
print(f&#39;P(X_2=50|Gentoo) = {px2_given_gentoo:.5f}&#39;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>P(X_2=50|Adelie) = 0.0000212
P(X_2=50|Chinstrap) = 0.112
P(X_2=50|Gentoo) = 0.09317
</pre></div>
</div>
</div>
</div>
<p>As an example, the probability density of <span class="math notranslate nohighlight">\(X_2=50\)</span> given <span class="math notranslate nohighlight">\(Y=A\)</span> is <span class="math notranslate nohighlight">\(0.0000212 \text{ mm}^{-1}\)</span>, and indeed from
the plot above, the area under the curve is definitely the smallest, and the lowest since there is
almost no Adelie with 50 mm bill length.</p>
<p>So the marginal PDF of observing a penguin with a 50 mm bill length is,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
f_{X_2}(50) &amp;= \sum_{y \in \mathcal{Y}} f_{X_2|Y}(50|y) f_{Y}(y) \\
&amp;= \frac{151}{342} \cdot 0.0000212 + \frac{68}{342} \cdot 0.112 + \frac{123}{342} \cdot 0.09317 \\
&amp;= 0.05579
\end{aligned}
\end{split}\]</div>
<p>Once again, this number is not a probability, it is the probability density, it just means for
every 1 mm, there is a 0.05579 mm chance of observing a penguin with a 50 mm bill length AROUND THAT SMALL NEIGHBORHOOD.
In laymen, it is how “dense” the probability is at that point.</p>
<p>Then, the posterior conditional PDF of <span class="math notranslate nohighlight">\(Y\)</span> given <span class="math notranslate nohighlight">\(X_2=50\)</span> is,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
f_{Y|X_2}(y=A|x_2=50) &amp;= \frac{f_{X_2|Y}(x_2=50|y=A) f_{Y}(y=A)}{f_{X_2}(x_2=50)} \\
&amp;= \dfrac{(151/342) \cdot 0.0000212}{0.05579} \\
&amp;\approx 0.0002
\end{aligned}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
f_{Y|X_2}(y=C|x_2=50) \approx 0.3992 \\
\end{aligned}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
f_{Y|X_2}(y=G|x_2=50) \approx 0.6006 \\
\end{aligned}
\end{split}\]</div>
<p>Again note that these are not probabilities, they are probability densities. This time round,
the final results was pushed over again by the fact that Gentoo is more common in the wild (prior <span class="math notranslate nohighlight">\(P(Y=G)\approx 0.3605\)</span>).</p>
</section>
<section id="two-predictors">
<h3>Two Predictors<a class="headerlink" href="#two-predictors" title="Permalink to this headline">#</a></h3>
<p>The reality is that the data is usually multi-dimensional, and we need to use multiple predictors
to make a prediction.</p>
<p>In the previous two examples, we have seen that for the same penguin with the following features,</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">body_mass_g</span></code> <span class="math notranslate nohighlight">\(&lt; 4200 \mathrm{~g}\)</span> (<code class="docutils literal notranslate"><span class="pre">overweight</span></code> = 0 if <code class="docutils literal notranslate"><span class="pre">body_mass_g</span></code> <span class="math notranslate nohighlight">\(&lt; 4200 \mathrm{~g}\)</span>, 1 otherwise)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bill_length_mm</span></code> <span class="math notranslate nohighlight">\(50 \mathrm{~mm}\)</span></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">flipper_length_mm</span></code> <span class="math notranslate nohighlight">\(195 \mathrm{~mm}\)</span></p></li>
</ul>
<p>the predictions for the species is an Adelie if we only use <code class="docutils literal notranslate"><span class="pre">overweight</span></code>, and
a Gentoo if we only use <code class="docutils literal notranslate"><span class="pre">bill_length_mm</span></code>. This inconsistency suggest that
our model may have room for improvement. Intuitively, we can add
multiple predictors to our model to make a more accurate prediction, though
this is not always the case if the predictors are correlated, or if there are
too many predictors (curse of dimensionality).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># plotting both distibutions on the same figure
_ = sns.kdeplot(data=penguins, x=&quot;flipper_length_mm&quot;, fill=True, common_norm=False, alpha=.5, linewidth=0, legend=True)
# plot vertical line 
_ = plt.axvline(x=penguins[&quot;flipper_length_mm&quot;].mean(), color=&#39;red&#39;, linestyle=&#39;--&#39;)
# plt.legend()
plt.show();
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/naive_bayes_example_70_0.png" src="../../../_images/naive_bayes_example_70_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># plotting both distibutions on the same figure
x3 = 195
_ = sns.kdeplot(
    data=penguins,
    x=&quot;flipper_length_mm&quot;,
    hue=&quot;species&quot;,
    fill=True,
    common_norm=False,
    alpha=0.5,
    linewidth=0,
    legend=True,
)
# plot vertical line
_ = plt.axvline(x3, color=&quot;r&quot;, linestyle=&quot;--&quot;, label=&quot;195 mm&quot;)
plt.title(&quot;Density plot of flipper length&quot;)
# plt.legend()
plt.show();
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/naive_bayes_example_71_0.png" src="../../../_images/naive_bayes_example_71_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># plotting both distibutions on the same figure
x2 = 50
_ = sns.kdeplot(
    data=penguins,
    x=&quot;bill_length_mm&quot;,
    hue=&quot;species&quot;,
    fill=True,
    common_norm=False,
    alpha=0.5,
    linewidth=0,
    legend=True,
)
# plot vertical line
_ = plt.axvline(x2, color=&quot;r&quot;, linestyle=&quot;--&quot;, label=&quot;50 mm&quot;)
plt.title(&quot;Density plot of bill length&quot;)
# plt.legend()
plt.show();
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/naive_bayes_example_72_0.png" src="../../../_images/naive_bayes_example_72_0.png" />
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">bill_length_mm</span></code> and <code class="docutils literal notranslate"><span class="pre">flipper_length_mm</span></code> are plotted above as density plots,
just by looking at them alone may be hard to distinguish between the
Chinstrap and Gentoo for the <code class="docutils literal notranslate"><span class="pre">bill_length_mm</span></code> plot because they are
quite close (overlap in density curve) with each other, while if you
only look at the <code class="docutils literal notranslate"><span class="pre">flipper_length_mm</span></code> plot, you can see that the Chinstrap and
Adelie are now quite close to each other, but if you combine
both <code class="docutils literal notranslate"><span class="pre">bill_length_mm</span></code> and <code class="docutils literal notranslate"><span class="pre">flipper_length_mm</span></code> together, you can see that
it is much easier to distinguish between them.</p>
<section id="plots">
<h4>Plots!!!<a class="headerlink" href="#plots" title="Permalink to this headline">#</a></h4>
<p>This plot is from <a class="reference external" href="https://seaborn.pydata.org/tutorial/distributions.html#distribution-visualization-in-other-settings">Seaborn guide</a>, one can see with default setting
we have a joint plot of <code class="docutils literal notranslate"><span class="pre">bill_length_mm</span></code> and <code class="docutils literal notranslate"><span class="pre">flipper_length_mm</span></code> with the marginal
histograms on the side. The joint plot is a scatter plot of the two variables.</p>
<p>The plot with <code class="docutils literal notranslate"><span class="pre">kind=hist</span></code> will show the joint distribution as a histogram, and the
rectangular bins are colored by the density of the points in each bin. The darker
the color the more points in that bin. You can understand this as impulses from chan’s book.</p>
<p>Lastly, the plot with <code class="docutils literal notranslate"><span class="pre">kind=kde</span></code> will show the joint distribution as a contour plot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>sns.jointplot(data=penguins, x=&quot;bill_length_mm&quot;, y=&quot;flipper_length_mm&quot;, color=&quot;orange&quot;)
sns.jointplot(
    data=penguins,
    x=&quot;bill_length_mm&quot;,
    y=&quot;flipper_length_mm&quot;,
    kind=&quot;hist&quot;,
    color=&quot;orange&quot;,
    cbar=True,
)
sns.jointplot(
    data=penguins,
    x=&quot;bill_length_mm&quot;,
    y=&quot;flipper_length_mm&quot;,
    kind=&quot;kde&quot;,
    color=&quot;orange&quot;,
    # cbar=True,
)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;seaborn.axisgrid.JointGrid at 0x7fcb5029af70&gt;
</pre></div>
</div>
<img alt="../../../_images/naive_bayes_example_76_1.png" src="../../../_images/naive_bayes_example_76_1.png" />
<img alt="../../../_images/naive_bayes_example_76_2.png" src="../../../_images/naive_bayes_example_76_2.png" />
<img alt="../../../_images/naive_bayes_example_76_3.png" src="../../../_images/naive_bayes_example_76_3.png" />
</div>
</div>
<p>To see the joint distribution conditioned on the class (species) <span class="math notranslate nohighlight">\(Y\)</span>, where <span class="math notranslate nohighlight">\(Y\)</span>
can be treated as a random variable as well <a class="footnote-reference brackets" href="#id4" id="id2">1</a>, we can use the <code class="docutils literal notranslate"><span class="pre">hue</span></code> argument.</p>
<p>To emphasise, the way to approach conditional distributions is to zoom in on the
“reduced” sample space, for example if I want to look at the conditional distribution
of <span class="math notranslate nohighlight">\(X_2\)</span> and <span class="math notranslate nohighlight">\(X_3\)</span> conditioned on <span class="math notranslate nohighlight">\(Y=G\)</span>, then you should look at the green colored
hues.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>sns.jointplot(data=penguins, x=&quot;bill_length_mm&quot;, y=&quot;flipper_length_mm&quot;, hue=&quot;species&quot;)
sns.jointplot(
    data=penguins, x=&quot;bill_length_mm&quot;, y=&quot;flipper_length_mm&quot;, hue=&quot;species&quot;, kind=&quot;hist&quot;
)
sns.jointplot(
    data=penguins, x=&quot;bill_length_mm&quot;, y=&quot;flipper_length_mm&quot;, hue=&quot;species&quot;, kind=&quot;kde&quot;
)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;seaborn.axisgrid.JointGrid at 0x7fcb4cde0640&gt;
</pre></div>
</div>
<img alt="../../../_images/naive_bayes_example_78_1.png" src="../../../_images/naive_bayes_example_78_1.png" />
<img alt="../../../_images/naive_bayes_example_78_2.png" src="../../../_images/naive_bayes_example_78_2.png" />
<img alt="../../../_images/naive_bayes_example_78_3.png" src="../../../_images/naive_bayes_example_78_3.png" />
</div>
</div>
<p>Now we plot two dashed lines for where <span class="math notranslate nohighlight">\(X_2=50\)</span> and <span class="math notranslate nohighlight">\(X_3=195\)</span> respectively, and
see that when combined together, the penguin lies amongst the Chinstrap species,
and it is not even close to the other two speces!!</p>
<p>To internalize this concept, we are again looking at the argmax expression below:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\hat{y} &amp;= \arg\max_{y \in \mathcal{Y}} \mathbb{P}(Y=y|X_2=x_2,X_3=x_3) \\
        &amp;= \arg\max_{y \in \mathcal{Y}} \mathbb{P}(Y=y|X_2=50,X_3=195) \\
\end{aligned}
\end{split}\]</div>
<p>or equivalently,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\hat{y} &amp;= \arg\max_{y \in \mathcal{Y}} f_{Y|X_2,X_3}(y|x_2,x_3) \\
        &amp;= \arg\max_{y \in \mathcal{Y}} f_{Y|X_2,X_3}(y|50,195) \\
\end{aligned}
\end{split}\]</div>
<p>We want to know <strong>given each species <span class="math notranslate nohighlight">\(Y\)</span></strong>, which <span class="math notranslate nohighlight">\(y\)</span> <strong>maximizes</strong>
this conditional probability density function <span class="math notranslate nohighlight">\(f_{Y|X_2,X_3}(y|x_2,x_3)\)</span>.
In other words, given <span class="math notranslate nohighlight">\(X_2=50\)</span> and <span class="math notranslate nohighlight">\(X_3=195\)</span>, which species <span class="math notranslate nohighlight">\(Y\)</span> has the highest
“probability” of being the true species of the penguin. And from the diagram,
we can see that the Chinstrap species has the highest probability of being the true
species of the penguin. We will see later that the geometric interpretation of this
is correct, with the formula.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># sns.jointplot(data=penguins, x=&quot;bill_length_mm&quot;, y=&quot;flipper_length_mm&quot;, hue=&quot;species&quot;)
sns.jointplot(
    data=penguins, x=&quot;bill_length_mm&quot;, y=&quot;flipper_length_mm&quot;, hue=&quot;species&quot;, kind=&quot;hist&quot;
)

_ = plt.axvline(x2, color=&quot;black&quot;, linestyle=&quot;--&quot;, label=&quot;50 mm&quot;)
_ = plt.axhline(x3, color=&quot;black&quot;, linestyle=&quot;--&quot;, label=&quot;195 mm&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/naive_bayes_example_80_0.png" src="../../../_images/naive_bayes_example_80_0.png" />
</div>
</div>
</section>
<section id="conditional-independence">
<h4>Conditional Independence<a class="headerlink" href="#conditional-independence" title="Permalink to this headline">#</a></h4>
<p>Let’s use naive Bayes to make a prediction for the same penguin with the following features
<span class="math notranslate nohighlight">\(X_2=x_2\)</span> and <span class="math notranslate nohighlight">\(X_3=x_3\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
f_{Y|X_2,X_3}(y|x_2,x_3) &amp;= \frac{f_{X_2,X_3|Y}(x_2,x_3|y) f_{Y}(y)}{f_{X_2,X_3}(x_2,x_3)} \\
                         &amp;= \frac{f_{X_2,X_3|Y}(x_2,x_3|y) f_{Y}(y)}{\sum_{y \in \mathcal{Y}} f_{X_2,X_3|Y}(x_2,x_3|y) f_{Y}(y)} \\
\end{aligned}
\end{split}\]</div>
<p>Another hurdle is presented in front of us, we need to compute the conditional
joint distribution (PDF) <span class="math notranslate nohighlight">\(f_{X_2,X_3|Y}(x_2,x_3|y)\)</span>, which is not easy to do…in fact
I have no idea how to do it. This is because we were dealing with only one-dimensional
distribution, even though there are two variables just now, but as I mentioned,
when we condition on <span class="math notranslate nohighlight">\(Y\)</span>, we are looking at the reduced sample space of <span class="math notranslate nohighlight">\(X\)</span> in
which <span class="math notranslate nohighlight">\(Y\)</span> is fixed,
and not the joint sample space of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>. This is the reason why we can happily
use 1dimensional Gaussian to get the conditional distribution of <span class="math notranslate nohighlight">\(X_2\)</span>.</p>
<p>To reconcile this, Naive Bayes assumes that the predictors are <strong>conditionally independent</strong><a class="footnote-reference brackets" href="#id4" id="id3">1</a>.
It states that given a set of continuous predictors <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span>, they are called conditionally
independent given <span class="math notranslate nohighlight">\(Y\)</span> if the joint distribution of <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span> given <span class="math notranslate nohighlight">\(Y\)</span> can be written as</p>
<div class="math notranslate nohighlight">
\[
f_{X_1,\ldots,X_n|Y}(x_1,\ldots,x_n|y) = \prod_{i=1}^n f_{X_i|Y}(x_i|y)
\]</div>
<p>The theorem holds for discrete case as well with the difference being that the
PDF is replaced by the PMF.</p>
<p>This theorem means that <em><strong>within each class (species)</strong></em>, the predictors are independent.
More concretely, if we say that <span class="math notranslate nohighlight">\(X_2\)</span> and <span class="math notranslate nohighlight">\(X_3\)</span> are conditionally independent given <span class="math notranslate nohighlight">\(Y\)</span>,
then we are saying that <em><strong>within each class (species)</strong></em>, the <code class="docutils literal notranslate"><span class="pre">bill_length_mm</span></code> (<span class="math notranslate nohighlight">\(X_2\)</span>)
is independent of the <code class="docutils literal notranslate"><span class="pre">flipper_length_mm</span></code> (<span class="math notranslate nohighlight">\(X_3\)</span>). This is a very strong assumption,
but is at the heart of Naive Bayes and many other algorithms. It simplifies the parameters a lot
(see d2l on the parameters reduction). Such a strong assumption is usually not always true,
see the plot below again, within the class <code class="docutils literal notranslate"><span class="pre">Gentoo</span></code>, we can see that the <code class="docutils literal notranslate"><span class="pre">bill_length_mm</span></code>
and <code class="docutils literal notranslate"><span class="pre">flipper_length_mm</span></code> exhibit some positive correlation. We will still use this assumption
even though it is not perfect.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>sns.jointplot(data=penguins, x=&quot;bill_length_mm&quot;, y=&quot;flipper_length_mm&quot;, hue=&quot;species&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;seaborn.axisgrid.JointGrid at 0x7fcb4cc13e80&gt;
</pre></div>
</div>
<img alt="../../../_images/naive_bayes_example_82_1.png" src="../../../_images/naive_bayes_example_82_1.png" />
</div>
</div>
<p>Back to our</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
f_{Y|X_2,X_3}(y|x_2,x_3) &amp;= \frac{f_{X_2,X_3|Y}(x_2,x_3|y) f_{Y}(y)}{f_{X_2,X_3}(x_2,x_3)} \\
                         &amp;= \frac{f_{X_2,X_3|Y}(x_2,x_3|y) f_{Y}(y)}{\sum_{y \in \mathcal{Y}} f_{X_2,X_3|Y}(x_2,x_3|y) f_{Y}(y)} \\
\end{aligned}
\end{split}\]</div>
<p>We now have an answer on how to unpack the joint conditional distribution <span class="math notranslate nohighlight">\(f_{X_2,X_3|Y}(x_2,x_3|y)\)</span>.</p>
<p>We can merely do</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
f_{X_2,X_3|Y}(x_2,x_3|y) &amp;= f_{X_2|Y}(x_2|y) f_{X_3|Y}(x_3|y) \\
\end{aligned}
\end{split}\]</div>
<p>so that our previous expression becomes</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
f_{Y|X_2,X_3}(y|x_2,x_3) &amp;= \frac{f_{X_2|Y}(x_2|y) f_{X_3|Y}(x_3|y) f_{Y}(y)}{\sum_{y \in \mathcal{Y}} f_{X_2|Y}(x_2|y) f_{X_3|Y}(x_3|y) f_{Y}(y)} \\
\end{aligned}
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>flipper_length_mean_std = penguins.groupby(&quot;species&quot;)[&quot;flipper_length_mm&quot;].agg([&quot;mean&quot;, &quot;std&quot;]) 
flipper_length_mean_std
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>std</th>
    </tr>
    <tr>
      <th>species</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Adelie</th>
      <td>189.953642</td>
      <td>6.539457</td>
    </tr>
    <tr>
      <th>Chinstrap</th>
      <td>195.823529</td>
      <td>7.131894</td>
    </tr>
    <tr>
      <th>Gentoo</th>
      <td>217.186992</td>
      <td>6.484976</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>rv_adelie = stats.norm(190., 6.54)
rv_chinstrap = stats.norm(196., 7.13)
rv_gentoo = stats.norm(217., 6.48)

fig, ax = plt.subplots(1, 1)
ax.plot(np.linspace(170, 240, 100), rv_adelie.pdf(np.linspace(170, 240, 100)), &#39;r-&#39;, lw=5, alpha=0.6, label=&#39;Adelie&#39;)
ax.plot(np.linspace(170, 240, 100), rv_chinstrap.pdf(np.linspace(170, 240, 100)), &#39;b-&#39;, lw=5, alpha=0.6, label=&#39;Chinstrap&#39;)
ax.plot(np.linspace(170, 240, 100), rv_gentoo.pdf(np.linspace(170, 240, 100)), &#39;g-&#39;, lw=5, alpha=0.6, label=&#39;Gentoo&#39;)
# plot vertical line
_ = plt.axvline(x3, color=&quot;y&quot;, linestyle=&quot;--&quot;, label=&quot;195 mm&quot;)
_ = plt.axvline(x3+0.5, color=&quot;y&quot;, linestyle=&quot;--&quot;, label=&quot;195.5 mm&quot;)
_ = plt.axvline(x3-0.5, color=&quot;y&quot;, linestyle=&quot;--&quot;, label=&quot;194.5 mm&quot;)
ax.set_xlabel(&#39;flipper_length_mm&#39;)
ax.set_ylabel(&#39;Probability density&#39;)
ax.legend()
plt.show();
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/naive_bayes_example_86_0.png" src="../../../_images/naive_bayes_example_86_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>px3_given_adelie = rv_adelie.pdf(x3)
px3_given_chinstrap = rv_chinstrap.pdf(x3)
px3_given_gentoo = rv_gentoo.pdf(x3)

print(f&#39;P(X_3=195|Adelie) = {px3_given_adelie:.5f}&#39;)
print(f&#39;P(X_3=195|Chinstrap) = {px3_given_chinstrap:.5f}&#39;)
print(f&#39;P(X_3=195|Gentoo) = {px3_given_gentoo:.7f}&#39;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>P(X_3=195|Adelie) = 0.04554
P(X_3=195|Chinstrap) = 0.05541
P(X_3=195|Gentoo) = 0.0001934
</pre></div>
</div>
</div>
</div>
<p>For formality, the code above translates to</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
f_{X_3|Y}(x_3=195|y=A) &amp;= \frac{1}{\sqrt{2\pi}\sigma_{A}} \exp\left(-\frac{(195-\mu_{A})^2}{2\sigma_A^2}\right) \\
&amp;= 0.04554
\end{aligned}
\end{split}\]</div>
<p>Similarly, we can compute the other two conditional distributions to be <span class="math notranslate nohighlight">\(0.05541\)</span> and <span class="math notranslate nohighlight">\(0.0001934\)</span> respectively.</p>
<p>Next, we compute the denominator, which is the joint distribution of observing <span class="math notranslate nohighlight">\(X_2=50\)</span> and <span class="math notranslate nohighlight">\(X_3=195\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\sum_{y \in \mathcal{Y}} f_{X_2|Y}(x_2|y) f_{X_3|Y}(x_3|y) f_{Y}(y) &amp;= \sum_{y \in \mathcal{Y}} f_{X_2|Y}(x_2=50|y) f_{X_3|Y}(x_3=195|y) f_{Y}(y) \\
&amp;= f_{X_2|Y=A}(x_2=50|y=A) f_{X_3|Y=A}(x_3=195|y=A) f_{Y}(y=A) \\
&amp;+ f_{X_2|Y=C}(x_2=50|y=C) f_{X_3|Y=C}(x_3=195|y=C) f_{Y}(y=C) \\
&amp;+ f_{X_2|Y=G}(x_2=50|y=G) f_{X_3|Y=G}(x_3=195|y=G) f_{Y}(y=G) \\
&amp;= 0.0000212 \cdot 0.04554 \cdot \frac{151}{342} \\
&amp;+ 0.112 \cdot 0.05541 \cdot \frac{68}{342} \\
&amp;+ 0.09317 \cdot 0.0001934 \cdot \frac{123}{342} \\
&amp;\approx 0.001241
\end{aligned}
\end{split}\]</div>
<p>We plug into Bayes’ rule to get the posterior probability of <span class="math notranslate nohighlight">\(Y=A\)</span> given <span class="math notranslate nohighlight">\(X_2=50\)</span> and <span class="math notranslate nohighlight">\(X_3=195\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
f_{Y=A|X_2,X_3}(y=A|x_2=50,x_3=195) &amp;= \frac{f_{X_2|Y=A}(x_2=50|y=A) f_{X_3|Y=A}(x_3=195|y=A) f_{Y}(y=A)}{\sum_{y \in \mathcal{Y}} f_{X_2|Y}(x_2|y) f_{X_3|Y}(x_3|y) f_{Y}(y)} \\
&amp;= \frac{0.0000212 \cdot 0.04554 \cdot \frac{151}{342}}{0.001241} \\
&amp;\approx 0.0003
\end{aligned}
\end{split}\]</div>
<p>Similarly, we can compute the posterior probability of <span class="math notranslate nohighlight">\(Y=C\)</span> and <span class="math notranslate nohighlight">\(Y=G\)</span> given <span class="math notranslate nohighlight">\(X_2=50\)</span> and <span class="math notranslate nohighlight">\(X_3=195\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
f_{Y=C|X_2,X_3}(y=C|x_2=50,x_3=195) &amp;\approx 0.9944 \\
f_{Y=G|X_2,X_3}(y=G|x_2=50,x_3=195) &amp;\approx 0.0052
\end{aligned}
\end{split}\]</div>
<p>We then take the argmax of the posterior probabilities to get the prediction. In this case,
it is <span class="math notranslate nohighlight">\(Y=C\)</span> with a posterior probability of <span class="math notranslate nohighlight">\(0.9944\)</span>. To be pedantic, be reminded that this value is
not a probability but a</p>
</section>
</section>
<section id="three-predictors">
<h3>Three Predictors<a class="headerlink" href="#three-predictors" title="Permalink to this headline">#</a></h3>
<p>Even though not mentioned in the book, we can also use 3 predictors to make a prediction.
The three predictors are <code class="docutils literal notranslate"><span class="pre">overweight</span></code>, <code class="docutils literal notranslate"><span class="pre">bill_length_mm</span></code>, and <code class="docutils literal notranslate"><span class="pre">flipper_length_mm</span></code>. The tricky
part is that <code class="docutils literal notranslate"><span class="pre">overweight</span></code> is a categorical variable, and <code class="docutils literal notranslate"><span class="pre">bill_length_mm</span></code> and <code class="docutils literal notranslate"><span class="pre">flipper_length_mm</span></code>
are continuous variables. We can still do it though, <a class="reference external" href="https://dafriedman97.github.io/mlbook/content/c4/concept.html#model-structure">dafriedman97</a>
mentioned that we for a random vector <span class="math notranslate nohighlight">\(\mathrm{X_n} = (X_{n1}, X_{n2}, \ldots, X_{nd})\)</span>, the individual
predictors can take on different distributions. In our case, <code class="docutils literal notranslate"><span class="pre">overweight</span></code> is assumed to be follow a Bernoulli distribution,
<code class="docutils literal notranslate"><span class="pre">bill_length_mm</span></code> and <code class="docutils literal notranslate"><span class="pre">flipper_length_mm</span></code> are assumed to be follow a Gaussian distribution. Note again that
this is an assumption, and we can always change it to something else if we want to since Naive Bayes does
not have any assumptions on the distribution of the predictors (potentially confusing part!).</p>
<p>16/12/2022</p>
<p>once i get prob of 0.3992 , 0.6006 and 0.0002, we can then reconcile with the
conditional KDE plot above, and “eyeball” that for 50mm, the area under the
curve for a small neighbourhood <span class="math notranslate nohighlight">\(\delta\)</span> is indeed higher for Gentoo than
Chinstrap. Is this decision boundary? No I doubt so, clarify?</p>
<ul class="simple">
<li><p>prior is low but likelihood is high, so posterior may not be high if prior &lt;&lt; likelihood</p></li>
<li><p>Normalizing constant. argmax so it does not matter if output is probability? because for an image vector input of a certain x = [1,2,3] for eg, the P(Y=k|x) for Y = 0, 1, 2,…9 for that image are fixed at the given x, and as a result we need to compare the p(y|x) for diff y and see which is highest value. MOreover, it says that in D2L the sum of p(y|x) over y is 1, so we can cross multiply anwyays to get?</p>
<ul>
<li><p>Furthermore, <span class="math notranslate nohighlight">\(P(X_i=x_i)\)</span> is constant during argmax comparison, see 14.1.1 and 14.1.2 in bayes rules book, denominator is rules of total probability, so it is constant.</p></li>
</ul>
</li>
<li><p>Key is estimating , like P(y) we dk population! but estimate from samples, so MLE tells us that indeed
the best estimate is derived from the sample mean. Same for the mean of the conditional distribution.</p></li>
</ul>
<p>conditionally normal means within each class, the data is normal. reasonable assumption because of central limit theorem.</p>
<ul class="simple">
<li><p>Plotting the tuned Normal models for each species confirms that this naive Bayes assumption isn’t perfect – it’s a bit more idealistic than the density plots of the raw data in Figure 14.2. But it’s fine enough to continue. THIS IS BECAUSE THE ORIGINAL DATA IS NOT EXACTLY NORMAL, BUT THE CONDITIONAL NORMAL ASSUMPTION IS GOOD ENOUGH TO CONTINUE.</p></li>
<li><p>Note only assumption of Naive Bayes is the random variables within xn are independent conditional on the class of observation.</p></li>
</ul>
<ul class="simple">
<li><p>The plot below is the 1-d empirical distribution of <code class="docutils literal notranslate"><span class="pre">bill_length_mm</span></code> for all species.</p></li>
<li><p>The vertical red line is the mean of the empirical distribution.</p></li>
<li><p>This is where one “naive” part of naive Bayes classification comes into play. The naive Bayes method typically assumes that any quantitative predictor, here <code class="docutils literal notranslate"><span class="pre">X_2</span> <span class="pre">=</span> <span class="pre">bill_length_mm</span></code>
is continuous and conditionally Normal.</p></li>
</ul>
<p><strong>Important</strong></p>
<p>From sklearn: and we can use Maximum A Posteriori (MAP) estimation to estimate <span class="math notranslate nohighlight">\(P(y)\)</span>  and <span class="math notranslate nohighlight">\(P(x_i|y)\)</span> ; the former is then the relative frequency of class y in the training set.</p>
<p>The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of <span class="math notranslate nohighlight">\(P(x_i|y)\)</span>.</p>
<p>page 475 of chans book says that sample mean of the normal distribution coincides when you use MLE to estimate the mean parameter of the normal distribution.
In other words, if <span class="math notranslate nohighlight">\(X_2 | Y = A \sim N(\mu_{A}, \sigma_{A}^2)\)</span>, then to find the <span class="math notranslate nohighlight">\(P(X_2 | Y = A)\)</span>, which is the probability of <span class="math notranslate nohighlight">\(X_2=x_2\)</span> given <span class="math notranslate nohighlight">\(Y=A\)</span>,
we know that conditional PDF is about <span class="math notranslate nohighlight">\(X_2\)</span> as <span class="math notranslate nohighlight">\(Y\)</span> is fixed, so we need the parameters <span class="math notranslate nohighlight">\(\mu_{A}\)</span> and <span class="math notranslate nohighlight">\(\sigma_{A}^2\)</span> to find the PDF.</p>
<p>So this is an inverse problem of finding the parameters <span class="math notranslate nohighlight">\(\mu_{A}\)</span> and <span class="math notranslate nohighlight">\(\sigma_{A}^2\)</span>.
A bit confused see chans page 469, but overall, the idea is that we want to find the parameters of the normal distribution that best fit the data.
And we use MLE to find the parameters of the normal distribution that best fit the data. It turns the parameters
found through MLE are the sample mean and sample variance of the data.</p>
<p>Penguin with bill length of 50mm.</p>
<p><span class="math notranslate nohighlight">\(P(x_2=50|y=Adelie)\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># plot conditional distribution of bill length given species

</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>sns.displot(penguins, x=&quot;bill_length_mm&quot;, y=&quot;bill_depth_mm&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;seaborn.axisgrid.FacetGrid at 0x7fcb4cf681c0&gt;
</pre></div>
</div>
<img alt="../../../_images/naive_bayes_example_99_1.png" src="../../../_images/naive_bayes_example_99_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>sns.displot(penguins, x=&quot;bill_length_mm&quot;, y=&quot;bill_depth_mm&quot;, hue=&quot;species&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;seaborn.axisgrid.FacetGrid at 0x7fcb52e962b0&gt;
</pre></div>
</div>
<img alt="../../../_images/naive_bayes_example_101_1.png" src="../../../_images/naive_bayes_example_101_1.png" />
</div>
</div>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Categorical_distribution">https://en.wikipedia.org/wiki/Categorical_distribution</a></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Lagrange_multiplier">https://en.wikipedia.org/wiki/Lagrange_multiplier</a></p></li>
</ul>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id4"><span class="brackets">1</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id2">2</a>,<a href="#id3">3</a>)</span></dt>
<dd><p>Cite Dive into Deep Learning on this. Also, the joint probability is intractable because the number of parameters to estimate is exponential in the number of features. Use binary bits example, see my notes.</p>
</dd>
</dl>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./machine_learning_algorithms/generative/naive_bayes"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="naive_bayes.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Naive Bayes</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../../../references_and_resources/bibliography.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Bibliography</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Gao Hongnan<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>